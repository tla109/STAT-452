---
title: "Homework 3"
author: "Tianyu Liu"
date: "2023-11-11"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem Set 13, Applications

## 1

### (a)
```{r}
vehdata <- read.csv("vehicle.csv")
summary(vehdata)
```

### (b)

```{r}
vehdata$class = factor(vehdata$class, labels=c('2D', '4D', 'BUS', 'VAN'))
summary(vehdata$class)
```

### (c)

```{r}
cor(vehdata[,1:18])
```
The following variables have strong correlations (beyond &pm;0.7):\
Compactness and Distance.Circularity\
Compactness and Scatter.Ratio\
Compactness and Elongatedness\
Compactness and Pr.Axis.Rectangularity\
Compactness and Scaled.Variance.Along.Major.Axis\
Compactness and Scaled.Variance.Along.Minor.Axis\
Circularity and Distance.Circularity\
Circularity and Scatter.Ratio\
Circularity and Elongatedness\
Circularity and Pr.Axis.Rectangularity\
Circularity and Scaled.Variance.Along.Major.Axis\
Circularity and Scaled.Variance.Along.Minor.Axis\
Distance.Circularity and Radius.Ratio\
Distance.Circularity and Pr.Axis.Rectangularity\
Distance.Circularity and Max.Length.Rectangularity\
Distance.Circularity and Scaled.Variance.Along.Major.Axis\
Distance.Circularity and Scaled.Variance.Along.Minor.Axis\
Distance.Circularity and Scaled.Radius.of.Gyration\
Radius.Ratio and Pr.Axis.Aspect.Ratio\
Radius.Ratio and Scatter.Ratio\
Radius.Ratio and Elongatedness\
Radius.Ratio and Pr.Axis.Rectangularity\
Radius.Ratio and Scaled.Variance.Along.Major.Axis\
Radius.Ratio and Scaled.Variance.Along.Minor.Axis\
Scatter.Ratio and Max.Length.Rectangularity\
Scatter.Ratio and Scaled.Radius.of.Gyration\
Elongatedness and Max.Length.Rectangularity\
Elongatedness and Scaled.Radius.of.Gyration\
Pr.Axis.Rectangularity and Max.Length.Rectangularity\
Pr.Axis.Rectangularity and Scaled.Radius.of.Gyration\
Max.Length.Rectangularity and Scaled.Variance.Along.Major.Axis\
Max.Length.Rectangularity and Scaled.Variance.Along.Minor.Axis\
Max.Length.Rectangularity and Scaled.Radius.of.Gyration\
Scaled.Variance.Along.Major.Axis and Scaled.Radius.of.Gyration\
Scaled.Variance.Along.Minor.Axis and Scaled.Radius.of.Gyration\
Skewness.About.Major.Axis and Kurtosis.About.Major.Axis\
Skewness.About.Major.Axis and Hollows.Ratio\
Kurtosis.About.Major.Axis and Hollows.Ratio\

The following variables have especially strong correlations (beyond &pm;0.9):\
Circularity and Max.Length.Rectangularity\
Circularity and Scaled.Radius.of.Gyration\
Distance.Circularity and Scatter.Ratio\
Distance.Circularity and Elongatedness\
Scatter.Ratio and Elongatedness\
Scatter.Ratio and Pr.Axis.Rectangularity\
Scatter.Ratio and Scaled.Variance.Along.Major.Axis\
Scatter.Ratio and Scaled.Variance.Along.Minor.Axis\
Elongatedness and Pr.Axis.Rectangularity\
Elongatedness and Scaled.Variance.Along.Major.Axis\
Elongatedness and Scaled.Variance.Along.Minor.Axis\
Pr.Axis.Rectangularity and Scaled.Variance.Along.Major.Axis\
Pr.Axis.Rectangularity and Scaled.Variance.Along.Minor.Axis\
Scaled.Variance.Along.Major.Axis and Scaled.Variance.Along.Minor.Axis\

## 2

```{r}

set.seed(46685326, kind = "Mersenne-Twister")
perm <- sample(x = nrow(vehdata))
set1 <- vehdata[which(perm <= 3*nrow(vehdata)/4), ]
set2 <- vehdata[which(perm > 3*nrow(vehdata)/4), ]
head(set1)
head(set2)
```

## 3
```{r}
library(FNN)

### Split up the predictor variables from the class labels.
X.train.raw = set1[, -19]
X.valid.raw = set2[, -19]
Y.train = set1[, 19]
Y.valid = set2[, 19]


### KNN is based on distances. If variables are measured on different
### scales, we can change which points are neighbours by measuring
### in different units.
### a function we can use to rescale the columns of
### a data frame to have mean 0 and SD 1. We can also use it to rescale
### a data frame based on the means and SDs of another (this is useful
### for scaling the validation set to match the training set).

### Rescale x1 using the means and SDs of x2
scale.1 <- function(x1, x2) {
  for (col in 1:ncol(x1)) {
    a <- mean(x2[, col])
    b <- sd(x2[, col])
    x1[, col] <- (x1[, col] - a) / b
  }
  x1
}

### Rescale our training and validation sets
X.train <- scale.1(X.train.raw, X.train.raw)
X.valid <- scale.1(X.valid.raw, X.train.raw) # Watch the order


### Now we can fit a KNN model using the knn function in the FNN
### package. The syntax of the knn function is a bit different from
### what we're used to. The first two inputs are the training and
### validation predictor matrices. The third input is the class labels
### for the training set. We can also set k to the number of
### neighbours we want. The function then outputs predicted class
### labels for the validation set. Let's use 1 neighbour.
pred.knn <- knn(X.train, X.valid, Y.train, k = 1)
```

## (a)

```{r}
### Let's make a confusion matrix. We get this using the table()
### function and providing both the predicted and true class labels
### for the validation set. We can also set the axis labels using
### the dnn input.
table(pred.knn, Y.valid, dnn = c("Predicted", "Observed"))
```
There are 55 vehicles of type 2D, 54 vehicles of type 4D, 59 vehicles of type BUS 
and 54 vehicles of type VAN. In particular, it seems to be difficult to distinguish
between 2D and 4D vehicles, whereas BUS and VAN vehicles are easy to predict.

## (b)
```{r}
### Next, let's get the misclassification rate
(misclass.knn <- mean(pred.knn != Y.valid))
(se.knn <- sapply(misclass.knn, function(r) {
     sqrt(r * (1 - r) / nrow(X.train))
 }))
```
The test misclassification rate is about 31%, and the standard error is approximately &pm;1.8%

# Problem Set 14, Applications

## 1

### (a)
```{r}
### Rescale the columns of x1 so that the columns of x2 fall between 0 and 1
rescale <- function(x1, x2) {
  for (col in 1:ncol(x1)) {
    a <- min(x2[, col])
    b <- max(x2[, col])
    x1[, col] <- (x1[, col] - a) / (b - a)
  }
  x1
}

### Create copies of our datasets and rescale
data.train.scale <- set1
data.valid.scale <- set2
data.train.scale[, -19] <- rescale(data.train.scale[, -19], set1[, -19])
data.valid.scale[, -19] <- rescale(data.valid.scale[, -19], set1[, -19])
summary(data.train.scale[,1:3])
summary(data.train.scale[,1:3])
```
It appears that the data has been correctly scaled, to lie between 0 and 1.

### (b)

#### (i)
```{r}
library(nnet)
library(car)
library(glmnet)
library(MASS)

fit.log.nnet <- multinom(class ~ ., data = data.train.scale)
Anova(fit.log.nnet)
```
According to Anova, the following variables are important (at 95% confidence level):\
Compactness\
Circularity\
Distance.Circularity\
Radius.Ratio\
Pr.Axis.Aspect.Ratio\
Max.Length.Aspect.Ratio\
Max.Length.Rectangularity\
Scaled.Variance.Along.Major.Axis\
Scaled.Radius.of.Gyration\
Skewness.About.Major.Axis\
Skewness.About.Minor.Axis\
Kurtosis.About.Minor.Axis\
Kurtosis.About.Major.Axis\
Hollows.Ratio\

So that's 14 out of 18 variables. The other 4 are relatively unimportant.

#### (ii)

```{r}
pred.log.nnet <- predict(fit.log.nnet, data.valid.scale)
(misclass.log.nnet <- mean(pred.log.nnet != Y.valid))
(misclass.log.se <- sapply(misclass.log.nnet, function(r) {
     sqrt(r * (1 - r) / nrow(X.train))
 }))
```
The misclassification rate is still just over 21%.
The standard error for the multinomial classification is about 1.6%, slightly lower than KNN but not by much.

#### (iii)
```{r}
table(Y.valid, pred.log.nnet, ### Confusion matrix
  dnn = c("Observed", "Predicted")
)
misclass.log.nnet
```
Note that the misclassification rate is lower (just above 21%). We still observe that 2D and 4D vehicles are hard to distinguish and that BUS and VAN are much easier to distinguish.

## 2

### (a)
```{r}
### The glmnet() function uses predictor matrix/response vector syntax,
### so we need to extract these from our training and validation sets.
### We also have to convert the predictors to a matrix using the
### as.matrix() function.
X.train.scale <- as.matrix(data.train.scale[, -19])
Y.train <- data.train.scale[, 19]
X.valid.scale <- as.matrix(data.valid.scale[, -19])
Y.valid <- data.valid.scale[, 19]

### While we're looking at the glmnet package, let's do LASSO. We need to
### choose lambda using CV. Fortunately, the cv.glmnet() function does this
### for us. The syntax for cv.glmnet() is the same as for glmnet().
fit.CV.lasso <- cv.glmnet(X.train.scale, Y.train, family = "multinomial")

### The CV-min values are stored in the output from
### cv.glmnet()
lambda.min <- fit.CV.lasso$lambda.min

### Let's check which predictors are included in each "best" model. We
### can get the coefficients using the coef() function, setting s to
### the appropriate lambda value.
coef(fit.CV.lasso, s = lambda.min)

### Now we can get predictions for both "best" models
pred.lasso.min <- predict(fit.CV.lasso, X.valid.scale,
  s = lambda.min,
  type = "class"
)

table(Y.valid, pred.lasso.min, dnn = c("Obs", "Pred"))
```
For predicting 2D, 17 of the 18 variables are important, except for Pr.Axis.Rectangularity.\
For predicting 4D, 16 of the 18 variables are important, except for Scatter.Ratio and Scaled.Variance.Along.Major.Axis.\
For predicting BUS, 14 of the 18 variables are important, except for Circularity, Scatter.Ratio, Pr.Axis.Rectangularity, and Scaled.Variance.Along.Minor.Axis.\
For predicting VAN, 15 of the 18 variables are important, except for Circularity, Scaled.Variance.Along.Major.Axis, and Scaled.Variance.Along.Minor.Axis.\

Recall from ANOVA, that Scatter.Ratio, Elongatedness, Pr.Axis.Rectangularity, and Scaled.Variance.Along.Minor.Axis are unimportant variables, but the LASSO logit model suggests that Circularity would be unimportant for predicting BUS and VAN, and Elongatedness is important in all cases. The other variables generally match with ANOVA.

### (b)

```{r}
(miss.lasso.min <- mean(Y.valid != pred.lasso.min))
(se.miss.lasso.min <- sapply(miss.lasso.min, function(r) {
     sqrt(r * (1 - r) / nrow(X.train.scale))
 }))
```
The misclassification rate is now at around 22%.
The error is still at 1.6%. No improvement has been made.

## 3

### (a)

```{r}
### For discriminant analysis, it's best to scale predictors
### to have mean 0 and SD 1 (this makes the results easier to
### interpret). We can do this using using the following function.

### Rescale x1 using the means and SDs of x2
scale.1 <- function(x1, x2) {
  for (col in 1:ncol(x1)) {
    a <- mean(x2[, col])
    b <- sd(x2[, col])
    x1[, col] <- (x1[, col] - a) / b
  }
  x1
}


X.train.DA <- scale.1(set1[, -19], set1[, -19])
X.valid.DA <- scale.1(set2[, -19], set1[, -19])

class.col <- ifelse(set1$class=='2D', y = 53,
n = ifelse(set1$class=='4D', y = 68, n = ifelse(set1$class=='BUS',y=203,n=464)))

### Fit an LDA model using the lda() funtion from the MASS package. This
### function uses predictor/response syntax.
fit.lda <- lda(X.train.DA, Y.train)

### We can plot the data using the linear discriminants. It's best to
### include colors. Let's just recycle the colors from above.
### There is no simple way to change the axis labels. Sometimes we just need
### to live with the defaults.
plot(fit.lda, col = class.col)
```

LD1 seems to be separating vehicles of class BUS from the rest.\
LD2 seems to be separating vehicles of class 2D and 4D from the rest. It cannot really distinguish between the 2 classes.\
LD3 seems to be separating vehicles of class VAN from LD2.\

### (b)
```{r}
### We get predictions by extracting the class object from the predict()
### function's output.
pred.lda <- predict(fit.lda, X.valid.DA)$class

table(Y.valid, pred.lda, dnn = c("Obs", "Pred"))

(miss.lda <- mean(Y.valid != pred.lda))
(se.miss.lda <- sapply(miss.lda, function(r) {
     sqrt(r * (1 - r) / nrow(X.train.DA))
 }))
```
The misclassification rate is just under 20%.
The error is just under 1.6%, so it is a very slight improvement compare to the rest.