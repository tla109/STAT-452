---
title: "Homework 3"
author: "Tianyu Liu"
date: "2023-11-11"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem Set 13, Applications

## 1

### (a)
```{r}
vehdata <- read.csv("vehicle.csv")
summary(vehdata)
```

### (b)

```{r}
vehdata$class = factor(vehdata$class, labels=c('2D', '4D', 'BUS', 'VAN'))
summary(vehdata$class)
```

### (c)

```{r}
cor(vehdata[,1:18])
```
The following variables have strong correlations (beyond &pm;0.7):
Compactness and Distance.Circularity
Compactness and Scatter.Ratio
Compactness and Elongatedness
Compactness and Pr.Axis.Rectangularity
Compactness and Scaled.Variance.Along.Major.Axis
Compactness and Scaled.Variance.Along.Minor.Axis
Circularity and Distance.Circularity
Circularity and Scatter.Ratio
Circularity and Elongatedness
Circularity and Pr.Axis.Rectangularity
Circularity and Scaled.Variance.Along.Major.Axis
Circularity and Scaled.Variance.Along.Minor.Axis
Distance.Circularity and Radius.Ratio
Distance.Circularity and Pr.Axis.Rectangularity
Distance.Circularity and Max.Length.Rectangularity
Distance.Circularity and Scaled.Variance.Along.Major.Axis
Distance.Circularity and Scaled.Variance.Along.Minor.Axis
Distance.Circularity and Scaled.Radius.of.Gyration
Radius.Ratio and Pr.Axis.Aspect.Ratio
Radius.Ratio and Scatter.Ratio
Radius.Ratio and Elongatedness
Radius.Ratio and Pr.Axis.Rectangularity
Radius.Ratio and Scaled.Variance.Along.Major.Axis
Radius.Ratio and Scaled.Variance.Along.Minor.Axis
Scatter.Ratio and Max.Length.Rectangularity
Scatter.Ratio and Scaled.Radius.of.Gyration
Elongatedness and Max.Length.Rectangularity
Elongatedness and Scaled.Radius.of.Gyration
Pr.Axis.Rectangularity and Max.Length.Rectangularity
Pr.Axis.Rectangularity and Scaled.Radius.of.Gyration
Max.Length.Rectangularity and Scaled.Variance.Along.Major.Axis
Max.Length.Rectangularity and Scaled.Variance.Along.Minor.Axis
Max.Length.Rectangularity and Scaled.Radius.of.Gyration
Scaled.Variance.Along.Major.Axis and Scaled.Radius.of.Gyration
Scaled.Variance.Along.Minor.Axis and Scaled.Radius.of.Gyration
Skewness.About.Major.Axis and Kurtosis.About.Major.Axis
Skewness.About.Major.Axis and Hollows.Ratio
Kurtosis.About.Major.Axis and Hollows.Ratio

The following variables have especially strong correlations (beyond &pm;0.9):
Circularity and Max.Length.Rectangularity
Circularity and Scaled.Radius.of.Gyration
Distance.Circularity and Scatter.Ratio
Distance.Circularity and Elongatedness
Scatter.Ratio and Elongatedness
Scatter.Ratio and Pr.Axis.Rectangularity
Scatter.Ratio and Scaled.Variance.Along.Major.Axis
Scatter.Ratio and Scaled.Variance.Along.Minor.Axis
Elongatedness and Pr.Axis.Rectangularity
Elongatedness and Scaled.Variance.Along.Major.Axis
Elongatedness and Scaled.Variance.Along.Minor.Axis
Pr.Axis.Rectangularity and Scaled.Variance.Along.Major.Axis
Pr.Axis.Rectangularity and Scaled.Variance.Along.Minor.Axis
Scaled.Variance.Along.Major.Axis and Scaled.Variance.Along.Minor.Axis

## 2

```{r}

set.seed(46685326, kind = "Mersenne-Twister")
perm <- sample(x = nrow(vehdata))
set1 <- vehdata[which(perm <= 3*nrow(vehdata)/4), ]
set2 <- vehdata[which(perm > 3*nrow(vehdata)/4), ]
head(set1)
head(set2)
```

## 3
```{r}
library(FNN)

### Split the data into training and validation sets
p.train <- 0.75
data = vehdata
n <- nrow(data)
n.train <- floor(p.train * n)

ind.random <- sample(1:n)
data.train <- data[ind.random <= n.train, ]
data.valid <- data[ind.random > n.train, ]

### Split up the predictor variables from the class labels.
X.train.raw <- data.train[, -19]
X.valid.raw <- data.valid[, -19]
Y.train <- data.train[, 19]
Y.valid <- data.valid[, 19]


### KNN is based on distances. If variables are measured on different
### scales, we can change which points are neighbours by measuring
### in different units.
### a function we can use to rescale the columns of
### a data frame to have mean 0 and SD 1. We can also use it to rescale
### a data frame based on the means and SDs of another (this is useful
### for scaling the validation set to match the training set).

### Rescale x1 using the means and SDs of x2
scale.1 <- function(x1, x2) {
  for (col in 1:ncol(x1)) {
    a <- mean(x2[, col])
    b <- sd(x2[, col])
    x1[, col] <- (x1[, col] - a) / b
  }
  x1
}

### Rescale our training and validation sets
X.train <- scale.1(X.train.raw, X.train.raw)
X.valid <- scale.1(X.valid.raw, X.train.raw) # Watch the order


### Now we can fit a KNN model using the knn function in the FNN
### package. The syntax of the knn function is a bit different from
### what we're used to. The first two inputs are the training and
### validation predictor matrices. The third input is the class labels
### for the training set. We can also set k to the number of
### neighbours we want. The function then outputs predicted class
### labels for the validation set. Let's use 1 neighbour.
pred.knn <- knn(X.train, X.valid, Y.train, k = 1)
```

## (a)

```{r}
### Let's make a confusion matrix. We get this using the table()
### function and providing both the predicted and true class labels
### for the validation set. We can also set the axis labels using
### the dnn input.
table(pred.knn, Y.valid, dnn = c("Predicted", "Observed"))
```
There are 57 vehicles of type 2D, 56 vehicles of type 4D, 48 vehicles of type BUS 
and 51 vehicles of type VAN. In particular, it seems to be difficult to distinguish
between 2D and 4D vehicles (there is actually more mispredictions for 2D than correct
predictions), whereas BUS and VAN vehicles are easy to predict.

## (b)
```{r}
### Next, let's get the misclassification rate
(misclass.knn <- mean(pred.knn != Y.valid))
(se.knn <- sd(pred.knn != Y.valid)/sqrt(212))
```
The test misclassification rate is about 31%, and the standard error is approximately &pm;3%
