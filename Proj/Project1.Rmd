---
title: "Project1"
author: "Terry"
date: "2023-10-27"
output: html_document
---

```{r}

remove(list=ls())
set.seed(301249861)
#load in the data
proj = read.csv("training_data.csv", header = TRUE)
head(proj)
dim(proj)
# 21 explanatory variables, 10000 rows
### Let's define a function for constructing CV folds
get.folds <- function(n, K) {
  ### Get the appropriate number of fold labels
  n.fold <- ceiling(n / K) # Number of observations per fold (rounded up)
  fold.ids.raw <- rep(1:K, times = n.fold) # Generate extra labels
  fold.ids <- fold.ids.raw[1:n] # Keep only the correct number of labels

  ### Shuffle the fold labels
  folds.rand <- fold.ids[sample.int(n)]

  return(folds.rand)
}

get.MSPE <- function(Y, Y.hat) {
  return(mean((Y - Y.hat)^2))
}
```

Model Selection

```{r}
library(beepr)
library(stringr)
library(MASS)
library(glmnet)
library(pls)
library(rpart)
library(randomForest)
library(gbm)

K <- 5

### Construct folds
n <- nrow(proj) # Sample size
folds <- get.folds(n, K)

### Create a container for MSPEs. Let's include ordinary least-squares
### regression for reference
all.models <- c("Ridge", "LASSO-Min", "LASSO-1se", "LS", "hybrid stepwise", "PLS", "RTBoot", "RF", "RF_Tune", "Boost", "Boost_Engineer")
all.MSPEs <- array(0, dim = c(K, length(all.models)))
colnames(all.MSPEs) <- all.models

pred.tree <- matrix(NA, nrow = n, ncol = K)

m_pars <- 2:4
nodes <- c(3, 5, 10)
rf_tun_best <- matrix(NA, nrow = K, ncol = 2)
rf_tun_best[, 2] <- 1:K

lam_vals <- c(0.001, 0.005, 0.025, 0.125)
d_vals <- c(2, 4, 6)

boost_orig_tun_best <- matrix(NA, nrow = K, ncol = 3)
boost_orig_tun_best[, 3] <- 1:K
boost_eng_tun_best <- matrix(NA, nrow = K, ncol = 3)
boost_eng_tun_best[, 3] <- 1:K

n_trees <- 10000

### Begin cross-validation
begin = Sys.time()

for (i in 1:K) {
  ### Split data
  data.train <- proj[folds != i, ]
  data.valid <- proj[folds == i, ]
  n.train <- nrow(data.train)

  ### Get response vectors
  Y.train <- data.train$Y
  Y.valid <- data.valid$Y

  ### Let's do LS. ###

  fit.ls <- lm(Y ~ ., data = data.train)
  pred.ls <- predict(fit.ls, newdata = data.valid)
  MSPE.ls <- get.MSPE(Y.valid, pred.ls)
  all.MSPEs[i, "LS"] <- MSPE.ls
  
  ### Let's do ridge regression.  ###

  lambda.vals <- seq(from = 0, to = 100, by = 0.05)

  fit.ridge <- lm.ridge(Y ~ .,
    lambda = lambda.vals,
    data = data.train
  )

  ind.min.GCV <- which.min(fit.ridge$GCV)
  lambda.min <- lambda.vals[ind.min.GCV]

  all.coefs.ridge <- coef(fit.ridge)
  coef.min <- all.coefs.ridge[ind.min.GCV, ]

  matrix.valid.ridge <- model.matrix(Y ~ ., data = data.valid)


  pred.ridge <- matrix.valid.ridge %*% coef.min
  MSPE.ridge <- get.MSPE(Y.valid, pred.ridge)
  all.MSPEs[i, "Ridge"] <- MSPE.ridge

  ### Now we can do the LASSO. ###

  matrix.train.raw <- model.matrix(Y ~ ., data = data.train)
  matrix.train <- matrix.train.raw[, -1]

  ### 'Best' can refer to
  ### either the value of lambda which gives the smallest CV-MSPE
  ### (called the min rule), or the value of lambda which gives the
  ### simplest model that gives CV-MSPE close to the minimum (called
  ### the 1se rule). 
  all.LASSOs <- cv.glmnet(x = matrix.train, y = Y.train)

  lambda.min <- all.LASSOs$lambda.min
  lambda.1se <- all.LASSOs$lambda.1se

  coef.LASSO.min <- predict(all.LASSOs, s = lambda.min, type = "coef")
  coef.LASSO.1se <- predict(all.LASSOs, s = lambda.1se, type = "coef")

  included.LASSO.min <- predict(all.LASSOs,
    s = lambda.min,
    type = "nonzero"
  )
  included.LASSO.1se <- predict(all.LASSOs,
    s = lambda.1se,
    type = "nonzero"
  )

  matrix.valid.LASSO.raw <- model.matrix(Y ~ ., data = data.valid)
  matrix.valid.LASSO <- matrix.valid.LASSO.raw[, -1]
  pred.LASSO.min <- predict(all.LASSOs,
    newx = matrix.valid.LASSO,
    s = lambda.min, type = "response"
  )
  pred.LASSO.1se <- predict(all.LASSOs,
    newx = matrix.valid.LASSO,
    s = lambda.1se, type = "response"
  )

  MSPE.LASSO.min <- get.MSPE(Y.valid, pred.LASSO.min)
  all.MSPEs[i, "LASSO-Min"] <- MSPE.LASSO.min

  MSPE.LASSO.1se <- get.MSPE(Y.valid, pred.LASSO.1se)
  all.MSPEs[i, "LASSO-1se"] <- MSPE.LASSO.1se
  
  ### the hybrid stepwise
  
  step <- step(
    object = lm(Y ~ 1, data = data.train), scope = list(upper = fit.ls), direction = "both",
    k = log(nrow(data.train)), trace = 0)
  pred.sw <- predict(step, as.data.frame(data.valid))
  MSPE.sw <- get.MSPE(Y.valid, pred.sw)
  all.MSPEs[i, "hybrid stepwise"] <- MSPE.sw
  
  ### Partial Least Squares (PLS)
  fit_pls <- plsr(Y ~ .,
    data = data.train, validation = "CV")

  CV_pls <- fit_pls$validation
  pls_comps <- CV_pls$PRESS
  n_comps <- which.min(pls_comps)

  pred.pls <- predict(fit_pls, data.valid, ncomp = n_comps)
  MSPE.pls <- get.MSPE(Y.valid, pred.pls)
  all.MSPEs[i, "PLS"] <- MSPE.pls
  
  ### Regression Tree (RT), with bootstrap
  
  pr.tree1 <- rpart(Y ~ ., method = "anova", data = data.train, cp = 0)
  cpt1 <- pr.tree1$cptable

  # Find location of minimum error
  minrow1 <- which.min(cpt1[, 4])
  # Take geometric mean of cp values at min error and one step up
  cplow.min1 <- cpt1[minrow1, 1]
  cpup.min1 <- ifelse(minrow1 == 1, yes = 1, no = cpt1[minrow1 - 1, 1])
  cp.min1 <- sqrt(cplow.min1 * cpup.min1)

  # Do pruning each way
  pr.prune.min1 <- prune(pr.tree1, cp = cp.min1)
  preds1 <- predict(pr.prune.min1, newdata = data.valid)

  pred.tree[, i] <- preds1
  
  MSPE.RTBoot.min <- get.MSPE(Y.valid, pred.tree)
  all.MSPEs[i, "RTBoot"] <- MSPE.RTBoot.min
  
  ###### Default Random Forest ######
  fit_rf <- randomForest(Y ~., data = data.train)
  pred_rf <- predict(fit_rf, newdata = data.valid)
  MSPE_rf <- get.MSPE(Y.valid, pred_rf)
  all.MSPEs[i, "RF"] <- MSPE_rf

  #### Tuning RF ######
  ## We need to tune for each of the K folds separately, but can use
  ## the OOB estimates to do this for us
  
  rf_tun_vals <- rep(0, length(m_pars) * length(nodes))
  # rf_vals <- ###
  counter <- 1
  for(m in m_pars){
    for(ns in nodes){
      rf_fit <- randomForest(data = data.train, Y ~ ., ntree = 500, mtry = m, nodesize = ns)
      ## then get and store the out of bag error for each of these
      rf_tun_vals[counter] <- mean((predict(rf_fit) - Y.valid)^2)
      counter <- counter + 1
    }
  }
  
  ## then figure out which of these is best
  parms <- expand.grid(m_pars, nodes)
  names(rf_tun_vals) <- paste(parms[, 2], parms[, 1], sep = "|")
  
  ## values which give you the best fit
  best <- names(which.min(rf_tun_vals))
  rf_tun_best[i, 1] <- best
  
  ## then need to use those settings
  m_best <- parms[which.min(rf_tun_vals), 1]
  node_best <- parms[which.min(rf_tun_vals), 2]

  ### then fit this best model again, predict for all the data
  rf_tuned <- randomForest(data = data.train, Y ~ ., ntree = 500, mtry = m_best, nodesize = node_best)

  ## get predictions for new data
  rf_tuned_pred <- predict(rf_tuned, newdata = data.valid)
  rf_tuned_mspe <- get.MSPE(Y.valid, rf_tuned_pred)
  all.MSPEs[i, "RF_Tune"] <- rf_tuned_mspe
  
  #### Boosting Versions ####
  ### Tuning using only original features
  boost_tun_values <- rep(0, length(lam_vals) * length(d_vals))
  boost_opt_tree <- rep(0, length(lam_vals) * length(d_vals))
  counter <- 1
  for (d in d_vals) {
    for (s in lam_vals) {
      pro.gbm <- gbm(Y ~ ., data = data.train, distribution = "gaussian",
      n.trees = n_trees, interaction.depth = d, shrinkage = s, bag.fraction = 0.8)
      treenum <- min(n_trees, 2 * gbm.perf(pro.gbm, method = "OOB", plot.it = FALSE))
      boost_opt_tree[counter] <- treenum
      preds <- predict(pro.gbm, newdata = data.valid, n.trees = treenum)
      boost_tun_values[counter] <- mean((preds - Y.valid)^2)
      counter <- counter + 1
    }
  }

  ## then figure out which setting was best for this fold
  parms <- expand.grid(lam_vals, d_vals)
  names(boost_tun_values) <- paste(parms[, 2], parms[, 1], sep = "|")
  best <- names(which.min(boost_tun_values))
  boost_orig_tun_best[i, 1] <- best
  boost_orig_tun_best[i, 2] <- boost_opt_tree[which.min(boost_tun_values)]
  all.MSPEs[i, "Boost"] <- min(boost_tun_values)

  #### Using new features also
  ### Tuning using only original features
  boost_tun_values <- rep(0, length(lam_vals) * length(d_vals))
  boost_opt_tree <- rep(0, length(lam_vals) * length(d_vals))
  counter <- 1
  for (d in d_vals) {
    for (s in lam_vals) {
      pro.gbm <- gbm(Y ~ ., data = data.train, distribution = "gaussian",
      n.trees = n_trees, interaction.depth = d, shrinkage = s, bag.fraction = 0.8)
      treenum <- min(n_trees, 2 * gbm.perf(pro.gbm, method = "OOB", plot.it = FALSE))
      boost_opt_tree[counter] <- treenum
      preds <- predict(pro.gbm, newdata = data.valid, n.trees = treenum)
      boost_tun_values[counter] <- mean((preds - Y.valid)^2)
      counter <- counter + 1
    }
  }

  ## then figure out which setting was best for this fold
  parms <- expand.grid(lam_vals, d_vals)
  names(boost_tun_values) <- paste(parms[, 2], parms[, 1], sep = "|")
  best <- names(which.min(boost_tun_values))
  boost_eng_tun_best[i, 1] <- best
  boost_eng_tun_best[i, 2] <- boost_opt_tree[which.min(boost_tun_values)]
  all.MSPEs[i, "Boost_Engineer"] <- min(boost_tun_values)

}

end = Sys.time()
(end - begin) ## can take up to 5 hours
beep()

avg.MSPEs = colMeans(all.MSPEs)
```


```{r}
boxplot(all.MSPEs, main = paste0("CV MSPEs over ", K, " folds"))

all.RMSPEs <- apply(all.MSPEs, 1, function(W) {
  best <- min(W)
  return(W / best)
})
all.RMSPEs <- t(all.RMSPEs)
boxplot(all.RMSPEs, main = paste0("CV RMSPEs over ", K, " folds"))
```
Random Forest has lowest MSPE.

```{r}
### now focus on random forest

begin.rf = Sys.time()

fit_rf <- randomForest(Y ~., data = proj, importance = T)
(importance(fit_rf))
varImpPlot(fit_rf)

end.rf = Sys.time()

(end.rf - begin.rf)
```

The important variables are X1, X5, X8, X11, X12, X13, X18, X20, and X21, so fit a regression tree with these variables to the prediction dataset.

```{r}
begin.rf = Sys.time()

fit_rf_imp <- randomForest(Y ~ X1 + X5 + X8 + X11 + X12 + X13 + X18 + X20 + X21, data = proj, importance = T)
(importance(fit_rf_imp))
varImpPlot(fit_rf_imp)

end.rf = Sys.time()

(end.rf - begin.rf)

tp = read.csv("test_predictors.csv", header = T)
tp_rf <- predict(fit_rf_imp, newdata = tp)
write.table(tp_rf, "response.csv", sep = ",", 
            row.names = FALSE, col.names = FALSE)
```

