version
version
version
version
version
knitr::opts_chunk$set(echo = TRUE)
vehdata <- read.csv("vehicle.csv")
summary(vehdata)
vehdata$class = factor(vehdata$class, labels=c('2D', '4D', 'BUS', 'VAN'))
summary(vehdata$class)
cor(vehdata[,1:18])
set.seed(46685326, kind = "Mersenne-Twister")
perm <- sample(x = nrow(vehdata))
set1 <- vehdata[which(perm <= 3*nrow(vehdata)/4), ]
set2 <- vehdata[which(perm > 3*nrow(vehdata)/4), ]
head(set1)
head(set2)
library(FNN)
### Split the data into training and validation sets
p.train <- 0.75
data = vehdata
n <- nrow(data)
n.train <- floor(p.train * n)
ind.random <- sample(1:n)
data.train <- data[ind.random <= n.train, ]
data.valid <- data[ind.random > n.train, ]
### Split up the predictor variables from the class labels.
X.train.raw <- data.train[, -19]
X.valid.raw <- data.valid[, -19]
Y.train <- data.train[, 19]
Y.valid <- data.valid[, 19]
### KNN is based on distances. If variables are measured on different
### scales, we can change which points are neighbours by measuring
### in different units.
### a function we can use to rescale the columns of
### a data frame to have mean 0 and SD 1. We can also use it to rescale
### a data frame based on the means and SDs of another (this is useful
### for scaling the validation set to match the training set).
### Rescale x1 using the means and SDs of x2
scale.1 <- function(x1, x2) {
for (col in 1:ncol(x1)) {
a <- mean(x2[, col])
b <- sd(x2[, col])
x1[, col] <- (x1[, col] - a) / b
}
x1
}
### Rescale our training and validation sets
X.train <- scale.1(X.train.raw, X.train.raw)
X.valid <- scale.1(X.valid.raw, X.train.raw) # Watch the order
### Now we can fit a KNN model using the knn function in the FNN
### package. The syntax of the knn function is a bit different from
### what we're used to. The first two inputs are the training and
### validation predictor matrices. The third input is the class labels
### for the training set. We can also set k to the number of
### neighbours we want. The function then outputs predicted class
### labels for the validation set. Let's use 1 neighbour.
pred.knn <- knn(X.train, X.valid, Y.train, k = 1)
### Let's make a confusion matrix. We get this using the table()
### function and providing both the predicted and true class labels
### for the validation set. We can also set the axis labels using
### the dnn input.
table(pred.knn, Y.valid, dnn = c("Predicted", "Observed"))
### Next, let's get the misclassification rate
(misclass.knn <- mean(pred.knn != Y.valid))
(se.knn <- sd(pred.knn != Y.valid)/sqrt(212))
?writecsv
write.csv(cor(vehdata[,1:18]), file = 'cor.csv')
knitr::opts_chunk$set(echo = TRUE)
vehdata <- read.csv("vehicle.csv")
summary(vehdata)
vehdata$class = factor(vehdata$class, labels=c('2D', '4D', 'BUS', 'VAN'))
summary(vehdata$class)
cor(vehdata[,1:18])
set.seed(46685326, kind = "Mersenne-Twister")
perm <- sample(x = nrow(vehdata))
set1 <- vehdata[which(perm <= 3*nrow(vehdata)/4), ]
set2 <- vehdata[which(perm > 3*nrow(vehdata)/4), ]
head(set1)
head(set2)
library(FNN)
### Split the data into training and validation sets
p.train <- 0.75
data = vehdata
n <- nrow(data)
n.train <- floor(p.train * n)
ind.random <- sample(1:n)
data.train <- data[ind.random <= n.train, ]
data.valid <- data[ind.random > n.train, ]
### Split up the predictor variables from the class labels.
X.train.raw <- data.train[, -19]
X.valid.raw <- data.valid[, -19]
Y.train <- data.train[, 19]
Y.valid <- data.valid[, 19]
### KNN is based on distances. If variables are measured on different
### scales, we can change which points are neighbours by measuring
### in different units.
### a function we can use to rescale the columns of
### a data frame to have mean 0 and SD 1. We can also use it to rescale
### a data frame based on the means and SDs of another (this is useful
### for scaling the validation set to match the training set).
### Rescale x1 using the means and SDs of x2
scale.1 <- function(x1, x2) {
for (col in 1:ncol(x1)) {
a <- mean(x2[, col])
b <- sd(x2[, col])
x1[, col] <- (x1[, col] - a) / b
}
x1
}
### Rescale our training and validation sets
X.train <- scale.1(X.train.raw, X.train.raw)
X.valid <- scale.1(X.valid.raw, X.train.raw) # Watch the order
### Now we can fit a KNN model using the knn function in the FNN
### package. The syntax of the knn function is a bit different from
### what we're used to. The first two inputs are the training and
### validation predictor matrices. The third input is the class labels
### for the training set. We can also set k to the number of
### neighbours we want. The function then outputs predicted class
### labels for the validation set. Let's use 1 neighbour.
pred.knn <- knn(X.train, X.valid, Y.train, k = 1)
### Let's make a confusion matrix. We get this using the table()
### function and providing both the predicted and true class labels
### for the validation set. We can also set the axis labels using
### the dnn input.
table(pred.knn, Y.valid, dnn = c("Predicted", "Observed"))
### Next, let's get the misclassification rate
(misclass.knn <- mean(pred.knn != Y.valid))
(se.knn <- sd(pred.knn != Y.valid)/sqrt(212))
dim(vehdata)
dim(X.train.raw)
### Next, let's get the misclassification rate
(misclass.knn <- mean(pred.knn != Y.valid))
(se.knn <- sd(pred.knn != Y.valid)/sqrt(49+51+56+56))
?sd
pred.knn != Y.valid
mean(c(TRUE FALSE FALSE))
sys.getlocale()
Sys.getlocale()
knitr::opts_chunk$set(echo = TRUE)
pred.log.nnet <- predict(fit.log.nnet, data.valid.scale)
knitr::opts_chunk$set(echo = TRUE)
vehdata <- read.csv("vehicle.csv")
summary(vehdata)
vehdata$class = factor(vehdata$class, labels=c('2D', '4D', 'BUS', 'VAN'))
summary(vehdata$class)
cor(vehdata[,1:18])
set.seed(46685326, kind = "Mersenne-Twister")
perm <- sample(x = nrow(vehdata))
set1 <- vehdata[which(perm <= 3*nrow(vehdata)/4), ]
set2 <- vehdata[which(perm > 3*nrow(vehdata)/4), ]
head(set1)
head(set2)
library(FNN)
### Split up the predictor variables from the class labels.
X.train.raw = set1[, -19]
X.valid.raw = set2[, -19]
Y.train = set1[, 19]
Y.valid = set2[, 19]
### KNN is based on distances. If variables are measured on different
### scales, we can change which points are neighbours by measuring
### in different units.
### a function we can use to rescale the columns of
### a data frame to have mean 0 and SD 1. We can also use it to rescale
### a data frame based on the means and SDs of another (this is useful
### for scaling the validation set to match the training set).
### Rescale x1 using the means and SDs of x2
scale.1 <- function(x1, x2) {
for (col in 1:ncol(x1)) {
a <- mean(x2[, col])
b <- sd(x2[, col])
x1[, col] <- (x1[, col] - a) / b
}
x1
}
### Rescale our training and validation sets
X.train <- scale.1(X.train.raw, X.train.raw)
X.valid <- scale.1(X.valid.raw, X.train.raw) # Watch the order
### Now we can fit a KNN model using the knn function in the FNN
### package. The syntax of the knn function is a bit different from
### what we're used to. The first two inputs are the training and
### validation predictor matrices. The third input is the class labels
### for the training set. We can also set k to the number of
### neighbours we want. The function then outputs predicted class
### labels for the validation set. Let's use 1 neighbour.
pred.knn <- knn(X.train, X.valid, Y.train, k = 1)
### Let's make a confusion matrix. We get this using the table()
### function and providing both the predicted and true class labels
### for the validation set. We can also set the axis labels using
### the dnn input.
table(pred.knn, Y.valid, dnn = c("Predicted", "Observed"))
### Next, let's get the misclassification rate
(misclass.knn <- mean(pred.knn != Y.valid))
(se.knn <- sapply(misclass.knn, function(r) {
sqrt(r * (1 - r) / nrow(X.train))
}))
### Rescale the columns of x1 so that the columns of x2 fall between 0 and 1
rescale <- function(x1, x2) {
for (col in 1:ncol(x1)) {
a <- min(x2[, col])
b <- max(x2[, col])
x1[, col] <- (x1[, col] - a) / (b - a)
}
x1
}
### Create copies of our datasets and rescale
data.train.scale <- set1
data.valid.scale <- set2
data.train.scale[, -19] <- rescale(data.train.scale[, -19], set1[, -19])
data.valid.scale[, -19] <- rescale(data.valid.scale[, -19], set1[, -19])
summary(data.train.scale[,1:3])
summary(data.train.scale[,1:3])
library(nnet)
library(car)
library(glmnet)
library(MASS)
fit.log.nnet <- multinom(class ~ ., data = data.train.scale)
Anova(fit.log.nnet)
pred.log.nnet <- predict(fit.log.nnet, data.valid.scale)
(misclass.log.nnet <- mean(pred.log.nnet != Y.valid))
(misclass.log.se <- sapply(misclass.log.nnet, function(r) {
sqrt(r * (1 - r) / nrow(X.train))
}))
table(Y.valid, pred.log.nnet, ### Confusion matrix
dnn = c("Observed", "Predicted")
)
misclass.log.nnet
### The glmnet() function uses predictor matrix/response vector syntax,
### so we need to extract these from our training and validation sets.
### We also have to convert the predictors to a matrix using the
### as.matrix() function.
X.train.scale <- as.matrix(data.train.scale[, -19])
Y.train <- data.train.scale[, 19]
X.valid.scale <- as.matrix(data.valid.scale[, -19])
Y.valid <- data.valid.scale[, 19]
### While we're looking at the glmnet package, let's do LASSO. We need to
### choose lambda using CV. Fortunately, the cv.glmnet() function does this
### for us. The syntax for cv.glmnet() is the same as for glmnet().
fit.CV.lasso <- cv.glmnet(X.train.scale, Y.train, family = "multinomial")
### The CV-min values are stored in the output from
### cv.glmnet()
lambda.min <- fit.CV.lasso$lambda.min
### Let's check which predictors are included in each "best" model. We
### can get the coefficients using the coef() function, setting s to
### the appropriate lambda value.
coef(fit.CV.lasso, s = lambda.min)
### Now we can get predictions for both "best" models
pred.lasso.min <- predict(fit.CV.lasso, X.valid.scale,
s = lambda.min,
type = "class"
)
table(Y.valid, pred.lasso.min, dnn = c("Obs", "Pred"))
(miss.lasso.min <- mean(Y.valid != pred.lasso.min))
(se.miss.lasso.min <- sapply(miss.lasso.min, function(r) {
sqrt(r * (1 - r) / nrow(X.train.scale))
}))
### For discriminant analysis, it's best to scale predictors
### to have mean 0 and SD 1 (this makes the results easier to
### interpret). We can do this using using the following function.
### Rescale x1 using the means and SDs of x2
scale.1 <- function(x1, x2) {
for (col in 1:ncol(x1)) {
a <- mean(x2[, col])
b <- sd(x2[, col])
x1[, col] <- (x1[, col] - a) / b
}
x1
}
X.train.DA <- scale.1(set1[, -19], set1[, -19])
X.valid.DA <- scale.1(set2[, -19], set1[, -19])
class.col <- ifelse(set1$class=='2D', y = 53,
n = ifelse(set1$class=='4D', y = 68, n = ifelse(set1$class=='BUS',y=203,n=464)))
### Fit an LDA model using the lda() funtion from the MASS package. This
### function uses predictor/response syntax.
fit.lda <- lda(X.train.DA, Y.train)
### We can plot the data using the linear discriminants. It's best to
### include colors. Let's just recycle the colors from above.
### There is no simple way to change the axis labels. Sometimes we just need
### to live with the defaults.
plot(fit.lda, col = class.col)
### We get predictions by extracting the class object from the predict()
### function's output.
pred.lda <- predict(fit.lda, X.valid.DA)$class
table(Y.valid, pred.lda, dnn = c("Obs", "Pred"))
(miss.lda <- mean(Y.valid != pred.lda))
(se.miss.lda <- sapply(miss.lda, function(r) {
sqrt(r * (1 - r) / nrow(X.train.DA))
}))
setwd("C:/Users/terry/Desktop/STAT-452/Proj")
?read.csv
library(ROCR)
rocplot =function (pred , truth , ...){
predob = prediction (pred , truth)
perf = performance (predob , "tpr", "fpr")
plot(perf ,...)}
water = read.csv("water_potability.csv", header = TRUE)
remove(list=ls())
water = read.csv("water_potability.csv", header = TRUE)
head(water)
summary(water)
water$Potability = as.factor(water$Potability)
summary(water)
