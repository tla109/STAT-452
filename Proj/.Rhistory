X.train.scale <- as.matrix(data.train.scale[, -10])
Y.train <- data.train.scale[, 10]
X.valid.scale <- as.matrix(data.valid.scale[, -10])
Y.valid <- data.valid.scale[, 10]
fit.log.nnet <- multinom(Potability ~ ., data = data.train.scale)
### first fit a logistic regression
library(nnet)
### need to rescale
rescale <- function(x1, x2) {
for (col in 1:ncol(x1)) {
a <- min(x2[, col])
b <- max(x2[, col])
x1[, col] <- (x1[, col] - a) / (b - a)
}
x1
}
p.train <- 0.75
n <- nrow(water)
n.train <- floor(p.train * n)
ind.random <- sample(1:n)
data.train <- water[ind.random <= n.train, ]
data.valid <- water[ind.random > n.train, ]
Y.valid <- data.valid[, 21]
data.train.scale <- data.train
data.valid.scale <- data.valid
data.train.scale[, -21] <- rescale(data.train.scale[, -21], data.train[, -21])
data.valid.scale[, -21] <- rescale(data.valid.scale[, -21], data.train[, -21])
X.train.scale <- as.matrix(data.train.scale[, -10])
Y.train <- data.train.scale[, 10]
X.valid.scale <- as.matrix(data.valid.scale[, -10])
Y.valid <- data.valid.scale[, 10]
fit.log.nnet <- multinom(is_safe ~ ., data = data.train.scale)
summary(fit.log.nnet)
### Next, let's investigate the LR's performance on the test set
pred.log.nnet <- predict(fit.log.nnet, data.valid.scale)
table(Y.valid, pred.log.nnet, ### Confusion matrix
dnn = c("Observed", "Predicted")
)
(misclass.log.nnet <- mean(pred.log.nnet != Y.valid)) ### Misclass rate
## misclass rate is terrible
## now to LDR
### For discriminant analysis, it's best to scale predictors
### to have mean 0 and SD 1 (this makes the results easier to
### interpret). We can do this using using the following function.
### Rescale x1 using the means and SDs of x2
scale.1 <- function(x1, x2) {
for (col in 1:ncol(x1)) {
a <- mean(x2[, col])
b <- sd(x2[, col])
x1[, col] <- (x1[, col] - a) / b
}
x1
}
X.train.DA <- scale.1(data.train[, -21], data.train[, -21])
X.valid.DA <- scale.1(data.valid[, -21], data.train[, -21])
### Fit an LDA model using the lda() funtion from the MASS package. This
### function uses predictor/response syntax.
fit.lda <- lda(X.train.DA, Y.train)
data.train
dim(data.train)
### first fit a logistic regression
library(nnet)
### need to rescale
rescale <- function(x1, x2) {
for (col in 1:ncol(x1)) {
a <- min(x2[, col])
b <- max(x2[, col])
x1[, col] <- (x1[, col] - a) / (b - a)
}
x1
}
p.train <- 0.75
n <- nrow(water)
n.train <- floor(p.train * n)
ind.random <- sample(1:n)
data.train <- water[ind.random <= n.train, ]
data.valid <- water[ind.random > n.train, ]
Y.valid <- data.valid[, 21]
data.train.scale <- data.train
data.valid.scale <- data.valid
data.train.scale[, -21] <- rescale(data.train.scale[, -21], data.train[, -21])
data.valid.scale[, -21] <- rescale(data.valid.scale[, -21], data.train[, -21])
X.train.scale <- as.matrix(data.train.scale[, -21])
Y.train <- data.train.scale[, 21]
X.valid.scale <- as.matrix(data.valid.scale[, -21])
Y.valid <- data.valid.scale[, 21]
fit.log.nnet <- multinom(is_safe ~ ., data = data.train.scale)
summary(fit.log.nnet)
### Next, let's investigate the LR's performance on the test set
pred.log.nnet <- predict(fit.log.nnet, data.valid.scale)
table(Y.valid, pred.log.nnet, ### Confusion matrix
dnn = c("Observed", "Predicted")
)
(misclass.log.nnet <- mean(pred.log.nnet != Y.valid)) ### Misclass rate
## misclass rate is terrible
## now to LDR
### For discriminant analysis, it's best to scale predictors
### to have mean 0 and SD 1 (this makes the results easier to
### interpret). We can do this using using the following function.
### Rescale x1 using the means and SDs of x2
scale.1 <- function(x1, x2) {
for (col in 1:ncol(x1)) {
a <- mean(x2[, col])
b <- sd(x2[, col])
x1[, col] <- (x1[, col] - a) / b
}
x1
}
X.train.DA <- scale.1(data.train[, -21], data.train[, -21])
X.valid.DA <- scale.1(data.valid[, -21], data.train[, -21])
### Fit an LDA model using the lda() funtion from the MASS package. This
### function uses predictor/response syntax.
fit.lda <- lda(X.train.DA, Y.train)
### We get predictions by extracting the class object from the predict()
### function's output.
pred.lda <- predict(fit.lda, X.valid.DA)$class
table(Y.valid, pred.lda, dnn = c("Obs", "Pred"))
(miss.lda <- mean(Y.valid != pred.lda))
table(Y.valid, pred.lda, dnn = c("Obs", "Pred"))
(miss.lda <- mean(Y.valid != pred.lda))
library(ROCR)
rocplot =function (pred , truth , ...){
predob = prediction (pred , truth)
perf = performance (predob , "tpr", "fpr")
plot(perf ,...)}
rocplot(pred.log.nnet, Y.valid)
rocplot =function(pred, truth, ...){
predob = prediction(pred, truth)
perf = performance (predob, "tpr", "fpr")
plot(perf,...)}
rocplot(pred.log.nnet, Y.valid)
pred.log.nnet
Y.valid
pred.log.nnet
rocplot(fit.log.nnet, Y.valid)
dim(pred.log.nnet)
head(pred.log.nnet)
head(Y.valid)
pred.log.nnet
rocplot(pred.log.nnet, Y.valid)
pred.lda
predict(fit.log.nnet, data.valid.scale)
prediction(fit.log.nnet, data.valid.scale)
?prediction
ROCR.simple
data(ROCR.simple)
ROCR.simple
prediction(pred.log.nnet, Y.valid)
class(ROCR.simple)
class(ROCR.simple$predictions)
class(ROCR.simple$labels)
dim(ROCR.simple$predictions)
head(pred.log.nnet)
head(ROCR.simple$predictions)
head(ROCR.simple$labels)
prediction(t(pred.log.nnet), t(Y.valid))
fitted.lr = attributes(predict(fit.log.nnet, data.train.scale, decision.values = TRUE))$decision.values
rocplot(fitted.lr, Y.valid)
fitted.lr
?attributes
?predict
fitted.lr = attributes(predict(fit.log.nnet, data.train.scale, decision.values = TRUE))
rocplot(fitted.lr, Y.valid)
?decision.values
??decision.values
fitted.lr = attributes(predict(fit.log.nnet, data.valid.scale, decision.values = TRUE))$decision.values
rocplot(fitted.lr, Y.valid)
fitted.lr
attributes(predict(fit.log.nnet, data.valid.scale, decision.values = TRUE))
predict(fit.log.nnet, data.valid.scale, decision.values = TRUE)
predict(fit.log.nnet, data.valid.scale)
remove(list=ls())
## water quality data, is_safe is a binary variable 0 or 1
# ammonia is incorrectly labeled as character
water = read.csv("waterQuality1.csv", header = TRUE)
summary(water)
water$ammonia = as.numeric(water$ammonia)
water$is_safe = as.numeric(water$is_safe)
water = na.omit(water)
# water$is_safe = as.factor(water$is_safe)
## might not need this line for ROC curves
summary(water)
water$ammonia[which(water$ammonia < 0)] = NA
water = na.omit(water)
summary(water)
dim(water)
### first fit a logistic regression
library(nnet)
### need to rescale
rescale <- function(x1, x2) {
for (col in 1:ncol(x1)) {
a <- min(x2[, col])
b <- max(x2[, col])
x1[, col] <- (x1[, col] - a) / (b - a)
}
x1
}
p.train <- 0.75
n <- nrow(water)
n.train <- floor(p.train * n)
ind.random <- sample(1:n)
data.train <- water[ind.random <= n.train, ]
data.valid <- water[ind.random > n.train, ]
Y.valid <- data.valid[, 21]
data.train.scale <- data.train
data.valid.scale <- data.valid
data.train.scale[, -21] <- rescale(data.train.scale[, -21], data.train[, -21])
data.valid.scale[, -21] <- rescale(data.valid.scale[, -21], data.train[, -21])
X.train.scale <- as.matrix(data.train.scale[, -21])
Y.train <- data.train.scale[, 21]
X.valid.scale <- as.matrix(data.valid.scale[, -21])
Y.valid <- data.valid.scale[, 21]
fit.log.nnet <- multinom(is_safe ~ ., data = data.train.scale)
summary(fit.log.nnet)
### Next, let's investigate the LR's performance on the test set
pred.log.nnet <- predict(fit.log.nnet, data.valid.scale)
table(Y.valid, pred.log.nnet, ### Confusion matrix
dnn = c("Observed", "Predicted")
)
(misclass.log.nnet <- mean(pred.log.nnet != Y.valid)) ### Misclass rate
## misclass rate is terrible
## now to LDR
### For discriminant analysis, it's best to scale predictors
### to have mean 0 and SD 1 (this makes the results easier to
### interpret). We can do this using using the following function.
### Rescale x1 using the means and SDs of x2
scale.1 <- function(x1, x2) {
for (col in 1:ncol(x1)) {
a <- mean(x2[, col])
b <- sd(x2[, col])
x1[, col] <- (x1[, col] - a) / b
}
x1
}
X.train.DA <- scale.1(data.train[, -21], data.train[, -21])
X.valid.DA <- scale.1(data.valid[, -21], data.train[, -21])
### Fit an LDA model using the lda() funtion from the MASS package. This
### function uses predictor/response syntax.
fit.lda <- lda(X.train.DA, Y.train)
### We get predictions by extracting the class object from the predict()
### function's output.
pred.lda <- predict(fit.lda, X.valid.DA)$class
table(Y.valid, pred.lda, dnn = c("Obs", "Pred"))
(miss.lda <- mean(Y.valid != pred.lda))
table(Y.valid, pred.lda, dnn = c("Obs", "Pred"))
(miss.lda <- mean(Y.valid != pred.lda))
fitted.lr = attributes(predict(fit.log.nnet, data.valid.scale, decision.values = TRUE))$decision.values
rocplot(fitted.lr, Y.valid)
### attempting ROC Curve
library(ROCR)
rocplot =function(pred, truth, ...){
predob = prediction(pred, truth)
perf = performance (predob, "tpr", "fpr")
plot(perf,...)}
fitted.lr = attributes(predict(fit.log.nnet, data.valid.scale, decision.values = TRUE))$decision.values
rocplot(fitted.lr, Y.valid)
fitted.lr
remove(list=ls())
## water quality data, is_safe is a binary variable 0 or 1
# ammonia is incorrectly labeled as character
water = read.csv("waterQuality1.csv", header = TRUE)
summary(water)
water$ammonia = as.numeric(water$ammonia)
water$is_safe = as.numeric(water$is_safe)
water = na.omit(water)
water$is_safe = as.factor(water$is_safe)
## might not need this line for ROC curves
summary(water)
water$ammonia[which(water$ammonia < 0)] = NA
water = na.omit(water)
summary(water)
dim(water)
### first fit a logistic regression
library(nnet)
### need to rescale
rescale <- function(x1, x2) {
for (col in 1:ncol(x1)) {
a <- min(x2[, col])
b <- max(x2[, col])
x1[, col] <- (x1[, col] - a) / (b - a)
}
x1
}
p.train <- 0.75
n <- nrow(water)
n.train <- floor(p.train * n)
ind.random <- sample(1:n)
data.train <- water[ind.random <= n.train, ]
data.valid <- water[ind.random > n.train, ]
Y.valid <- data.valid[, 21]
data.train.scale <- data.train
data.valid.scale <- data.valid
data.train.scale[, -21] <- rescale(data.train.scale[, -21], data.train[, -21])
data.valid.scale[, -21] <- rescale(data.valid.scale[, -21], data.train[, -21])
X.train.scale <- as.matrix(data.train.scale[, -21])
Y.train <- data.train.scale[, 21]
X.valid.scale <- as.matrix(data.valid.scale[, -21])
Y.valid <- data.valid.scale[, 21]
fit.log.nnet <- multinom(is_safe ~ ., data = data.train.scale)
summary(fit.log.nnet)
### Next, let's investigate the LR's performance on the test set
pred.log.nnet <- predict(fit.log.nnet, data.valid.scale)
table(Y.valid, pred.log.nnet, ### Confusion matrix
dnn = c("Observed", "Predicted")
)
(misclass.log.nnet <- mean(pred.log.nnet != Y.valid)) ### Misclass rate
## misclass rate is terrible
## now to LDR
### For discriminant analysis, it's best to scale predictors
### to have mean 0 and SD 1 (this makes the results easier to
### interpret). We can do this using using the following function.
### Rescale x1 using the means and SDs of x2
scale.1 <- function(x1, x2) {
for (col in 1:ncol(x1)) {
a <- mean(x2[, col])
b <- sd(x2[, col])
x1[, col] <- (x1[, col] - a) / b
}
x1
}
X.train.DA <- scale.1(data.train[, -21], data.train[, -21])
X.valid.DA <- scale.1(data.valid[, -21], data.train[, -21])
### Fit an LDA model using the lda() funtion from the MASS package. This
### function uses predictor/response syntax.
fit.lda <- lda(X.train.DA, Y.train)
### We get predictions by extracting the class object from the predict()
### function's output.
pred.lda <- predict(fit.lda, X.valid.DA)$class
table(Y.valid, pred.lda, dnn = c("Obs", "Pred"))
(miss.lda <- mean(Y.valid != pred.lda))
table(Y.valid, pred.lda, dnn = c("Obs", "Pred"))
(miss.lda <- mean(Y.valid != pred.lda))
rocplot(as.numeric(pred.log.nnet), as.numeric(Y.valid)
rocplot(as.numeric(pred.log.nnet), as.numeric(Y.valid))
rocplot(as.numeric(pred.log.nnet), as.numeric(Y.valid))
### attempting ROC Curve
library(ROCR)
rocplot =function(pred, truth, ...){
predob = prediction(pred, truth)
perf = performance (predob, "tpr", "fpr")
plot(perf,...)}
fitted.lr = attributes(predict(fit.log.nnet, data.valid.scale, decision.values = TRUE))$decision.values
rocplot(as.numeric(pred.log.nnet), as.numeric(Y.valid))
remove(list=ls())
## water quality data, is_safe is a binary variable 0 or 1
# ammonia is incorrectly labeled as character
water = read.csv("waterQuality1.csv", header = TRUE)
summary(water)
water$ammonia = as.numeric(water$ammonia)
water$is_safe = as.numeric(water$is_safe)
water = na.omit(water)
# water$is_safe = as.factor(water$is_safe)
## might not need this line for ROC curves
summary(water)
water$ammonia[which(water$ammonia < 0)] = NA
water = na.omit(water)
summary(water)
dim(water)
### first fit a logistic regression
library(nnet)
### need to rescale
rescale <- function(x1, x2) {
for (col in 1:ncol(x1)) {
a <- min(x2[, col])
b <- max(x2[, col])
x1[, col] <- (x1[, col] - a) / (b - a)
}
x1
}
p.train <- 0.75
n <- nrow(water)
n.train <- floor(p.train * n)
ind.random <- sample(1:n)
data.train <- water[ind.random <= n.train, ]
data.valid <- water[ind.random > n.train, ]
Y.valid <- data.valid[, 21]
data.train.scale <- data.train
data.valid.scale <- data.valid
data.train.scale[, -21] <- rescale(data.train.scale[, -21], data.train[, -21])
data.valid.scale[, -21] <- rescale(data.valid.scale[, -21], data.train[, -21])
X.train.scale <- as.matrix(data.train.scale[, -21])
Y.train <- data.train.scale[, 21]
X.valid.scale <- as.matrix(data.valid.scale[, -21])
Y.valid <- data.valid.scale[, 21]
fit.log.nnet <- multinom(is_safe ~ ., data = data.train.scale)
summary(fit.log.nnet)
### Next, let's investigate the LR's performance on the test set
pred.log.nnet <- predict(fit.log.nnet, data.valid.scale)
table(Y.valid, pred.log.nnet, ### Confusion matrix
dnn = c("Observed", "Predicted")
)
(misclass.log.nnet <- mean(pred.log.nnet != Y.valid)) ### Misclass rate
## misclass rate is terrible
## now to LDR
### For discriminant analysis, it's best to scale predictors
### to have mean 0 and SD 1 (this makes the results easier to
### interpret). We can do this using using the following function.
### Rescale x1 using the means and SDs of x2
scale.1 <- function(x1, x2) {
for (col in 1:ncol(x1)) {
a <- mean(x2[, col])
b <- sd(x2[, col])
x1[, col] <- (x1[, col] - a) / b
}
x1
}
X.train.DA <- scale.1(data.train[, -21], data.train[, -21])
X.valid.DA <- scale.1(data.valid[, -21], data.train[, -21])
### Fit an LDA model using the lda() funtion from the MASS package. This
### function uses predictor/response syntax.
fit.lda <- lda(X.train.DA, Y.train)
### We get predictions by extracting the class object from the predict()
### function's output.
pred.lda <- predict(fit.lda, X.valid.DA)$class
table(Y.valid, pred.lda, dnn = c("Obs", "Pred"))
(miss.lda <- mean(Y.valid != pred.lda))
table(Y.valid, pred.lda, dnn = c("Obs", "Pred"))
(miss.lda <- mean(Y.valid != pred.lda))
### attempting ROC Curve
library(ROCR)
rocplot =function(pred, truth, ...){
predob = prediction(pred, truth)
perf = performance (predob, "tpr", "fpr")
plot(perf,...)}
fitted.lr = attributes(predict(fit.log.nnet, data.valid.scale, decision.values = TRUE))$decision.values
rocplot(pred.log.nnet, Y.valid)
pred.log.nnet
remove(list=ls())
## water quality data, is_safe is a binary variable 0 or 1
# ammonia is incorrectly labeled as character
water = read.csv("waterQuality1.csv", header = TRUE)
summary(water)
water$ammonia = as.numeric(water$ammonia)
water$is_safe = as.numeric(water$is_safe)
water = na.omit(water)
water$is_safe = as.factor(water$is_safe)
## might not need this line for ROC curves
summary(water)
water$ammonia[which(water$ammonia < 0)] = NA
water = na.omit(water)
summary(water)
dim(water)
### first fit a logistic regression
library(nnet)
### need to rescale
rescale <- function(x1, x2) {
for (col in 1:ncol(x1)) {
a <- min(x2[, col])
b <- max(x2[, col])
x1[, col] <- (x1[, col] - a) / (b - a)
}
x1
}
p.train <- 0.75
n <- nrow(water)
n.train <- floor(p.train * n)
ind.random <- sample(1:n)
data.train <- water[ind.random <= n.train, ]
data.valid <- water[ind.random > n.train, ]
Y.valid <- data.valid[, 21]
data.train.scale <- data.train
data.valid.scale <- data.valid
data.train.scale[, -21] <- rescale(data.train.scale[, -21], data.train[, -21])
data.valid.scale[, -21] <- rescale(data.valid.scale[, -21], data.train[, -21])
X.train.scale <- as.matrix(data.train.scale[, -21])
Y.train <- data.train.scale[, 21]
X.valid.scale <- as.matrix(data.valid.scale[, -21])
Y.valid <- data.valid.scale[, 21]
fit.log.nnet <- multinom(is_safe ~ ., data = data.train.scale)
summary(fit.log.nnet)
### Next, let's investigate the LR's performance on the test set
pred.log.nnet <- predict(fit.log.nnet, data.valid.scale)
table(Y.valid, pred.log.nnet, ### Confusion matrix
dnn = c("Observed", "Predicted")
)
(misclass.log.nnet <- mean(pred.log.nnet != Y.valid)) ### Misclass rate
## misclass rate is terrible
## now to LDR
### For discriminant analysis, it's best to scale predictors
### to have mean 0 and SD 1 (this makes the results easier to
### interpret). We can do this using using the following function.
### Rescale x1 using the means and SDs of x2
scale.1 <- function(x1, x2) {
for (col in 1:ncol(x1)) {
a <- mean(x2[, col])
b <- sd(x2[, col])
x1[, col] <- (x1[, col] - a) / b
}
x1
}
X.train.DA <- scale.1(data.train[, -21], data.train[, -21])
X.valid.DA <- scale.1(data.valid[, -21], data.train[, -21])
### Fit an LDA model using the lda() funtion from the MASS package. This
### function uses predictor/response syntax.
fit.lda <- lda(X.train.DA, Y.train)
### We get predictions by extracting the class object from the predict()
### function's output.
pred.lda <- predict(fit.lda, X.valid.DA)$class
table(Y.valid, pred.lda, dnn = c("Obs", "Pred"))
(miss.lda <- mean(Y.valid != pred.lda))
table(Y.valid, pred.lda, dnn = c("Obs", "Pred"))
(miss.lda <- mean(Y.valid != pred.lda))
### attempting ROC Curve
library(ROCR)
rocplot =function(pred, truth, ...){
predob = prediction(pred, truth)
perf = performance (predob, "tpr", "fpr")
plot(perf,...)}
fitted.lr = attributes(predict(fit.log.nnet, data.valid.scale, decision.values = TRUE))$decision.values
rocplot(as.numeric(pred.log.nnet), as.numeric(Y.valid))
rocplot(as.numeric(pred.lda), as.numeric(Y.valid))
rocplot(as.list(pred.log.nnet), as.list(Y.valid))
