# Gradient Boosted Trees using gbm
# Prostate data

prostate <- read.table("Datasets/Prostate.csv",
  header = TRUE, sep = ",", na.strings = " "
)

library(gbm)
####################################################################
## Gradient boosting through gbm() allows different models to
#    be used for boosting.  To fit regression trees with RSS
#    for splits, specify, "distribution=gaussian"
## Tuning parameters and defaults include
# n.trees=100: number of trees (=B). Usually need to increase this
# interaction.depth = 1: Relates to d. Default implies additive model.
#    USUALLY need to increase this.
# shrinkage = .001: The learning rate parameter "lambda"
#
### Optional tuning parameters
# bag.fraction=0.5: The subsampling fraction
# n.minobsinnode=10: number of obs in terminal nodes
#         Tuning this can improve performance, especially making it smaller
#         in smaller data sets.
####################################################################

# Only 2 variables, so trying small tree size, lots of trees
# Run these two functions several times to see change in CV suggestion
# Doing 10,000 trees because prior runs with fewer suggested need for more
prb.1_.001 <- gbm(
  data = prostate, lpsa ~ lcavol + pgg45, distribution = "gaussian",
  n.trees = 10000, interaction.depth = 1, shrinkage = 0.001,
  bag.fraction = 0.8
)

# par(mfrow = c(1, 2))
gbm.perf(prb.1_.001, method = "OOB", oobag.curve = FALSE)
gbm.perf(prb.1_.001, method = "OOB", oobag.curve = TRUE)
gbm.perf(prb.1_.001, method = "OOB", plot.it = FALSE) # No Plot

### NOTE: Can add trees using gbm.more(), in case you have a long-running
#    function and need more trees.
# Need to include "keep.data=TRUE" if you plan to do this

# Variable Importance
summary(prb.1_.001)


## Use CV to estimate B instead

gbm_cv <- gbm(
  data = prostate, lpsa ~ lcavol + pgg45, distribution = "gaussian",
  n.trees = 10000, interaction.depth = 1, shrinkage = 0.001,
  bag.fraction = 0.8, cv.folds = 5
)

gbm.perf(gbm_cv, method = "cv")


# 3D plot of surface

library(rgl)
x1 <- seq(from = -2, to = 4, by = .05)
x2 <- seq(from = 0, to = 100, by = .5)
xy1 <- data.frame(expand.grid(lcavol = x1, pgg45 = x2))

# predict.gbm() needs to know how many trees to use
### Convenient! (you can fit too many but base predictions on a smaller number)
# I am using a hack that doubles the number of treed that OOB gives me.
#  I made this up, but it comes closer to what CV would estimate is optimal.

opt.trees <- gbm.perf(prb.1_.001, method = "OOB", plot.it = FALSE) * 2

surface <- matrix(predict(prb.1_.001,
  newdata = xy1, n.trees = opt.trees,
  type = "response"
), nrow = length(x1))

open3d()
persp3d(
  x = x1, y = x2,
  z = surface, col = "orange", xlab = "lcavol", ylab = "pgg45",
  zlab = "Predicted lpsa"
)
points3d(prostate$lpsa ~ prostate$lcavol + prostate$pgg45, col = "blue")

####################################################################
# Repeat, increasing the tree depth to 6,
#        Due to limited sample size, increasing interaction.depth
#        to allow larger trees
####################################################################

prb.6_.001 <- gbm(
  data = prostate, lpsa ~ lcavol + pgg45, distribution = "gaussian",
  n.trees = 10000, interaction.depth = 6, shrinkage = 0.001,
  bag.fraction = 0.8
)

gbm.perf(prb.6_.001, method = "OOB", oobag.curve = FALSE)
gbm.perf(prb.6_.001, method = "OOB", oobag.curve = TRUE)
gbm.perf(prb.6_.001, method = "OOB", plot.it = FALSE) # No Plot

# Variable Importance
summary(prb.6_.001)

# 3D plot of surface

library(rgl)
x1 <- seq(from = -2, to = 4, by = .05)
x2 <- seq(from = 0, to = 100, by = .5)
xy1 <- data.frame(expand.grid(lcavol = x1, pgg45 = x2))

# predict.gbm() needs to know how many trees to use
### Convenient! (you can fit too many but base predictions on a smaller number)
# I am using a hack that doubles the number of treed that OOB gives me.
#  I made this up, but it comes closer to what CV would estimate is optimal.

opt.trees <- gbm.perf(prb.6_.001, method = "OOB", plot.it = FALSE) * 2

surface <- matrix(predict(prb.6_.001,
  newdata = xy1, n.trees = opt.trees,
  type = "response"
), nrow = length(x1))

open3d()
persp3d(
  x = x1, y = x2,
  z = surface, col = "orange", xlab = "lcavol", ylab = "pgg45",
  zlab = "Predicted lpsa"
)
points3d(prostate$lpsa ~ prostate$lcavol + prostate$pgg45, col = "blue")


####################################################################
# Repeat, increasing learning rqate (larger shrinkage lambda)
####################################################################

prb.6_.1 <- gbm(
  data = prostate, lpsa ~ lcavol + pgg45, distribution = "gaussian",
  n.trees = 10000, interaction.depth = 6, shrinkage = 0.1,
  bag.fraction = 0.8
)

gbm.perf(prb.6_.1, method = "OOB", oobag.curve = FALSE)
gbm.perf(prb.6_.1, method = "OOB", oobag.curve = TRUE)
gbm.perf(prb.6_.1, method = "OOB", plot.it = FALSE) # No Plot

# Variable Importance
summary(prb.6_.001)

# 3D plot of surface

library(rgl)
x1 <- seq(from = -2, to = 4, by = .05)
x2 <- seq(from = 0, to = 100, by = .5)
xy1 <- data.frame(expand.grid(lcavol = x1, pgg45 = x2))

# predict.gbm() needs to know how many trees to use
### Convenient! (you can fit too many but base predictions on a smaller number)
# I am using a hack that doubles the number of treed that OOB gives me.
#  I made this up, but it comes closer to what CV would estimate is optimal.

opt.trees <- gbm.perf(prb.6_.1, method = "OOB", plot.it = FALSE) * 2

surface <- matrix(predict(prb.6_.1,
  newdata = xy1, n.trees = opt.trees,
  type = "response"
), nrow = length(x1))

open3d()
persp3d(
  x = x1, y = x2,
  z = surface, col = "orange", xlab = "lcavol", ylab = "pgg45",
  zlab = "Predicted lpsa"
)
points3d(prostate$lpsa ~ prostate$lcavol + prostate$pgg45, col = "blue")



###############################################################
## Now let's look at entire data set, adding tuning

#  Let's do R=2 reps of 5-fold CV.
set.seed(182674455)
V <- 5
R <- 2
n2 <- nrow(prostate)
# Create the folds and save in a matrix
folds <- matrix(NA, nrow = n2, ncol = R)
for (r in 1:R) {
  folds[, r] <- floor((sample.int(n2) - 1) * V / n2) + 1
}

shr <- c(.001, .005, .025, .125)
dep <- c(2, 4, 6)
### Second grid
# dep = c(1,2,3,4)
# shr = c(0.0025, 0.005, 0.0075, 0.01)
trees <- 10000

NS <- length(shr)
ND <- length(dep)
gb.cv <- matrix(NA, nrow = ND * NS, ncol = V * R)
opt.tree <- matrix(NA, nrow = ND * NS, ncol = V * R)

qq <- 1
for (r in 1:R) {
  for (v in 1:V) {
    pro.train <- prostate[folds[, r] != v, ]
    pro.test <- prostate[folds[, r] == v, ]
    counter <- 1
    for (d in dep) {
      for (s in shr) {
        pro.gbm <- gbm(
          data = pro.train, lpsa ~ ., distribution = "gaussian",
          n.trees = trees, interaction.depth = d, shrinkage = s,
          bag.fraction = 0.8
        )
        treenum <- min(trees, 2 * gbm.perf(pro.gbm, method = "OOB", 
                                           plot.it = FALSE))
        opt.tree[counter, qq] <- treenum
        preds <- predict(pro.gbm, newdata = pro.test, n.trees = treenum)
        gb.cv[counter, qq] <- mean((preds - pro.test$lpsa)^2)
        counter <- counter + 1
      }
    }
    qq <- qq + 1
  }
}

parms <- expand.grid(shr, dep)
row.names(gb.cv) <- paste(parms[, 2], parms[, 1], sep = "|")
row.names(opt.tree) <- paste(parms[, 2], parms[, 1], sep = "|")

opt.tree
gb.cv

(mean.tree <- apply(opt.tree, 1, mean))
(mean.cv <- sqrt(apply(gb.cv, 1, mean)))
min.cv <- apply(gb.cv, 2, min)

boxplot(sqrt(gb.cv), use.cols = FALSE, las = 2)

boxplot(sqrt(t(gb.cv) / min.cv),
  use.cols = TRUE, las = 2,
  main = "GBM Fine-Tuning Variables and Node Sizes"
)

##############################################################
# Refit Final Model

pro.opt <- gbm(
  data = prostate, lpsa ~ ., distribution = "gaussian",
  n.trees = 500, interaction.depth = 2, shrinkage = 0.0075,
  bag.fraction = 0.8
)

# Variable Importance
summary(pro.opt)
