### In this tutorial, we are starting classification. We will still use
### the wine data, but now we will be predicting quality (specifically,
### we group quality values into high, medium and low). For now, we will
### look at nearest-neighbours.

library(FNN)

### Read-in and process the data
source("Read_Wine_Data_Class.R")
head(data)
summary(data)
table(data$quality)

### Some of our code will be random, so we have to set the seed.
### Use Mersenne-Twister for compatibility.
set.seed(46536737, kind = "Mersenne-Twister")


### Split the data into training and validation sets
p.train <- 0.75
n <- nrow(data)
n.train <- floor(p.train * n)

ind.random <- sample(1:n)
data.train <- data[ind.random <= n.train, ]
data.valid <- data[ind.random > n.train, ]

### Split up the predictor variables from the class labels.
X.train.raw <- data.train[, -1]
X.valid.raw <- data.valid[, -1]
Y.train <- data.train[, 1]
Y.valid <- data.valid[, 1]


### KNN is based on distances. If variables are measured on different
### scales, we can change which points are neighbours by measuring
### in different units.
### a function we can use to rescale the columns of
### a data frame to have mean 0 and SD 1. We can also use it to rescale
### a data frame based on the means and SDs of another (this is useful
### for scaling the validation set to match the training set).

### Rescale x1 using the means and SDs of x2
scale.1 <- function(x1, x2) {
  for (col in 1:ncol(x1)) {
    a <- mean(x2[, col])
    b <- sd(x2[, col])
    x1[, col] <- (x1[, col] - a) / b
  }
  x1
}

### Rescale our training and validation sets
X.train <- scale.1(X.train.raw, X.train.raw)
X.valid <- scale.1(X.valid.raw, X.train.raw) # Watch the order


### Now we can fit a KNN model using the knn function in the FNN
### package. The syntax of the knn function is a bit different from
### what we're used to. The first two inputs are the training and
### validation predictor matrices. The third input is the class labels
### for the training set. We can also set k to the number of
### neighbours we want. The function then outputs predicted class
### labels for the validation set. Let's use 3 neighbours.
pred.knn <- knn(X.train, X.valid, Y.train, k = 3)

### Let's make a confusion matrix. We get this using the table()
### function and providing both the predicted and true class labels
### for the validation set. We can also set the axis labels using
### the dnn input.
table(pred.knn, Y.valid, dnn = c("Predicted", "Observed"))

### Next, let's get the misclassification rate
(misclass.knn <- mean(pred.knn != Y.valid))


### That's a pretty bad error rate. Can we do any better by tuning the
### number of neighbours? Fortunately, the knn.cv() function
### automatically computes CV misclassification rates on a training set.
### This function does leave-one-out CV, and returns the predicted class
### at each point based on the rest of the dataset.

K.max <- 200 # Maximum number of neighbours

### Container to store CV misclassification rates
mis.CV <- rep(0, times = K.max)

for (i in 1:K.max) {
  ### Progress update
  print(paste0(i, " of ", K.max))

  ### Fit leave-one-out CV
  this.knn <- knn.cv(X.train, Y.train, k = i)

  ### Get and store CV misclassification rate
  this.mis.CV <- mean(this.knn != Y.train)
  mis.CV[i] <- this.mis.CV
}

### Get SEs
SE.mis.CV <- sapply(mis.CV, function(r) {
  sqrt(r * (1 - r) / nrow(X.train))
})


### Plot CV misclassification rates
plot(1:K.max, mis.CV,
  xlab = "K", ylab = "Misclassification Rate",
  ylim = c(0.4, 0.64)
)

### Add +/- 1 SE error bars. The easiest way to do this is with the
### lines() function. For each value of K, create a line joining
### the bottom to the top of our error bar
for (i in 1:K.max) {
  lower <- mis.CV[i] - SE.mis.CV[i]
  upper <- mis.CV[i] + SE.mis.CV[i]

  lines(x = c(i, i), y = c(lower, upper))
}

### Get CV min value for K
k.min <- which.min(mis.CV)

### Add a horizontal line at min + 1SE
thresh <- mis.CV[k.min] + SE.mis.CV[k.min]
abline(h = thresh, col = "red")

### Get CV 1SE value for K
k.1se <- max(which(mis.CV <= thresh))


### Finally, let's see how our tuned KNN models do
knn.min <- knn(X.train, X.valid, Y.train, k.min)
knn.1se <- knn(X.train, X.valid, Y.train, k.1se)

table(knn.min, Y.valid, dnn = c("Predicted", "Observed"))
table(knn.1se, Y.valid, dnn = c("Predicted", "Observed"))

(mis.min <- mean(Y.valid != knn.min))
(mis.1se <- mean(Y.valid != knn.1se))
