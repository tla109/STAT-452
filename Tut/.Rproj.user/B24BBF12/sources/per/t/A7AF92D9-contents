---
title: "Homework 1"
author: "Tianyu Liu (301249861)"
date: "2023-09-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1

I have read and acknowledge the SFU student academic integrity policy.

# 2 (Q2 from Problem Set 3)

## (a)

### i
There would be less bias, since with a reduced curvature the linear regression would have a better fit than if there is more curvature.

### ii
The variance would be about the same, because Figure 6 is also using a linear fit with n = 10 so the variability would be about the same.

## (b)
The variance gets smaller as we increase sample size, because the larger the sample, the more likely it is that errors average out above and below the true mean. The bias would not change with increasing sample size because when approximating a curve with a line the bias always exists.

# 3 (Application, from Problem Set 4)
## 1
```{r}
AQ <- na.omit(airquality[,1:4])
dim(AQ)
head(AQ)
pairs(AQ)
```

## 2
```{r}
set.seed(4099183)
n = nrow(AQ)
reorder = sample.int(n)
size.train <- floor(n * 0.75) ### Number of observations in our training
### set. Use floor() to round down
ind.train <- reorder[1:size.train] ### Indices of observations
### to put in training set
ind.valid <- reorder[(size.train + 1):n] ### Indices of observations
### to put in validation set
data.train <- AQ[ind.train,] ### Keep only observations in ind.train
data.valid <- AQ[ind.valid,] ### Keep only observations in ind.valid
print(ind.valid)
```
These are the validation indices.

## 3
```{r}
### Fit linear models to predict Ozone using each predictor
### individually, all predictors together, and all interactions with curvature
### Note: These models must be fit using data.train so that we can
### evaluate their MSPE on data.valid
fit.temp <- lm(Ozone ~ Temp, data = data.train)
fit.wind <- lm(Ozone ~ Wind, data = data.train)
fit.solar <- lm(Ozone ~ Solar.R, data = data.train)
fit.all <- lm(Ozone ~ Temp + Wind + Solar.R, data = data.train)
fit.intcurv <- lm(Ozone ~ Temp + Wind + Solar.R + I(Temp^2) + I(Wind^2) + I(Solar.R^2)
+ Temp*Wind + Temp*Solar.R + Wind*Solar.R, data = data.train)

### Get predictions on the validation set for each model using the
### predict() function.
pred.temp <- predict(fit.temp, data.valid)
pred.wind <- predict(fit.wind, data.valid)
pred.solar <- predict(fit.solar, data.valid)
pred.all <- predict(fit.all, data.valid)
pred.intcurv <- predict(fit.intcurv, data.valid)

### When we calculate validation set MSPEs for our models, we will
### end up repeating the same calculation 5 times. Let's make a
### function to do this for us.
get.MSPE <- function(Y, Y.hat) {
  residuals <- Y - Y.hat
  resid.sq <- residuals^2
  SSPE <- sum(resid.sq)
  MSPE <- SSPE / length(Y)
  return(MSPE)
}

### Use our get.MSPE() function to calculate the validation set MSPE
### of each model
Y.valid <- data.valid$Ozone
MSPE.temp <- get.MSPE(Y.valid, pred.temp)
MSPE.wind <- get.MSPE(Y.valid, pred.wind)
MSPE.solar <- get.MSPE(Y.valid, pred.solar)
MSPE.all <- get.MSPE(Y.valid, pred.all)
MSPE.intcurv <- get.MSPE(Y.valid, pred.intcurv)

### Let's compare these validation set MSPEs
print(MSPE.temp)
print(MSPE.wind)
print(MSPE.solar)
print(MSPE.all)
print(MSPE.intcurv)
```

As expected, the most complicated model wins the competition (The one with interactions and curvature).

## 4
```{r}
### We need to divide the dataset into 5 folds.
### We can do this by randomly sampling the numbers from 1 to 10 and
### attaching these to our dataset as fold labels
M = 5
n.fold <- n / M # Number of observations in each fold
n.fold <- ceiling(n.fold) # Round up to make sure we get enough labels
# We can remove any excess later
ordered.ids <- rep(1:M, times = n.fold)
ordered.ids <- ordered.ids[1:n] # Remove excess label(s)
shuffle <- sample.int(n) # Randomly permute the numbers 1 to n
shuffled.ids <- ordered.ids[shuffle] # Use shuffle to permute
# the fold labels
data.CV <- AQ # Create a copy of our dataset
data.CV$fold <- shuffled.ids # Add a column to our new dataset containing
# the fold labels


### Next, let's actually do the cross validation. This will be easier
### with a for loop than with the replicate function. First, we will
### need to make an array to store the MSPEs
CV.MSPEs <- array(0, dim = c(M, 5))
colnames(CV.MSPEs) <- c("temp", "wind", "solar", "all", "intcurv") 
### Set model names

library(dplyr)
for (i in 1:M) {
  ### Use fold i for validation and the rest for training
  data.train <- filter(data.CV, fold != i)
  data.valid <- filter(data.CV, fold == i)

  ### Remove fold from training and validation sets since it
  ### isn't a real predictor
  data.train <- select(data.train, -fold)
  data.valid <- select(data.valid, -fold)

  ### Fit linear models to predict Ozone using each predictor
  ### individually, all predictors together, and all interactions
  ### Note: These models must be fit using data.train so that we can
  ### evaluate their MSPE on data.valid
  fit.temp <- lm(Ozone ~ Temp, data = data.train)
  fit.wind <- lm(Ozone ~ Wind, data = data.train)
  fit.solar <- lm(Ozone ~ Solar.R, data = data.train)
  fit.all <- lm(Ozone ~ Temp + Wind + Solar.R, data = data.train)
  fit.intcurv <- lm(Ozone ~ Temp + Wind + Solar.R + I(Temp^2) + I(Wind^2) + I(Solar.R^2)
  + Temp*Wind + Temp*Solar.R + Wind*Solar.R, data = data.train)


  ### Get predictions on the validation set for each model using the
  ### predict() function.
  pred.temp <- predict(fit.temp, data.valid)
  pred.wind <- predict(fit.wind, data.valid)
  pred.solar <- predict(fit.solar, data.valid)
  pred.all <- predict(fit.all, data.valid)
  pred.intcurv <- predict(fit.intcurv, data.valid)

  ### Use our get.MSPE() function to calculate the validation set MSPE
  ### of each model
  Y.valid <- data.valid$Ozone
  MSPE.temp <- get.MSPE(Y.valid, pred.temp)
  MSPE.wind <- get.MSPE(Y.valid, pred.wind)
  MSPE.solar <- get.MSPE(Y.valid, pred.solar)
  MSPE.all <- get.MSPE(Y.valid, pred.all)
  MSPE.intcurv <- get.MSPE(Y.valid, pred.intcurv)

  ### Store MSPEs
  CV.MSPEs[i, 1] <- MSPE.temp
  CV.MSPEs[i, 2] <- MSPE.wind
  CV.MSPEs[i, 3] <- MSPE.solar
  CV.MSPEs[i, 4] <- MSPE.all
  CV.MSPEs[i, 5] <- MSPE.intcurv
}

MSPE.mean <- apply(X = CV.MSPEs, MARGIN = 2 , FUN = mean)
MSPE.mean #means

MSPE.sd <- apply(X = CV.MSPEs, MARGIN = 2 , FUN = sd)
MSPE.CIl <- MSPE.mean - qt (p = .975 , df = M-1) * MSPE.sd/sqrt(M)
MSPE.CIu <- MSPE.mean + qt (p = .975 , df = M-1) * MSPE.sd/sqrt(M)
round(cbind(MSPE.CIl,MSPE.CIu), 2) #confidence intervals

```
The model involving solar has a very high MSPE, while the model involving temp and all
predictors have high variance. Taking a balance between variability and the actual
MSPE, wind seems like a much better model.
Only the model for interaction and curvature has low variance and low MSPE. It's clearly
the best model but that's intuitive since it considers all the factors.

## 5
```{r}
n.rep <- 20 # Number of times to repeat CV

### First, we need a container to store the average CV errors
ave.CV.MSPEs <- array(0, dim = c(n.rep, 5))
colnames(ave.CV.MSPEs) <- colnames(CV.MSPEs)

### We will put the entire CV section from above inside another
### for loop. This will repeat the entire CV process
for (j in 1:n.rep) {
  ordered.ids <- rep(1:M, times = n.fold)
  ordered.ids <- ordered.ids[1:n]
  shuffle <- sample.int(n)
  shuffled.ids <- ordered.ids[shuffle]

  data.CV <- AQ
  data.CV$fold <- shuffled.ids

  CV.MSPEs <- array(0, dim = c(M, 5))
  colnames(CV.MSPEs) <- c("temp", "wind", "solar", "all", "intcurv")

  for (i in 1:M) {
    data.train <- filter(data.CV, fold != i)
    data.valid <- filter(data.CV, fold == i)

    ### In tutorial, I was getting an error because I wrote -folds
    ### instead of -fold. Whoops!
    data.train <- select(data.train, -fold)
    data.valid <- select(data.valid, -fold)

    fit.temp <- lm(Ozone ~ Temp, data = data.train)
    fit.wind <- lm(Ozone ~ Wind, data = data.train)
    fit.solar <- lm(Ozone ~ Solar.R, data = data.train)
    fit.all <- lm(Ozone ~ Temp + Wind + Solar.R, data = data.train)
    fit.intcurv <- lm(Ozone ~ Temp + Wind + Solar.R + I(Temp^2) + I(Wind^2) + I(Solar.R^2)
    + Temp*Wind + Temp*Solar.R + Wind*Solar.R, data = data.train)

    pred.temp <- predict(fit.temp, data.valid)
    pred.wind <- predict(fit.wind, data.valid)
    pred.solar <- predict(fit.solar, data.valid)
    pred.all <- predict(fit.all, data.valid)
    pred.intcurv <- predict(fit.intcurv, data.valid)

    Y.valid <- data.valid$Ozone
    MSPE.temp <- get.MSPE(Y.valid, pred.temp)
    MSPE.wind <- get.MSPE(Y.valid, pred.wind)
    MSPE.solar <- get.MSPE(Y.valid, pred.solar)
    MSPE.all <- get.MSPE(Y.valid, pred.all)
    MSPE.intcurv <- get.MSPE(Y.valid, pred.intcurv)

    CV.MSPEs[i, 1] <- MSPE.temp
    CV.MSPEs[i, 2] <- MSPE.wind
    CV.MSPEs[i, 3] <- MSPE.solar
    CV.MSPEs[i, 4] <- MSPE.all
    CV.MSPEs[i, 5] <- MSPE.intcurv
  }

  ### We now have MSPEs for each fold of one iteration of CV. Let's
  ### get the average error across these folds (think of each fold
  ### as a data split), and store the result in ave.CV.MSPEs
  this.ave.MSPEs <- apply(CV.MSPEs, 2, mean)
  ave.CV.MSPEs[j, ] <- this.ave.MSPEs # We are replacing a whole
  # row at once
}

```
### (a)
```{r}
boxplot(ave.CV.MSPEs,
  main = "Boxplot of 20 Replicates of Average 5-Fold CV Error"
)
```

The models for all variables and the interactions are still good, but the model for temp is also decent, with low variance compared to other models. The model for wind and solar are much worse.

### (b)
```{r}

rel.ave.CV.MSPEs <- apply(ave.CV.MSPEs, 1, function(W) {
  best <- min(W)
  return(W / best)
})
rel.ave.CV.MSPEs <- t(rel.ave.CV.MSPEs)

boxplot(rel.ave.CV.MSPEs,
  main = "Boxplot of 20 Replicates of Relative Average 5-Fold CV Error"
)
```

Relative to the best model, the model for all predictors has the lowest MSPE and
would be the next best model. The model for temp is also a good model with relatively
low MSPE and low variability.

## 6
The interactive model is not too practical as it has too many predictors and can 
lead to high variance when predicting the true model. Considering the MSPE, variance,
and practicality, I would choose the all predictors model, or if I'm restricted to 
only using 1 predictor then I would pick the model with temperature.

# 4 (Problem Set 5B Categorical Explanatories)
```{r}
ins = read.csv("Insurance.csv", header = TRUE)
ins <- ins[ins$claims>0,]
ins$zone <- as.factor(ins$zone)
ins$make <- as.factor(ins$make)
levels(ins$zone)
levels(ins$make)
dim(ins)
head(ins)
pairs(ins)
```

## 1

```{r}
ins.fit = lm(per ~ . , data = ins)
summary(ins.fit)
```
### i
There are 6 variables, but R is treating each level of categorical variable differently,
thus lm produced the estimate for 19 parameters: 1 for the intercept, 1 for each 
of the 4 numerical variables (km, bonus, insured, and claims), 6 for zone (1 less
than the 7 levels - baseline dropped), and 8 for make (same logic - dropped 1 level)

### ii
This is the default baseline that R uses, so the intercept is just 11.186

### iii
The intercept would be the Beta-0 + the coefficient for zone7 + the coefficient 
for make9, which is
```{r}
11.186 - 2.862 + 1.459
```

