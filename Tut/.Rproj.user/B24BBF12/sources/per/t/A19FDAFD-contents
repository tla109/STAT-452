---
title: "Homework 1"
author: "Tianyu Liu (301249861)"
date: "2023-09-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1

I have read and acknowledge the SFU student academic integrity policy.

# 2 (Q2 from Problem Set 3)

## (a)

### i
There would be less bias, since with a reduced curvature the linear regression would have a better fit than if there is more curvature.

### ii
The variance would be about the same, because Figure 6 is also using a linear fit with n = 10 so the variability would be about the same.

## (b)
The variance gets smaller as we increase sample size, because the larger the sample, the more likely it is that errors average out above and below the true mean. The bias would not change with increasing sample size because when approximating a curve with a line the bias always exists.

# 3 (Application, from Problem Set 4)
## 1
```{r}
AQ <- na.omit(airquality[,1:4])
dim(AQ)
head(AQ)
pairs(AQ)
```

## 2
```{r}
set.seed(4099183)
n = nrow(AQ)
reorder = sample.int(n)
size.train <- floor(n * 0.75) ### Number of observations in our training
### set. Use floor() to round down
ind.train <- reorder[1:size.train] ### Indices of observations
### to put in training set
ind.valid <- reorder[(size.train + 1):n] ### Indices of observations
### to put in validation set
data.train <- AQ[ind.train,] ### Keep only observations in ind.train
data.valid <- AQ[ind.valid,] ### Keep only observations in ind.valid
print(ind.valid)
```
These are the validation indices.

## 3
```{r}
### Fit linear models to predict Ozone using each predictor
### individually, all predictors together, and all interactions with curvature
### Note: These models must be fit using data.train so that we can
### evaluate their MSPE on data.valid
fit.temp <- lm(Ozone ~ Temp, data = data.train)
fit.wind <- lm(Ozone ~ Wind, data = data.train)
fit.solar <- lm(Ozone ~ Solar.R, data = data.train)
fit.all <- lm(Ozone ~ Temp + Wind + Solar.R, data = data.train)
fit.intcurv <- lm(Ozone ~ Temp + Wind + Solar.R + I(Temp^2) + I(Wind^2) + I(Solar.R^2)
+ Temp*Wind + Temp*Solar.R + Wind*Solar.R, data = data.train)

### Get predictions on the validation set for each model using the
### predict() function.
pred.temp <- predict(fit.temp, data.valid)
pred.wind <- predict(fit.wind, data.valid)
pred.solar <- predict(fit.solar, data.valid)
pred.all <- predict(fit.all, data.valid)
pred.intcurv <- predict(fit.intcurv, data.valid)

### When we calculate validation set MSPEs for our models, we will
### end up repeating the same calculation 5 times. Let's make a
### function to do this for us.
get.MSPE <- function(Y, Y.hat) {
  residuals <- Y - Y.hat
  resid.sq <- residuals^2
  SSPE <- sum(resid.sq)
  MSPE <- SSPE / length(Y)
  return(MSPE)
}

### Use our get.MSPE() function to calculate the validation set MSPE
### of each model
Y.valid <- data.valid$Ozone
MSPE.temp <- get.MSPE(Y.valid, pred.temp)
MSPE.wind <- get.MSPE(Y.valid, pred.wind)
MSPE.solar <- get.MSPE(Y.valid, pred.solar)
MSPE.all <- get.MSPE(Y.valid, pred.all)
MSPE.intcurv <- get.MSPE(Y.valid, pred.intcurv)

### Let's compare these validation set MSPEs
print(MSPE.temp)
print(MSPE.wind)
print(MSPE.solar)
print(MSPE.all)
print(MSPE.intcurv)
```

As expected, the most complicated model wins the competition (The one with interactions and curvature).

## 4
```{r}
M <- 5 ### number of times to split
all.split.MSPEs <- array(0, c(M, 5)) ### An array to store calculated
### MSPEs.
colnames(all.split.MSPEs) <- c("temp", "wind", "solar", "all", "intcurv") ### Set model names

for (i in 1:M) {
  #######################################################################
  ### Most of this code is copy-pasted from above. A few things that  ###
  ### would be redundant have been deleted.                           ###
  #######################################################################
  reorder = sample.int(n)
  size.train <- floor(n * 0.75) ### Number of observations in our training
  ### set. Use floor() to round down
  ind.train <- reorder[1:size.train] ### Indices of observations
  ### to put in training set
  ind.valid <- reorder[(size.train + 1):n] ### Indices of observations
  ### to put in validation set
  data.train <- AQ[ind.train,] ### Keep only observations in ind.train
  data.valid <- AQ[ind.valid,] ### Keep only observations in ind.valid
  ### Fit linear models to predict Ozone using each predictor
  ### individually, all predictors together, and all interactions with curvature
  ### Note: These models must be fit using data.train so that we can
  ### evaluate their MSPE on data.valid
  fit.temp <- lm(Ozone ~ Temp, data = data.train)
  fit.wind <- lm(Ozone ~ Wind, data = data.train)
  fit.solar <- lm(Ozone ~ Solar.R, data = data.train)
  fit.all <- lm(Ozone ~ Temp + Wind + Solar.R, data = data.train)
  fit.intcurv <- lm(Ozone ~ Temp + Wind + Solar.R + I(Temp^2) + I(Wind^2) + I(Solar.R^2)
  + Temp*Wind + Temp*Solar.R + Wind*Solar.R, data = data.train)

  ### Get predictions on the validation set for each model using the
  ### predict() function.
  pred.temp <- predict(fit.temp, data.valid)
  pred.wind <- predict(fit.wind, data.valid)
  pred.solar <- predict(fit.solar, data.valid)
  pred.all <- predict(fit.all, data.valid)
  pred.intcurv <- predict(fit.intcurv, data.valid)

  ### Use our get.MSPE() function to calculate the validation set MSPE
  ### of each model
  Y.valid <- data.valid$Ozone
  MSPE.temp <- get.MSPE(Y.valid, pred.temp)
  MSPE.wind <- get.MSPE(Y.valid, pred.wind)
  MSPE.solar <- get.MSPE(Y.valid, pred.solar)
  MSPE.all <- get.MSPE(Y.valid, pred.all)
  MSPE.intcurv <- get.MSPE(Y.valid, pred.intcurv)
  
  ##############################################################
  ### This part is new!! Store calculated MSPEs in our array ###
  ### Note: We use the loop variable to tell which column of ###
  ### our array to use.                                      ###
  ##############################################################
  all.split.MSPEs[i, 1] <- MSPE.temp
  all.split.MSPEs[i, 2] <- MSPE.wind
  all.split.MSPEs[i, 3] <- MSPE.solar
  all.split.MSPEs[i, 4] <- MSPE.all
  all.split.MSPEs[i, 5] <- MSPE.intcurv
}

MSPE.mean <- apply(X = all.split.MSPEs, MARGIN = 2 , FUN = mean)
MSPE.mean #means

MSPE.sd <- apply(X = all.split.MSPEs, MARGIN = 2 , FUN = sd)
MSPE.CIl <- MSPE.mean - qt (p = .975 , df = M-1) * MSPE.sd/sqrt(M)
MSPE.CIu <- MSPE.mean + qt (p = .975 , df = M-1) * MSPE.sd/sqrt(M)
round(cbind(MSPE.CIl,MSPE.CIu), 2) #confidence intervals

```
The model involving wind and the model involving solar are clearly bad, they have a very high MSPE and large standard error.
The model for the interaction is much better, but that's intuitive since it considers all the factors.

## 5
```{r}
### We need to divide the dataset into 20 folds.
### We can do this by randomly sampling the numbers from 1 to 10 and
### attaching these to our dataset as fold labels
n.fold <- n / 20 # Number of observations in each fold
n.fold <- ceiling(n.fold) # Round up to make sure we get enough labels
# We can remove any excess later
ordered.ids <- rep(1:20, times = n.fold)
ordered.ids <- ordered.ids[1:n] # Remove excess label(s)
shuffle <- sample.int(n) # Randomly permute the numbers 1 to n
shuffled.ids <- ordered.ids[shuffle] # Use shuffle to permute
# the fold labels
data.CV <- AQ # Create a copy of our dataset
data.CV$fold <- shuffled.ids # Add a column to our new dataset containing
# the fold labels


### Next, let's actually do the cross validation. This will be easier
### with a for loop than with the replicate function. First, we will
### need to make an array to store the MSPEs
CV.MSPEs <- array(0, dim = c(20, M))
colnames(CV.MSPEs) <- colnames(all.split.MSPEs) # We can recycle the
# model names from
# all.split.MSPEs

for (i in 1:20) {
  ### Use fold i for validation and the rest for training
  data.train <- filter(data.CV, fold != i)
  data.valid <- filter(data.CV, fold == i)

  ### Remove fold from training and validation sets since it
  ### isn't a real predictor
  data.train <- select(data.train, -fold)
  data.valid <- select(data.valid, -fold)

  ### Fit linear models to predict Ozone using each predictor
  ### individually, all predictors together, and all interactions
  ### Note: These models must be fit using data.train so that we can
  ### evaluate their MSPE on data.valid
  fit.temp <- lm(Ozone ~ Temp, data = data.train)
  fit.wind <- lm(Ozone ~ Wind, data = data.train)
  fit.solar <- lm(Ozone ~ Solar.R, data = data.train)
  fit.all <- lm(Ozone ~ Temp + Wind + Solar.R, data = data.train)
  fit.intcurv <- lm(Ozone ~ Temp + Wind + Solar.R + I(Temp^2) + I(Wind^2) + I(Solar.R^2)
  + Temp*Wind + Temp*Solar.R + Wind*Solar.R, data = data.train)


  ### Get predictions on the validation set for each model using the
  ### predict() function.
  pred.temp <- predict(fit.temp, data.valid)
  pred.wind <- predict(fit.wind, data.valid)
  pred.solar <- predict(fit.solar, data.valid)
  pred.all <- predict(fit.all, data.valid)
  pred.intcurv <- predict(fit.intcurv, data.valid)

  ### Use our get.MSPE() function to calculate the validation set MSPE
  ### of each model
  Y.valid <- data.valid$Ozone
  MSPE.temp <- get.MSPE(Y.valid, pred.temp)
  MSPE.wind <- get.MSPE(Y.valid, pred.wind)
  MSPE.solar <- get.MSPE(Y.valid, pred.solar)
  MSPE.all <- get.MSPE(Y.valid, pred.all)
  MSPE.intcurv <- get.MSPE(Y.valid, pred.intcurv)

  ### Store MSPEs
  CV.MSPEs[i, 1] <- MSPE.temp
  CV.MSPEs[i, 2] <- MSPE.wind
  CV.MSPEs[i, 3] <- MSPE.solar
  CV.MSPEs[i, 4] <- MSPE.all
  CV.MSPEs[i, 5] <- MSPE.intcurv
}


### Make a boxplot of the scores
boxplot(CV.MSPEs,
  main = "Boxplot of CV Error With 20 Folds"
)


### Calculate relative errors and make boxplots
rel.CV.MSPEs <- apply(CV.MSPEs, 1, function(W) {
  best <- min(W)
  return(W / best)
})

rel.CV.MSPEs <- t(rel.CV.MSPEs)

boxplot(rel.CV.MSPEs,
  main = "Boxplot of Relative CV Error With 20 Folds"
)
```
### (a)
The models for all variables and the interactions are still good, but the model for wind is also decent, with low variance compared to other models.
###
