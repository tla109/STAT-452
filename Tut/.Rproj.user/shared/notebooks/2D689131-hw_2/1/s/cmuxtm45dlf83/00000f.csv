"0","set.seed(2928893)"
"0","### Let's define a function for constructing CV folds"
"0","get.folds <- function(n, K) {"
"0","  ### Get the appropriate number of fold labels"
"0","  n.fold <- ceiling(n / K) # Number of observations per fold (rounded up)"
"0","  fold.ids.raw <- rep(1:K, times = n.fold) # Generate extra labels"
"0","  fold.ids <- fold.ids.raw[1:n] # Keep only the correct number of labels"
"0",""
"0","  ### Shuffle the fold labels"
"0","  folds.rand <- fold.ids[sample.int(n)]"
"0",""
"0","  return(folds.rand)"
"0","}"
"0",""
"0","get.MSPE <- function(Y, Y.hat) {"
"0","  return(mean((Y - Y.hat)^2))"
"0","}"
"0",""
"0","### Number of folds"
"0","K <- 10"
"0",""
"0","### Construct folds"
"0","n <- nrow(AQ) # Sample size"
"0","folds <- get.folds(n, K)"
"0",""
"0","### Create a container for MSPEs. Let's include ordinary least-squares"
"0","### regression for reference"
"0","all.models <- c(""Ridge"", ""LASSO-Min"", ""LASSO-1se"")"
"0","all.MSPEs <- array(0, dim = c(K, length(all.models)))"
"0","colnames(all.MSPEs) <- all.models"
"0",""
"0","### Begin cross-validation"
"0","for (i in 1:K) {"
"0","  ### Split data"
"0","  data.train <- AQ[folds != i, ]"
"0","  data.valid <- AQ[folds == i, ]"
"0","  n.train <- nrow(data.train)"
"0",""
"0","  ### Get response vectors"
"0","  Y.train <- data.train$Ozone"
"0","  Y.valid <- data.valid$Ozone"
"0",""
"0","  #######################################################################"
"0","  ### Let's do ridge regression. This model is fit using the    ###"
"0","  ### lm.ridge() function in the MASS package. We will need to make a ###"
"0","  ### list of candidate lambda values for the function to choose      ###"
"0","  ### from. Prediction also has some extra steps, but we'll discuss   ###"
"0","  ### that when we get there.                                         ###"
"0","  #######################################################################"
"0",""
"0","  ### Make a list of lambda values. The lm.ridge() function will"
"0","  ### then choose the best value from this list. Use the seq()"
"0","  ### function to create an equally-spaced list."
"0","  lambda.vals <- seq(from = 0, to = 100, by = 0.05)"
"0",""
"0","  ### Use the lm.ridge() function to fit a ridge regression model. The"
"0","  ### syntax is almost identical to the lm() function, we just need"
"0","  ### to set lambda equal to our list of candidate values."
"0","  fit.ridge <- lm.ridge(Ozone ~ .,"
"0","    lambda = lambda.vals,"
"0","    data = data.train"
"0","  )"
"0",""
"0","  ### To get predictions, we need to evaluate the fitted regression"
"0","  ### equation directly (sadly, no predict() function to do this for us)."
"0","  ### You could do this using a for loop if you prefer, but there is"
"0","  ### a shortcut which uses matrix-vector multiplication. The syntax"
"0","  ### for this multiplication method is much shorter."
"0",""
"0","  ### Get best lambda value and its index"
"0","  ### Note: Best is chosen according to smallest GCV value. We can"
"0","  ###       get GCV from a ridge regression object using $GCV"
"0","  ind.min.GCV <- which.min(fit.ridge$GCV)"
"0","  lambda.min <- lambda.vals[ind.min.GCV]"
"0",""
"0","  ### Get coefficients corresponding to best lambda value"
"0","  ### We can get the coefficients for every value of lambda using"
"0","  ### the coef() function on a ridge regression object"
"0","  all.coefs.ridge <- coef(fit.ridge)"
"0","  coef.min <- all.coefs.ridge[ind.min.GCV, ]"
"0",""
"0","  ### We will multiply the dataset by this coefficients vector, but"
"0","  ### we need to add a column to our dataset for the intercept and"
"0","  ### create indicators for our categorical predictors. A simple"
"0","  ### way to do this is using the model.matrix() function from last"
"0","  ### week."
"0","  matrix.valid.ridge <- model.matrix(Ozone ~ ., data = data.valid)"
"0",""
"0","  ### Now we can multiply the data by our coefficient vector. The"
"0","  ### syntax in R for matrix-vector multiplication is %*%. Note that,"
"0","  ### for this type of multiplication, order matters. That is,"
"0","  ### A %*% B != B %*% A. Make sure you do data %*% coefficients."
"0","  ### For more information, see me in a Q&A session or, better still,"
"0","  ### take a course on linear algebra (it's really neat stuff)"
"0","  pred.ridge <- matrix.valid.ridge %*% coef.min"
"0",""
"0","  ### Now we just need to calculate the MSPE and store it"
"0","  MSPE.ridge <- get.MSPE(Y.valid, pred.ridge)"
"0","  all.MSPEs[i, ""Ridge""] <- MSPE.ridge"
"0",""
"0",""
"0","  #######################################################################"
"0","  ### Now we can do the LASSO. This model is fit using the glmnet()   ###"
"0","  ### or cv.glmnet() functions in the glmnet package. LASSO also has  ###"
"0","  ### a tuning parameter, lambda, which we have to choose.            ###"
"0","  ### Fortunately, the cv.glmnet() function does CV internally, and   ###"
"0","  ### lets us automatically find the 'best' value of lambda.          ###"
"0","  #######################################################################"
"0",""
"0","  ### The cv.glmnet() function has different syntax from what we're"
"0","  ### used to. Here, we have to provide a matrix with all of our"
"0","  ### predictors, and a vector of our response. LASSO handles"
"0","  ### the intercept differently, so we want to make sure our data"
"0","  ### matrix does not include an intercept (then let cv.glmnet() add"
"0","  ### an intercept later). Unfortunately, the model.matrix() function"
"0","  ### gets confused if we ask it to construct indicators for our"
"0","  ### categorical predictors without also including an intercept."
"0","  ### A simple way to fix this is to create the data matrix with an"
"0","  ### intercept, then delete the intercept."
"0","  matrix.train.raw <- model.matrix(Ozone ~ ., data = data.train)"
"0","  matrix.train <- matrix.train.raw[, -1]"
"0",""
"0","  ### The cv.glmnet() function creates a list of lambda values, then"
"0","  ### does CV internally to choose the 'best' one. 'Best' can refer to"
"0","  ### either the value of lambda which gives the smallest CV-MSPE"
"0","  ### (called the min rule), or the value of lambda which gives the"
"0","  ### simplest model that gives CV-MSPE close to the minimum (called"
"0","  ### the 1se rule). The cv.glmnet() function gets both of these"
"0","  ### lambda values."
"0","  all.LASSOs <- cv.glmnet(x = matrix.train, y = Y.train)"
"0",""
"0","  ### Get both 'best' lambda values using $lambda.min and $lambda.1se"
"0","  lambda.min <- all.LASSOs$lambda.min"
"0","  lambda.1se <- all.LASSOs$lambda.1se"
"0",""
"0","  ### cv.glmnet() has a predict() function (yay!). This predict function"
"0","  ### also does other things, like get the coefficients, or tell us"
"0","  ### which predictors get non-zero coefficients. We are also able"
"0","  ### to specify the value of lambda for which we want our output"
"0","  ### (remember that, with ridge, we got a matrix of coefficients and"
"0","  ### had to choose the row matching our lambda). Strangely, the name"
"0","  ### of the input where we specify our value of lambda is s."
"0",""
"0","  ### Get the coefficients for our two 'best' LASSO models"
"0","  coef.LASSO.min <- predict(all.LASSOs, s = lambda.min, type = ""coef"")"
"0","  coef.LASSO.1se <- predict(all.LASSOs, s = lambda.1se, type = ""coef"")"
"0",""
"0","  ### Get which predictors are included in our models (i.e. which"
"0","  ### predictors have non-zero coefficients)"
"0","  included.LASSO.min <- predict(all.LASSOs,"
"0","    s = lambda.min,"
"0","    type = ""nonzero"""
"0","  )"
"0","  included.LASSO.1se <- predict(all.LASSOs,"
"0","    s = lambda.1se,"
"0","    type = ""nonzero"""
"0","  )"
"0",""
"0","  ### Get predictions from both models on the validation fold. First,"
"0","  ### we need to create a predictor matrix from the validation set."
"0","  ### Remember to include the intercept in model.matrix(), then delete"
"0","  ### it in the next step."
"0","  matrix.valid.LASSO.raw <- model.matrix(Ozone ~ ., data = data.valid)"
"0","  matrix.valid.LASSO <- matrix.valid.LASSO.raw[, -1]"
"0","  pred.LASSO.min <- predict(all.LASSOs,"
"0","    newx = matrix.valid.LASSO,"
"0","    s = lambda.min, type = ""response"""
"0","  )"
"0","  pred.LASSO.1se <- predict(all.LASSOs,"
"0","    newx = matrix.valid.LASSO,"
"0","    s = lambda.1se, type = ""response"""
"0","  )"
"0",""
"0","  ### Calculate MSPEs and store them"
"0","  MSPE.LASSO.min <- get.MSPE(Y.valid, pred.LASSO.min)"
"0","  all.MSPEs[i, ""LASSO-Min""] <- MSPE.LASSO.min"
"0",""
"0","  MSPE.LASSO.1se <- get.MSPE(Y.valid, pred.LASSO.1se)"
"0","  all.MSPEs[i, ""LASSO-1se""] <- MSPE.LASSO.1se"
"0","}"
"0",""
"0","all.MSPEs"
"1","     "
"1","     Ridge"
"1"," LASSO-Min"
"1"," LASSO-1se"
"1","
 [1,]"
"1","  280.6840"
"1","  314.2722"
"1","  415.7364"
"1","
 [2,]"
"1","  363.7170"
"1","  316.8250"
"1","  236.9153"
"1","
 [3,]"
"1","  539.3697"
"1","  586.1201"
"1","  717.2987"
"1","
 [4,]"
"1","  108.3192"
"1","  114.6189"
"1","  157.0742"
"1","
 [5,]"
"1","  185.4001"
"1","  190.1739"
"1","  323.5662"
"1","
 [6,]"
"1","  371.7604"
"1","  373.4553"
"1","  668.8504"
"1","
 [7,]"
"1","  219.6295"
"1","  217.3012"
"1","  366.8750"
"1","
 [8,]"
"1","  587.6590"
"1","  585.7460"
"1","  643.3282"
"1","
 [9,]"
"1"," 1042.9887"
"1"," 1045.5646"
"1"," 1125.8526"
"1","
[10,]"
"1","  725.3818"
"1","  650.6659"
"1","  377.7573"
"1","
"
"0","avg.MSPEs = colMeans(all.MSPEs)"
"0","avg.MSPEs"
"1","    Ridge "
"1","LASSO-Min "
"1","LASSO-1se "
"1","
"
"1"," 442.4909 "
"1"," 439.4743 "
"1"," 503.3254 "
"1","
"
