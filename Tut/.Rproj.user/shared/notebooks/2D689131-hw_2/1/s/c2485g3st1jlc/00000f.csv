"0","### Begin cross-validation
for (i in 1:K) {
  ### Split data
  data.train <- AQ[folds != i, ]
  data.valid <- AQ[folds == i, ]
  n.train <- nrow(data.train)

  ### Get response vectors
  Y.train <- data.train$Ozone
  Y.valid <- data.valid$Ozone

  ###################################################################
  ### First, let's quickly do LS so we have a reference point for ###
  ### how well the other models do.                               ###
  ###################################################################

  fit.ls <- lm(Ozone ~ ., data = data.train)
  pred.ls <- predict(fit.ls, newdata = data.valid)
  MSPE.ls <- get.MSPE(Y.valid, pred.ls)
  all.MSPEs[i, ""LS""] <- MSPE.ls


  #######################################################################
  ### Next, let's do ridge regression. This model is fit using the    ###
  ### lm.ridge() function in the MASS package. We will need to make a ###
  ### list of candidate lambda values for the function to choose      ###
  ### from. Prediction also has some extra steps, but we'll discuss   ###
  ### that when we get there.                                         ###
  #######################################################################

  ### Make a list of lambda values. The lm.ridge() function will
  ### then choose the best value from this list. Use the seq()
  ### function to create an equally-spaced list.
  lambda.vals <- seq(from = 0, to = 100, by = 0.05)

  ### Use the lm.ridge() function to fit a ridge regression model. The
  ### syntax is almost identical to the lm() function, we just need
  ### to set lambda equal to our list of candidate values.
  fit.ridge <- lm.ridge(Ozone ~ .,
    lambda = lambda.vals,
    data = data.train
  )

  ### To get predictions, we need to evaluate the fitted regression
  ### equation directly (sadly, no predict() function to do this for us).
  ### You could do this using a for loop if you prefer, but there is
  ### a shortcut which uses matrix-vector multiplication. The syntax
  ### for this multiplication method is much shorter.

  ### Get best lambda value and its index
  ### Note: Best is chosen according to smallest GCV value. We can
  ###       get GCV from a ridge regression object using $GCV
  ind.min.GCV <- which.min(fit.ridge$GCV)
  lambda.min <- lambda.vals[ind.min.GCV]

  ### Get coefficients corresponding to best lambda value
  ### We can get the coefficients for every value of lambda using
  ### the coef() function on a ridge regression object
  all.coefs.ridge <- coef(fit.ridge)
  coef.min <- all.coefs.ridge[ind.min.GCV, ]

  ### We will multiply the dataset by this coefficients vector, but
  ### we need to add a column to our dataset for the intercept and
  ### create indicators for our categorical predictors. A simple
  ### way to do this is using the model.matrix() function from last
  ### week.
  matrix.valid.ridge <- model.matrix(Ozone ~ ., data = data.valid)

  ### Now we can multiply the data by our coefficient vector. The
  ### syntax in R for matrix-vector multiplication is %*%. Note that,
  ### for this type of multiplication, order matters. That is,
  ### A %*% B != B %*% A. Make sure you do data %*% coefficients.
  ### For more information, see me in a Q&A session or, better still,
  ### take a course on linear algebra (it's really neat stuff)
  pred.ridge <- matrix.valid.ridge %*% coef.min

  ### Now we just need to calculate the MSPE and store it
  MSPE.ridge <- get.MSPE(Y.valid, pred.ridge)
  all.MSPEs[i, ""Ridge""] <- MSPE.ridge


  #######################################################################
  ### Now we can do the LASSO. This model is fit using the glmnet()   ###
  ### or cv.glmnet() functions in the glmnet package. LASSO also has  ###
  ### a tuning parameter, lambda, which we have to choose.            ###
  ### Fortunately, the cv.glmnet() function does CV internally, and   ###
  ### lets us automatically find the 'best' value of lambda.          ###
  #######################################################################

  ### The cv.glmnet() function has different syntax from what we're
  ### used to. Here, we have to provide a matrix with all of our
  ### predictors, and a vector of our response. LASSO handles
  ### the intercept differently, so we want to make sure our data
  ### matrix does not include an intercept (then let cv.glmnet() add
  ### an intercept later). Unfortunately, the model.matrix() function
  ### gets confused if we ask it to construct indicators for our
  ### categorical predictors without also including an intercept.
  ### A simple way to fix this is to create the data matrix with an
  ### intercept, then delete the intercept.
  matrix.train.raw <- model.matrix(Ozone ~ ., data = data.train)
  matrix.train <- matrix.train.raw[, -1]

  ### The cv.glmnet() function creates a list of lambda values, then
  ### does CV internally to choose the 'best' one. 'Best' can refer to
  ### either the value of lambda which gives the smallest CV-MSPE
  ### (called the min rule), or the value of lambda which gives the
  ### simplest model that gives CV-MSPE close to the minimum (called
  ### the 1se rule). The cv.glmnet() function gets both of these
  ### lambda values.
  all.LASSOs <- cv.glmnet(x = matrix.train, y = Y.train)

  ### Get both 'best' lambda values using $lambda.min and $lambda.1se
  lambda.min <- all.LASSOs$lambda.min
  lambda.1se <- all.LASSOs$lambda.1se

  ### cv.glmnet() has a predict() function (yay!). This predict function
  ### also does other things, like get the coefficients, or tell us
  ### which predictors get non-zero coefficients. We are also able
  ### to specify the value of lambda for which we want our output
  ### (remember that, with ridge, we got a matrix of coefficients and
  ### had to choose the row matching our lambda). Strangely, the name
  ### of the input where we specify our value of lambda is s.

  ### Get the coefficients for our two 'best' LASSO models
  coef.LASSO.min <- predict(all.LASSOs, s = lambda.min, type = ""coef"")
  coef.LASSO.1se <- predict(all.LASSOs, s = lambda.1se, type = ""coef"")

  ### Get which predictors are included in our models (i.e. which
  ### predictors have non-zero coefficients)
  included.LASSO.min <- predict(all.LASSOs,
    s = lambda.min,
    type = ""nonzero""
  )
  included.LASSO.1se <- predict(all.LASSOs,
    s = lambda.1se,
    type = ""nonzero""
  )

  ### Get predictions from both models on the validation fold. First,
  ### we need to create a predictor matrix from the validation set.
  ### Remember to include the intercept in model.matrix(), then delete
  ### it in the next step.
  matrix.valid.LASSO.raw <- model.matrix(Ozone ~ ., data = data.valid)
  matrix.valid.LASSO <- matrix.valid.LASSO.raw[, -1]
  pred.LASSO.min <- predict(all.LASSOs,
    newx = matrix.valid.LASSO,
    s = lambda.min, type = ""response""
  )
  pred.LASSO.1se <- predict(all.LASSOs,
    newx = matrix.valid.LASSO,
    s = lambda.1se, type = ""response""
  )

  ### Calculate MSPEs and store them
  MSPE.LASSO.min <- get.MSPE(Y.valid, pred.LASSO.min)
  all.MSPEs[i, ""LASSO-Min""] <- MSPE.LASSO.min

  MSPE.LASSO.1se <- get.MSPE(Y.valid, pred.LASSO.1se)
  all.MSPEs[i, ""LASSO-1se""] <- MSPE.LASSO.1se
}
"
"2","Error in get.MSPE(Y.valid, pred.ls) : could not find function ""get.MSPE""
"
