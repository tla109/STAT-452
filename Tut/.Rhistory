avg.MSPEs
boxplot(all.MSPEs, main = paste0("CV MSPEs over ", K, " folds"))
all.RMSPEs <- apply(all.MSPEs.all, 1, function(W) {
best <- min(W)
return(W / best)
})
boxplot(all.MSPEs, main = paste0("CV MSPEs over ", K, " folds"))
all.RMSPEs <- apply(all.MSPEs, 1, function(W) {
best <- min(W)
return(W / best)
})
all.RMSPEs <- t(all.RMSPEs)
boxplot(all.RMSPEs, main = paste0("CV RMSPEs over ", K, " folds"))
boxplot(all.RMSPEs, main = paste0("CV RMSPEs over ", K, " folds"))
apply(all.MSPEs, 2, min)
?apply
apply(all.MSPEs, 2, var)
knitr::opts_chunk$set(echo = TRUE)
### Fit polynomial regression models
fit.poly.3 <- lm(Ozone ~ poly(Temp, degree = 3), data = AQ)
### Fit polynomial regression models
fit.poly.3 <- lm(Ozone ~ poly(Temp, degree = 3), data = AQ)
knitr::opts_chunk$set(echo = TRUE)
AQ <- na.omit(airquality[,1:4])
AQ$TWcp = AQ$Temp*AQ$Wind
AQ$TWrat = AQ$Temp/AQ$Wind
head(AQ)
library(MASS)
###############################################################
# Ridge regression using lm.ridge()
#
#  Running sequence from 0 to 100 by a small increment may be a little expensive
#  but probably covers range.
ridge <- lm.ridge(Ozone ~ ., lambda = seq(0, 100, .05), data = AQ)
select(ridge)
(coef.ri.best <- coef(ridge)[which.min(ridge$GCV), ])
coef.ri.best
# Compare MSPE to MSPE from lm
mod.lm <- lm(Ozone ~ ., data = AQ)
coef.ls <- coef(mod.lm)
length(which(coef.ls[2:6] - coef.ri.best[2:6] > 0)) /length(coef.ls[2:6])
library(glmnet)
y<- AQ[,1]
x<- as.matrix(AQ[, c(2:6)])
# Fit LASSO by glmnet(y=, x=). Gaussian is default, but other families are available
#  Function produces series of fits for many values of lambda.
lasso <- glmnet(y = y, x = x, family = "gaussian")
# cv.glmnet() uses crossvalidation to estimate optimal lambda
cv.lasso <- cv.glmnet(y = y, x = x, family = "gaussian")
cv.lasso
coef(cv.lasso, s = cv.lasso$lambda.min)
coef(cv.lasso, s = cv.lasso$lambda.1se)
step <- step(
object = lm(y ~ 1, data = AQ), scope = list(upper = mod.lm), direction = "both",
k = log(nrow(AQ)), trace = 0)
coef.step <- coef(step)
coef.step
set.seed(2928893)
### Let's define a function for constructing CV folds
get.folds <- function(n, K) {
### Get the appropriate number of fold labels
n.fold <- ceiling(n / K) # Number of observations per fold (rounded up)
fold.ids.raw <- rep(1:K, times = n.fold) # Generate extra labels
fold.ids <- fold.ids.raw[1:n] # Keep only the correct number of labels
### Shuffle the fold labels
folds.rand <- fold.ids[sample.int(n)]
return(folds.rand)
}
get.MSPE <- function(Y, Y.hat) {
return(mean((Y - Y.hat)^2))
}
### Number of folds
K <- 10
### Construct folds
n <- nrow(AQ) # Sample size
folds <- get.folds(n, K)
### Create a container for MSPEs. Let's include ordinary least-squares
### regression for reference
all.models <- c("Ridge", "LASSO-Min", "LASSO-1se")
all.MSPEs <- array(0, dim = c(K, length(all.models)))
colnames(all.MSPEs) <- all.models
### Begin cross-validation
for (i in 1:K) {
### Split data
data.train <- AQ[folds != i, ]
data.valid <- AQ[folds == i, ]
n.train <- nrow(data.train)
### Get response vectors
Y.train <- data.train$Ozone
Y.valid <- data.valid$Ozone
#######################################################################
### Let's do ridge regression. This model is fit using the    ###
### lm.ridge() function in the MASS package. We will need to make a ###
### list of candidate lambda values for the function to choose      ###
### from. Prediction also has some extra steps, but we'll discuss   ###
### that when we get there.                                         ###
#######################################################################
### Make a list of lambda values. The lm.ridge() function will
### then choose the best value from this list. Use the seq()
### function to create an equally-spaced list.
lambda.vals <- seq(from = 0, to = 100, by = 0.05)
### Use the lm.ridge() function to fit a ridge regression model. The
### syntax is almost identical to the lm() function, we just need
### to set lambda equal to our list of candidate values.
fit.ridge <- lm.ridge(Ozone ~ .,
lambda = lambda.vals,
data = data.train
)
### To get predictions, we need to evaluate the fitted regression
### equation directly (sadly, no predict() function to do this for us).
### You could do this using a for loop if you prefer, but there is
### a shortcut which uses matrix-vector multiplication. The syntax
### for this multiplication method is much shorter.
### Get best lambda value and its index
### Note: Best is chosen according to smallest GCV value. We can
###       get GCV from a ridge regression object using $GCV
ind.min.GCV <- which.min(fit.ridge$GCV)
lambda.min <- lambda.vals[ind.min.GCV]
### Get coefficients corresponding to best lambda value
### We can get the coefficients for every value of lambda using
### the coef() function on a ridge regression object
all.coefs.ridge <- coef(fit.ridge)
coef.min <- all.coefs.ridge[ind.min.GCV, ]
### We will multiply the dataset by this coefficients vector, but
### we need to add a column to our dataset for the intercept and
### create indicators for our categorical predictors. A simple
### way to do this is using the model.matrix() function from last
### week.
matrix.valid.ridge <- model.matrix(Ozone ~ ., data = data.valid)
### Now we can multiply the data by our coefficient vector. The
### syntax in R for matrix-vector multiplication is %*%. Note that,
### for this type of multiplication, order matters. That is,
### A %*% B != B %*% A. Make sure you do data %*% coefficients.
### For more information, see me in a Q&A session or, better still,
### take a course on linear algebra (it's really neat stuff)
pred.ridge <- matrix.valid.ridge %*% coef.min
### Now we just need to calculate the MSPE and store it
MSPE.ridge <- get.MSPE(Y.valid, pred.ridge)
all.MSPEs[i, "Ridge"] <- MSPE.ridge
#######################################################################
### Now we can do the LASSO. This model is fit using the glmnet()   ###
### or cv.glmnet() functions in the glmnet package. LASSO also has  ###
### a tuning parameter, lambda, which we have to choose.            ###
### Fortunately, the cv.glmnet() function does CV internally, and   ###
### lets us automatically find the 'best' value of lambda.          ###
#######################################################################
### The cv.glmnet() function has different syntax from what we're
### used to. Here, we have to provide a matrix with all of our
### predictors, and a vector of our response. LASSO handles
### the intercept differently, so we want to make sure our data
### matrix does not include an intercept (then let cv.glmnet() add
### an intercept later). Unfortunately, the model.matrix() function
### gets confused if we ask it to construct indicators for our
### categorical predictors without also including an intercept.
### A simple way to fix this is to create the data matrix with an
### intercept, then delete the intercept.
matrix.train.raw <- model.matrix(Ozone ~ ., data = data.train)
matrix.train <- matrix.train.raw[, -1]
### The cv.glmnet() function creates a list of lambda values, then
### does CV internally to choose the 'best' one. 'Best' can refer to
### either the value of lambda which gives the smallest CV-MSPE
### (called the min rule), or the value of lambda which gives the
### simplest model that gives CV-MSPE close to the minimum (called
### the 1se rule). The cv.glmnet() function gets both of these
### lambda values.
all.LASSOs <- cv.glmnet(x = matrix.train, y = Y.train)
### Get both 'best' lambda values using $lambda.min and $lambda.1se
lambda.min <- all.LASSOs$lambda.min
lambda.1se <- all.LASSOs$lambda.1se
### cv.glmnet() has a predict() function (yay!). This predict function
### also does other things, like get the coefficients, or tell us
### which predictors get non-zero coefficients. We are also able
### to specify the value of lambda for which we want our output
### (remember that, with ridge, we got a matrix of coefficients and
### had to choose the row matching our lambda). Strangely, the name
### of the input where we specify our value of lambda is s.
### Get the coefficients for our two 'best' LASSO models
coef.LASSO.min <- predict(all.LASSOs, s = lambda.min, type = "coef")
coef.LASSO.1se <- predict(all.LASSOs, s = lambda.1se, type = "coef")
### Get which predictors are included in our models (i.e. which
### predictors have non-zero coefficients)
included.LASSO.min <- predict(all.LASSOs,
s = lambda.min,
type = "nonzero"
)
included.LASSO.1se <- predict(all.LASSOs,
s = lambda.1se,
type = "nonzero"
)
### Get predictions from both models on the validation fold. First,
### we need to create a predictor matrix from the validation set.
### Remember to include the intercept in model.matrix(), then delete
### it in the next step.
matrix.valid.LASSO.raw <- model.matrix(Ozone ~ ., data = data.valid)
matrix.valid.LASSO <- matrix.valid.LASSO.raw[, -1]
pred.LASSO.min <- predict(all.LASSOs,
newx = matrix.valid.LASSO,
s = lambda.min, type = "response"
)
pred.LASSO.1se <- predict(all.LASSOs,
newx = matrix.valid.LASSO,
s = lambda.1se, type = "response"
)
### Calculate MSPEs and store them
MSPE.LASSO.min <- get.MSPE(Y.valid, pred.LASSO.min)
all.MSPEs[i, "LASSO-Min"] <- MSPE.LASSO.min
MSPE.LASSO.1se <- get.MSPE(Y.valid, pred.LASSO.1se)
all.MSPEs[i, "LASSO-1se"] <- MSPE.LASSO.1se
}
all.MSPEs
avg.MSPEs = colMeans(all.MSPEs)
avg.MSPEs
library(stringr)
all.MSPEs.LS.step <- array(0, dim = c(K, 2))
colnames(all.MSPEs.LS.step) <- c("LS", "hybrid stepwise")
coefs.sw <- matrix(NA, nrow = K, ncol = 11)
for (i in 1:K) {
### Split data
data.train <- AQ[folds != i, ]
data.valid <- AQ[folds == i, ]
n.train <- nrow(data.train)
### Get response vectors
Y.train <- data.train$Ozone
Y.valid <- data.valid$Ozone
###################################################################
### First, let's quickly do LS so we have a reference point for ###
### how well the other models do.                               ###
###################################################################
fit.ls <- lm(Ozone ~ ., data = data.train)
pred.ls <- predict(fit.ls, newdata = data.valid)
MSPE.ls <- get.MSPE(Y.valid, pred.ls)
all.MSPEs.LS.step[i, "LS"] <- MSPE.ls
step <- step(
object = lm(Ozone ~ 1, data = data.train), scope = list(upper = fit.ls), direction = "both",
k = log(nrow(data.train)), trace = 0)
pred.sw <- predict(step, as.data.frame(data.valid))
MSPE.sw <- get.MSPE(Y.valid, pred.sw)
all.MSPEs.LS.step[i, "hybrid stepwise"] <- MSPE.sw
}
all.MSPEs.all = cbind(all.MSPEs.LS.step, all.MSPEs)
boxplot(all.MSPEs.all, main = paste0("CV MSPEs over ", K, " folds"))
all.RMSPEs <- apply(all.MSPEs.all, 1, function(W) {
best <- min(W)
return(W / best)
})
all.RMSPEs <- t(all.RMSPEs)
boxplot(all.RMSPEs, main = paste0("CV RMSPEs over ", K, " folds"))
library(pls)
all.MSPEs.pls <- array(0, dim = c(K, 1))
colnames(all.MSPEs.pls) <- c("PLS")
n_comps = array(0, dim = c(K,1))
for (i in 1:K) {
### Split data
data.train <- AQ[folds != i, ]
data.valid <- AQ[folds == i, ]
n.train <- nrow(data.train)
### Get response vectors
Y.train <- data.train$Ozone
Y.valid <- data.valid$Ozone
### Now, let's do PLS using the plsr() function. The syntax is
### very similar to lm(). If we set validation = "CV", the plsr()
### function will do its own internal CV, and give MSPEs for each
### number of components. We can then use this to choose how many
### componenets to keep when doing prediction on the validation
### fold. We can use an optional input called segments to specify
### how many folds we want plsr() to use for its internal CV
### (default is 10).
fit_pls <- plsr(Ozone ~ .,
data = data.train, validation = "CV")
### Investigate the fitted PLS model. Comment out the next two
### lines when running a CV loop
### The summary function gives us lots of information about how
### errors change as we increase the number of components
# summary(fit.pls)
### The validationplot() function shows how MSPE from the internal
### CV of plsr() changes with the number of included components.
# validationplot(fit.pls)
### Get the best model from PLS. To do this, we need to find the model
### that minimizes MSPE for the plsr() function's internal CV. It
### takes a few steps, but all the information we need is contained
### in the output of plsr().
CV_pls <- fit_pls$validation
pls_comps <- CV_pls$PRESS
n_comps[i] <- which.min(pls_comps)
### Get predictions and calculate MSPE on the validation fold
### Set ncomps equal to the optimal number of components
pred.pls <- predict(fit_pls, data.valid, ncomp = n_comps[i])
MSPE.pls <- get.MSPE(Y.valid, pred.pls)
all.MSPEs.pls[i, "PLS"] <- MSPE.pls
}
n_comps
all.MSPEs.pls
mean(all.MSPEs.pls)
boxplot(cbind(all.MSPEs.all, all.MSPEs.pls), main = paste0("CV MSPEs over ", K, " folds"))
all.RMSPEs.all <- apply(cbind(all.MSPEs.all, all.MSPEs.pls), 1, function(W) {
best <- min(W)
return(W / best)
})
all.RMSPEs.all <- t(all.RMSPEs.all)
boxplot(all.RMSPEs.all, main = paste0("CV RMSPEs over ", K, " folds"))
### Fit polynomial regression models
fit.poly.3 <- lm(Ozone ~ poly(Temp, degree = 3), data = AQ)
### Fit basis splines
library(splines)
fit.basis.5 <- lm(Ozone ~ bs(Temp, degree = 5), data = AQ)
fit.basis.7 <- lm(Ozone ~ bs(Temp, degree = 7), data = AQ)
fit.basis.9 <- lm(Ozone ~ bs(Temp, degree = 9), data = AQ)
fit.basis.20 <- lm(Ozone ~ bs(Temp, degree = 20), data = AQ)
### Predicting
Temp.sort <- data.frame(Temp = sort(AQ$Temp))
pred.poly.3 <- predict(fit.poly.3, Temp.sort)
pred.basis.5 <- predict(fit.basis.5, Temp.sort)
pred.basis.7 <- predict(fit.basis.7, Temp.sort)
pred.basis.9 <- predict(fit.basis.9, Temp.sort)
pred.basis.20 <- predict(fit.basis.20, Temp.sort)
### Plots
with(AQ, plot(Temp, Ozone))
lines(Temp.sort$Temp, pred.poly.3)
lines(Temp.sort$Temp, pred.basis.5, col = 'red')
lines(Temp.sort$Temp, pred.basis.7, col = 'blue')
lines(Temp.sort$Temp, pred.basis.9, col = 'green')
lines(Temp.sort$Temp, pred.basis.20, col = 'gold')
legend(x = 55, y = 180, legend = c(
"Cubic Poly", "Cubic Spline 5 df",
"Cubic Spline 7 df", "Cubic Spline 9 df", "Cubic Spline 20 df"
),
lty = "solid", col = c('black', 'red', 'blue', 'green', 'gold'), lwd = 2
)
### Plots
with(AQ, plot(Temp, Ozone))
lines(Temp.sort$Temp, pred.poly.3)
with(AQ, plot(Temp, Ozone))
lines(Temp.sort$Temp, pred.poly.3)
### Plots
with(AQ, plot(Temp, Ozone))
lines(Temp.sort$Temp, pred.poly.3)
with(AQ, plot(Temp, Ozone))
lines(Temp.sort$Temp, pred.poly.3)
lines(Temp.sort$Temp, pred.basis.5, col = 'red')
library(FNN)
### Read-in and process the data
source("Read_Wine_Data_Class.R")
head(data)
summary(data)
### Plot CV misclassification rates
plot(1:K.max, mis.CV,
xlab = "K", ylab = "Misclassification Rate",
ylim = c(0.4, 0.64)
)
### Read-in and process the data
source("Read_Wine_Data_Class.R")
head(data)
summary(data)
table(data$quality)
### Some of our code will be random, so we have to set the seed.
### Use Mersenne-Twister for compatibility.
set.seed(46536737, kind = "Mersenne-Twister")
### Split the data into training and validation sets
p.train <- 0.75
n <- nrow(data)
n.train <- floor(p.train * n)
ind.random <- sample(1:n)
data.train <- data[ind.random <= n.train, ]
remove(list=ls())
source("C:/Users/Terry Liu/Desktop/STAT 452/Tut/Sec11_Tutorial_KNN.R", echo=TRUE)
remove(list=ls())
# K-Nearest neighbour on the Wheat data
##########
# Enter data and do some processing
wheat <- read.csv("Datasets/wheat.csv")
setwd("C:/Users/Terry Liu/Desktop/STAT 452/Class Examples")
# K-Nearest neighbour on the Wheat data
##########
# Enter data and do some processing
wheat <- read.csv("Datasets/wheat.csv")
# K-Nearest neighbour on the Wheat data
##########
# Enter data and do some processing
wheat <- read.csv("Datasets/wheat.csv")
head(wheat)
summary(wheat)
# Variable "type" is the response variable.  "class" is another explanatory.
class(wheat$type)
wheat$type <- as.factor(wheat$type)
wheat$class <- as.factor(wheat$class)
summary(wheat)
# Create a numerical version of "class" for methods that need numbers
wheat$classnum <- as.numeric((wheat$class))
# Remove "id"
wheat <- wheat[, -1]
summary(wheat)
# KNN requires numerical explanatories, so remove factor class
# Creating TWO sets: 200 train, 75 test
set.seed(67982193)
perm <- sample(x = nrow(wheat))
set1 <- wheat[which(perm <= 200), -1]
set2 <- wheat[which(perm > 200), -1]
# Function to Scaling x1 using mean and SD from set2
############
scale.1 <- function(x1, x2) {
for (col in 1:ncol(x1)) {
a <- mean(x2[, col])
b <- sd(x2[, col])
x1[, col] <- (x1[, col] - a) / b
}
x1
}
# Creating training and test X matrices, then scaling them.
x.1.unscaled <- as.matrix(set1[, -6])
x.1 <- scale.1(x.1.unscaled, x.1.unscaled)
x.2.unscaled <- as.matrix(set2[, -6])
x.2 <- scale.1(x.2.unscaled, x.1.unscaled)
summary(x.1)
summary(x.2)
library(FNN)
###########################################################################
# FNN
# The knn() function uses the X-values from the train= set as a supply of neighbours
#   from which to choose for each row in test= .  The true classes for the training
#   set are given in cl= .  Obviously, k=  gives the number of neighbours.
# The output is a factor list containing the predicted values (no "predict()" needed).
###########################################################################
# Fit the 1-NN function using set 1 to train AND test
#   (compute training error)
knnfit.1.1 <- knn(train = x.1, test = x.1, cl = set1[, 6], k = 1)
# Create Confusion Matrix and misclass rate
table(knnfit.1.1, set1[, 6], dnn = c("Predicted", "Observed"))
(misclass.knn1.1 <-
mean(ifelse(knnfit.1.1 == set1[, 6], yes = 0, no = 1)))
# Fit the 1-NN function using set 1 to train and set2 to test
#   (compute test error)
knnfit.1.2 <- knn(train = x.1, test = x.2, cl = set1[, 6], k = 1)
# Create Confusion Matrix and misclass rate
table(knnfit.1.2, set2[, 6], dnn = c("Predicted", "Observed"))
(misclass.knn1.2 <-
mean(ifelse(knnfit.1.2 == set2[, 6], yes = 0, no = 1)))
#  Now we tune using cv.knn(), which does Leave-one-out (n-fold) CV
# I created the steps below to fit a sequence of k  values to tune the knn.
#  Enter the maximum k as kmax.  Run knn on training set and predict test set.
#  Then compute test misclassification proportion as the output,
#  Need to change the data sets in the two lines in the "runknn" function.
kmax <- 40
k <- matrix(c(1:kmax), nrow = kmax)
runknn <- function(x) {
knncv.fit <- knn.cv(train = x.1, cl = set1[, 6], k = x)
# Fitted values are for deleted data from CV
mean(ifelse(knncv.fit == set1[, 6], yes = 0, no = 1))
}
mis <- apply(X = k, MARGIN = 1, FUN = runknn)
mis.se <- sqrt(mis * (1 - mis) / nrow(set2)) # SE of misclass rates
# Now plot results
# Plot like the CV plots, with 1SE bars and a horizontal line
#   at 1SE above minimum.
plot(x = k, y = mis, type = "b", ylim = c(.25, .50))
for (ii in c(1:kmax)) {
lines(x = c(k[ii], k[ii]), y = c(mis[ii] - mis.se[ii], mis[ii] + mis.se[ii]),
col = colors()[220])
}
abline(h = min(mis + mis.se), lty = "dotted")
# k for Minimum CV error
mink <- which.min(mis)
# Trying the value of k with the lowest validation error on test data set.
knnfitmin.2 <- knn(train = x.1, test = x.2, cl = set1[, 6], k = mink)
table(knnfitmin.2, set2[, 6], dnn = c("Predicted", "Observed"))
(misclass.2.knnmin <- mean(ifelse(knnfitmin.2 == set2[, 6], yes = 0, no = 1)))
# Less variable models have larger k, so find largest k within
#   1 SE of minimum validation error
serule <- max(which(mis < mis[mink] + mis.se[mink]))
knnfitse.2 <- knn(train = x.1, test = x.2, cl = set1[, 6], k = serule)
table(knnfitse.2, set2[, 6], dnn = c("Predicted", "Observed"))
(misclass.2.knnse <- mean(ifelse(knnfitse.2 == set2[, 6], yes = 0, no = 1)))
mis
mis.se
mean(ifelse(knnfit.1.1 == set1[, 6], yes = 0, no = 1)))
mean(ifelse(knnfit.1.1 == set1[, 6], yes = 0, no = 1))
source("C:/Users/Terry Liu/Desktop/STAT 452/Class Examples/Sec11_Nearest_Neighbour_Wheat.R", echo=TRUE)
k.min
setwd("C:/Users/Terry Liu/Desktop/STAT 452/Tut")
source("C:/Users/Terry Liu/Desktop/STAT 452/Tut/Sec11_Tutorial_KNN.R", echo=TRUE)
k.min
k.1se
knn.min
### Plot CV misclassification rates
plot(1:K.max, mis.CV,
xlab = "K", ylab = "Misclassification Rate",
ylim = c(0.4, 0.64)
)
### Add +/- 1 SE error bars. The easiest way to do this is with the
### lines() function. For each value of K, create a line joining
### the bottom to the top of our error bar
for (i in 1:K.max) {
lower <- mis.CV[i] - SE.mis.CV[i]
upper <- mis.CV[i] + SE.mis.CV[i]
lines(x = c(i, i), y = c(lower, upper))
}
### Get CV min value for K
k.min <- which.min(mis.CV)
### Add a horizontal line at min + 1SE
thresh <- mis.CV[k.min] + SE.mis.CV[k.min]
abline(h = thresh, col = "red")
### Get CV 1SE value for K
k.1se <- max(which(mis.CV <= thresh))
### Finally, let's see how our tuned KNN models do
knn.min <- knn(X.train, X.valid, Y.train, k.min)
knn.1se <- knn(X.train, X.valid, Y.train, k.1se)
table(knn.min, Y.valid, dnn = c("Predicted", "Observed"))
table(knn.1se, Y.valid, dnn = c("Predicted", "Observed"))
(mis.min <- mean(Y.valid != knn.min))
(mis.1se <- mean(Y.valid != knn.1se))
table(knnfit.1.2, set2[, 6], dnn = c("Predicted", "Observed"))
(misclass.knn1.2 <-
mean(ifelse(knnfit.1.2 == set2[, 6], yes = 0, no = 1)))
