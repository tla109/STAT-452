# Show coefficient path
plot(ridge1)
# abline(h = 0, col = "red")
select(ridge1)
(coef.ri.best1 <- coef(ridge1)[which.min(ridge1$GCV), ])
# Unfortunately, there is no predict() function for lm.ridge
#  Have to compute predicted values manually using matrix multiplication
#  Formula is
#  as.matrix(cbind(1,DATA)) %*% coef(RIDGE MODEL)
#  where DATA is the X-data for which you want predicted values,
#  and RIDGE MODEL is the lm.ridge object you have created.
#  The "1" adds a column of 1's for the intercept term.
#
# Here I can use the optimal lambda to make predictions
#  on test data (set==2)
pred.ri1 <- as.matrix(cbind(1, prostate[set == 2, 1:8])) %*% coef.ri.best1
MSPE1.ridge <- mean((prostate[set == 2, 9] - pred.ri1)^2)
pred.ri1
AQ <- na.omit(airquality[,1:4])
AQ$TWcp = AQ$Temp*AQ$Wind
AQ$TWrat = AQ$Temp/AQ$Wind
head(AQ)
set <- ifelse(runif(n = nrow(AQ)) > 0.5, yes = 2, no = 1)
library(MASS)
###############################################################
# Ridge regression using lm.ridge()
#
#  Running sequence from 0 to 100 by a small increment may be a little expensive
#  but probably covers range.
ridge1 <- lm.ridge(Ozone ~ ., lambda = seq(0, 100, .05), data = AQ[set == 1,])
select(ridge1)
(coef.ri.best1 <- coef(ridge1)[which.min(ridge1$GCV), ])
coef.ri.best1
pred.ri1 <- as.matrix(cbind(1, AQ[set == 2, 2:6])) %*% coef.ri.best1
MSPE1.ridge <- mean((AQ[set == 2, 1] - pred.ri1)^2)
AQ <- na.omit(airquality[,1:4])
AQ$TWcp = AQ$Temp*AQ$Wind
AQ$TWrat = AQ$Temp/AQ$Wind
head(AQ)
set <- ifelse(runif(n = nrow(AQ)) > 0.5, yes = 2, no = 1)
library(MASS)
###############################################################
# Ridge regression using lm.ridge()
#
#  Running sequence from 0 to 100 by a small increment may be a little expensive
#  but probably covers range.
ridge1 <- lm.ridge(Ozone ~ ., lambda = seq(0, 100, .05), data = AQ[set == 1,])
select(ridge1)
(coef.ri.best1 <- coef(ridge1)[which.min(ridge1$GCV), ])
coef.ri.best1
pred.ri1 <- as.matrix(cbind(1, AQ[set == 2, 2:6])) %*% coef.ri.best1
MSPE1.ridge <- mean((AQ[set == 2, 1] - pred.ri1)^2)
# Compare MSPE to MSPE from lm
mod.lm1 <- lm(Ozone ~ ., data = AQ[set == 1, ])
pred.lm1 <- predict(mod.lm1, newdata = AQ[set == 2, ])
MSPE1.lm <- mean((AQ[set == 2, 1] - pred.lm1)^2)
remove(list=ls())
pred.lm1
AQ <- na.omit(airquality[,1:4])
AQ$TWcp = AQ$Temp*AQ$Wind
AQ$TWrat = AQ$Temp/AQ$Wind
head(AQ)
set <- ifelse(runif(n = nrow(AQ)) > 0.5, yes = 2, no = 1)
library(MASS)
###############################################################
# Ridge regression using lm.ridge()
#
#  Running sequence from 0 to 100 by a small increment may be a little expensive
#  but probably covers range.
ridge1 <- lm.ridge(Ozone ~ ., lambda = seq(0, 100, .05), data = AQ[set == 1,])
select(ridge1)
(coef.ri.best1 <- coef(ridge1)[which.min(ridge1$GCV), ])
coef.ri.best1
pred.ri1 <- as.matrix(cbind(1, AQ[set == 2, 2:6])) %*% coef.ri.best1
MSPE1.ridge <- mean((AQ[set == 2, 1] - pred.ri1)^2)
# Compare MSPE to MSPE from lm
mod.lm1 <- lm(Ozone ~ ., data = AQ[set == 1, ])
pred.lm1 <- predict(mod.lm1, newdata = AQ[set == 2, ])
MSPE1.lm <- mean((AQ[set == 2, 1] - pred.lm1)^2)
pred.ril
AQ <- na.omit(airquality[,1:4])
AQ$TWcp = AQ$Temp*AQ$Wind
AQ$TWrat = AQ$Temp/AQ$Wind
head(AQ)
set <- ifelse(runif(n = nrow(AQ)) > 0.5, yes = 2, no = 1)
library(MASS)
###############################################################
# Ridge regression using lm.ridge()
#
#  Running sequence from 0 to 100 by a small increment may be a little expensive
#  but probably covers range.
ridge1 <- lm.ridge(Ozone ~ ., lambda = seq(0, 100, .05), data = AQ[set == 1,])
select(ridge1)
(coef.ri.best1 <- coef(ridge1)[which.min(ridge1$GCV), ])
coef.ri.best1
pred.ri1 <- as.matrix(cbind(1, AQ[set == 2, 2:6])) %*% coef.ri.best1
MSPE1.ridge <- mean((AQ[set == 2, 1] - pred.ri1)^2)
# Compare MSPE to MSPE from lm
mod.lm1 <- lm(Ozone ~ ., data = AQ[set == 1, ])
pred.lm1 <- predict(mod.lm1, newdata = AQ[set == 2, ])
MSPE1.lm <- mean((AQ[set == 2, 1] - pred.lm1)^2)
pred.ri1
pred.lm1
pred.ril - t(pred.lm1)
pred.ri1 - t(pred.lm1)
dim(pred.lm1)
dim(pred.ri1)
length(pred.ri1)
length(pred.lm1)
pred.ri1 - pred.lm1
pred.lm1 - pred.ri1
which(pred.lm1 - pred.ri1)
?which
which(pred.lm1 - pred.ri1 < 0)
length(which(pred.lm1 - pred.ri1 > 0)) /length(pred.lm1)
pred.lm1 - pred.ri1
2^5
knitr::opts_chunk$set(echo = TRUE)
y.1 <- AQ[set == 1, 1]
x.1 <- as.matrix(AQ[set == 1, c(2:9)])
head(AQ)
y.1 <- AQ[set == 1, 1]
x.1 <- as.matrix(AQ[set == 1, c(2:6)])
y.2 <- AQ[set == 2, 1]
x.2 <- as.matrix(AQ[set == 2, c(2:6)])
# Fit LASSO by glmnet(y=, x=). Gaussian is default, but other families are available
#  Function produces series of fits for many values of lambda.
# First half of data
lasso.1 <- glmnet(y = y.1, x = x.1, family = "gaussian")
library(glmnet)
# First half of data
lasso.1 <- glmnet(y = y.1, x = x.1, family = "gaussian")
# ("%Dev" in output below is R-square in linear regression)
lasso.1
plot(lasso.1) # Plots coefficient path
##### Note that these are on original scales, even though LASSO scaled variables
coef(lasso.1) # Lists out coefficients for each lambda
cv.lasso.1 <- cv.glmnet(y = y.1, x = x.1, family = "gaussian")
cv.lasso.1
library(glmnet)
y.1 <- AQ[set == 1, 1]
x.1 <- as.matrix(AQ[set == 1, c(2:6)])
y.2 <- AQ[set == 2, 1]
x.2 <- as.matrix(AQ[set == 2, c(2:6)])
# Fit LASSO by glmnet(y=, x=). Gaussian is default, but other families are available
#  Function produces series of fits for many values of lambda.
# First half of data
lasso.1 <- glmnet(y = y.1, x = x.1, family = "gaussian")
# cv.glmnet() uses crossvalidation to estimate optimal lambda
cv.lasso.1 <- cv.glmnet(y = y.1, x = x.1, family = "gaussian")
cv.lasso.1
coef(cv.lasso.1) # Print out coefficients at optimal lambda
head(AQ)
coef(cv.lasso.1) # Print out coefficients at optimal lambda
coef(cv.lasso.1, s = cv.lasso.1$lambda.min) # Another way to do this.
# Using the "+1SE rule" (see later) produces a sparser solution
coef(cv.lasso.1, s = cv.lasso.1$lambda.1se) # Another way to do this.
coef(cv.lasso.1, s = cv.lasso.1$lambda.min)
coef(cv.lasso.1, s = cv.lasso.1$lambda.1se)
coef(cv.lasso.1, s = cv.lasso.1$lambda.min)
coef(cv.lasso.1, s = cv.lasso.1$lambda.1se)
library(stringr)
library(glmnet)
library(MASS)
set.seed(3014312)
n <- 50 #  PLAY WITH THIS NUMBER
p <- 10 #  PLAY WITH THIS NUMBER
beta1 <- 1
sigma <- 3 # PLAY WITH THIS NUMBER
iter <- 100
coefs.lm <- matrix(NA, nrow = iter, ncol = p + 1)
coefs.sw <- matrix(NA, nrow = iter, ncol = p + 1)
coefs.ri <- matrix(NA, nrow = iter, ncol = p + 1)
coefs.la <- matrix(NA, nrow = iter, ncol = p + 1)
MSPEs <- matrix(NA, nrow = iter, ncol = 4)
colnames(MSPEs) <- c("LM", "STEP", "RIDGE", "LASSO")
## First generate some test data under the true model, and test X values
testx1 <- rnorm(n = 1000)
testx <- cbind(testx1, matrix(0, ncol = p-1, nrow = length(testx1)))
testx.all <- cbind(testx1, matrix(rnorm(n = (p-1) * length(testx1)),
ncol = (p-1), nrow = length(testx1)))
colnames(testx) <- paste0("X", 1:p)
colnames(testx.all) <- colnames(testx)
testy <- testx1 * beta1
## Then fit an example test set of size n
## Example Data Set
x <- matrix(rnorm(n = n * p), nrow = n)
eps <- rnorm(n, 0, sigma)
y <- beta1 * x[, 1] + eps
curve(
expr = beta1 * x, from = -3, to = 3,
ylim = c(-8, 8),
col = "red", lwd = 3, xlab = "X", ylab = "Y",
main = paste(
"One data set with n=", n, "\n Population R-squared=",
round(1 / (1 + sigma^2), 2)
)
)
points(x = x[, 1], y = y, pch = "X", col = "blue")
## then for 100 iterations fit each of the models to a new training set
## and look at the estimated coefficients and the MSPE on the test data
for (i in 1:iter) {
x <- matrix(rnorm(n = n * p), nrow = n)
eps <- rnorm(n, 0, sigma)
y <- beta1 * x[, 1] + eps
xydat <- data.frame(y, x)
mod.lm <- lm(y ~ ., data = xydat)
coefs.lm[i, ] <- coef(mod.lm)
step1 <- step(
object = lm(y ~ 1, data = xydat), scope = list(upper = mod.lm), direction = "both",
k = log(nrow(xydat)), trace = 0
)
coef.locs <- c(1, 1 + as.numeric(str_remove_all(names(coef(step1))[-1], "X")))
coefs.sw[i, ] <- 0
coefs.sw[i, coef.locs] <- coef(step1)
ridgec <- lm.ridge(y ~ ., lambda = seq(0, 100, .05), data = xydat)
coefs.ri[i, ] <- coef(ridgec)[which.min(ridgec$GCV), ]
cv.lasso.1 <- cv.glmnet(y = y, x = x, family = "gaussian")
coefs.la[i, ] <- coef(cv.lasso.1)[, 1]
pred.lm <- predict(mod.lm, as.data.frame(testx.all))
pred.sw <- predict(step1, as.data.frame(testx.all))
pred.ri <- as.matrix(cbind(1, testx.all)) %*% coef(ridgec)[which.min(ridgec$GCV), ]
pred.la <- predict(cv.lasso.1, testx.all)
MSPEs[i, ] <- c(
mean((testy - pred.lm)^2),
mean((testy - pred.sw)^2),
mean((testy - pred.ri)^2),
mean((testy - pred.la)^2)
)
}
boxplot(MSPEs[, 1:2],
main = paste0(
"Comparison of MSPEs\n R-squared=",
round(1 / (1 + sigma^2), 2), ", n=", n, ", p=", p
),
names = c("lm", "step")
)
coef(cv.lasso.1, s = cv.lasso.1$lambda.min)
remove(list=ls())
AQ <- na.omit(airquality[,1:4])
AQ$TWcp = AQ$Temp*AQ$Wind
AQ$TWrat = AQ$Temp/AQ$Wind
head(AQ)
set <- ifelse(runif(n = nrow(AQ)) > 0.5, yes = 2, no = 1)
library(MASS)
###############################################################
# Ridge regression using lm.ridge()
#
#  Running sequence from 0 to 100 by a small increment may be a little expensive
#  but probably covers range.
ridge1 <- lm.ridge(Ozone ~ ., lambda = seq(0, 100, .05), data = AQ[set == 1,])
select(ridge1)
(coef.ri.best1 <- coef(ridge1)[which.min(ridge1$GCV), ])
coef.ri.best1
length(which(pred.lm1 - pred.ri1 > 0)) /length(pred.lm1)
coef.ri.best1
summary(mod.lm1)
# Compare MSPE to MSPE from lm
mod.lm1 <- lm(Ozone ~ ., data = AQ[set == 1, ])
summary(mod.lm1)
coef.ls <- coef(mod.lm1)
coef.ls
coef.ri.best1
length(which(coef.ls - coef.ri.best1 > 0)) /length(coef.ls)
a = c(1:5)
a
a[2:]
a[2:,]
a[2:]
# Compare MSPE to MSPE from lm
mod.lm1 <- lm(Ozone ~ ., data = AQ[set == 1, ])
coef.ls <- coef(mod.lm1)
length(which(coef.ls[2:6] - coef.ri.best1[2:6] > 0)) /length(coef.ls[2:6])
AQ <- na.omit(airquality[,1:4])
AQ$TWcp = AQ$Temp*AQ$Wind
AQ$TWrat = AQ$Temp/AQ$Wind
head(AQ)
library(MASS)
###############################################################
# Ridge regression using lm.ridge()
#
#  Running sequence from 0 to 100 by a small increment may be a little expensive
#  but probably covers range.
ridge1 <- lm.ridge(Ozone ~ ., lambda = seq(0, 100, .05), data = AQ)
select(ridge1)
(coef.ri.best1 <- coef(ridge1)[which.min(ridge1$GCV), ])
coef.ri.best1
# Compare MSPE to MSPE from lm
mod.lm1 <- lm(Ozone ~ ., data = AQ)
coef.ls <- coef(mod.lm1)
length(which(coef.ls[2:6] - coef.ri.best1[2:6] > 0)) /length(coef.ls[2:6])
AQ <- na.omit(airquality[,1:4])
AQ$TWcp = AQ$Temp*AQ$Wind
AQ$TWrat = AQ$Temp/AQ$Wind
head(AQ)
library(MASS)
###############################################################
# Ridge regression using lm.ridge()
#
#  Running sequence from 0 to 100 by a small increment may be a little expensive
#  but probably covers range.
ridge1 <- lm.ridge(Ozone ~ ., lambda = seq(0, 100, .05), data = AQ)
select(ridge1)
(coef.ri.best1 <- coef(ridge1)[which.min(ridge1$GCV), ])
coef.ri.best1
AQ <- na.omit(airquality[,1:4])
AQ$TWcp = AQ$Temp*AQ$Wind
AQ$TWrat = AQ$Temp/AQ$Wind
head(AQ)
library(MASS)
###############################################################
# Ridge regression using lm.ridge()
#
#  Running sequence from 0 to 100 by a small increment may be a little expensive
#  but probably covers range.
ridge <- lm.ridge(Ozone ~ ., lambda = seq(0, 100, .05), data = AQ)
select(ridge)
(coef.ri.best <- coef(ridge)[which.min(ridge$GCV), ])
coef.ri.best
remove(list=ls())
AQ <- na.omit(airquality[,1:4])
AQ$TWcp = AQ$Temp*AQ$Wind
AQ$TWrat = AQ$Temp/AQ$Wind
head(AQ)
library(MASS)
###############################################################
# Ridge regression using lm.ridge()
#
#  Running sequence from 0 to 100 by a small increment may be a little expensive
#  but probably covers range.
ridge <- lm.ridge(Ozone ~ ., lambda = seq(0, 100, .05), data = AQ)
select(ridge)
(coef.ri.best <- coef(ridge)[which.min(ridge$GCV), ])
coef.ri.best
# Compare MSPE to MSPE from lm
mod.lm <- lm(Ozone ~ ., data = AQ)
coef.ls <- coef(mod.lm)
length(which(coef.ls[2:6] - coef.ri.best[2:6] > 0)) /length(coef.ls[2:6])
library(glmnet)
y<- AQ[,1]
x<- as.matrix(AQ[, c(2:6)])
# Fit LASSO by glmnet(y=, x=). Gaussian is default, but other families are available
#  Function produces series of fits for many values of lambda.
lasso <- glmnet(y = y, x = x, family = "gaussian")
# cv.glmnet() uses crossvalidation to estimate optimal lambda
cv.lasso <- cv.glmnet(y = y, x = x, family = "gaussian")
cv.lasso
coef(cv.lasso, s = cv.lasso$lambda.min)
coef(cv.lasso, s = cv.lasso$lambda.1se)
step <- step(
object = lm(y ~ 1, data = AQ), scope = list(upper = mod.lm), direction = "both",
k = log(nrow(AQ)), trace = 0)
step
coef.step <- coef(step)
coef.step
set.seed(2928893)
### Let's define a function for constructing CV folds
get.folds <- function(n, K) {
### Get the appropriate number of fold labels
n.fold <- ceiling(n / K) # Number of observations per fold (rounded up)
fold.ids.raw <- rep(1:K, times = n.fold) # Generate extra labels
fold.ids <- fold.ids.raw[1:n] # Keep only the correct number of labels
### Shuffle the fold labels
folds.rand <- fold.ids[sample.int(n)]
return(folds.rand)
}
### Number of folds
K <- 10
### Construct folds
n <- nrow(AQ) # Sample size
folds <- get.folds(n, K)
### Create a container for MSPEs. Let's include ordinary least-squares
### regression for reference
all.models <- c("LS", "Ridge", "LASSO-Min", "LASSO-1se")
all.MSPEs <- array(0, dim = c(K, length(all.models)))
colnames(all.MSPEs) <- all.models
### Begin cross-validation
for (i in 1:K) {
### Split data
data.train <- AQ[folds != i, ]
data.valid <- AQ[folds == i, ]
n.train <- nrow(data.train)
### Get response vectors
Y.train <- data.train$Ozone
Y.valid <- data.valid$Ozone
###################################################################
### First, let's quickly do LS so we have a reference point for ###
### how well the other models do.                               ###
###################################################################
fit.ls <- lm(Ozone ~ ., data = data.train)
pred.ls <- predict(fit.ls, newdata = data.valid)
MSPE.ls <- get.MSPE(Y.valid, pred.ls)
all.MSPEs[i, "LS"] <- MSPE.ls
#######################################################################
### Next, let's do ridge regression. This model is fit using the    ###
### lm.ridge() function in the MASS package. We will need to make a ###
### list of candidate lambda values for the function to choose      ###
### from. Prediction also has some extra steps, but we'll discuss   ###
### that when we get there.                                         ###
#######################################################################
### Make a list of lambda values. The lm.ridge() function will
### then choose the best value from this list. Use the seq()
### function to create an equally-spaced list.
lambda.vals <- seq(from = 0, to = 100, by = 0.05)
### Use the lm.ridge() function to fit a ridge regression model. The
### syntax is almost identical to the lm() function, we just need
### to set lambda equal to our list of candidate values.
fit.ridge <- lm.ridge(Ozone ~ .,
lambda = lambda.vals,
data = data.train
)
### To get predictions, we need to evaluate the fitted regression
### equation directly (sadly, no predict() function to do this for us).
### You could do this using a for loop if you prefer, but there is
### a shortcut which uses matrix-vector multiplication. The syntax
### for this multiplication method is much shorter.
### Get best lambda value and its index
### Note: Best is chosen according to smallest GCV value. We can
###       get GCV from a ridge regression object using $GCV
ind.min.GCV <- which.min(fit.ridge$GCV)
lambda.min <- lambda.vals[ind.min.GCV]
### Get coefficients corresponding to best lambda value
### We can get the coefficients for every value of lambda using
### the coef() function on a ridge regression object
all.coefs.ridge <- coef(fit.ridge)
coef.min <- all.coefs.ridge[ind.min.GCV, ]
### We will multiply the dataset by this coefficients vector, but
### we need to add a column to our dataset for the intercept and
### create indicators for our categorical predictors. A simple
### way to do this is using the model.matrix() function from last
### week.
matrix.valid.ridge <- model.matrix(Ozone ~ ., data = data.valid)
### Now we can multiply the data by our coefficient vector. The
### syntax in R for matrix-vector multiplication is %*%. Note that,
### for this type of multiplication, order matters. That is,
### A %*% B != B %*% A. Make sure you do data %*% coefficients.
### For more information, see me in a Q&A session or, better still,
### take a course on linear algebra (it's really neat stuff)
pred.ridge <- matrix.valid.ridge %*% coef.min
### Now we just need to calculate the MSPE and store it
MSPE.ridge <- get.MSPE(Y.valid, pred.ridge)
all.MSPEs[i, "Ridge"] <- MSPE.ridge
#######################################################################
### Now we can do the LASSO. This model is fit using the glmnet()   ###
### or cv.glmnet() functions in the glmnet package. LASSO also has  ###
### a tuning parameter, lambda, which we have to choose.            ###
### Fortunately, the cv.glmnet() function does CV internally, and   ###
### lets us automatically find the 'best' value of lambda.          ###
#######################################################################
### The cv.glmnet() function has different syntax from what we're
### used to. Here, we have to provide a matrix with all of our
### predictors, and a vector of our response. LASSO handles
### the intercept differently, so we want to make sure our data
### matrix does not include an intercept (then let cv.glmnet() add
### an intercept later). Unfortunately, the model.matrix() function
### gets confused if we ask it to construct indicators for our
### categorical predictors without also including an intercept.
### A simple way to fix this is to create the data matrix with an
### intercept, then delete the intercept.
matrix.train.raw <- model.matrix(Ozone ~ ., data = data.train)
matrix.train <- matrix.train.raw[, -1]
### The cv.glmnet() function creates a list of lambda values, then
### does CV internally to choose the 'best' one. 'Best' can refer to
### either the value of lambda which gives the smallest CV-MSPE
### (called the min rule), or the value of lambda which gives the
### simplest model that gives CV-MSPE close to the minimum (called
### the 1se rule). The cv.glmnet() function gets both of these
### lambda values.
all.LASSOs <- cv.glmnet(x = matrix.train, y = Y.train)
### Get both 'best' lambda values using $lambda.min and $lambda.1se
lambda.min <- all.LASSOs$lambda.min
lambda.1se <- all.LASSOs$lambda.1se
### cv.glmnet() has a predict() function (yay!). This predict function
### also does other things, like get the coefficients, or tell us
### which predictors get non-zero coefficients. We are also able
### to specify the value of lambda for which we want our output
### (remember that, with ridge, we got a matrix of coefficients and
### had to choose the row matching our lambda). Strangely, the name
### of the input where we specify our value of lambda is s.
### Get the coefficients for our two 'best' LASSO models
coef.LASSO.min <- predict(all.LASSOs, s = lambda.min, type = "coef")
coef.LASSO.1se <- predict(all.LASSOs, s = lambda.1se, type = "coef")
### Get which predictors are included in our models (i.e. which
### predictors have non-zero coefficients)
included.LASSO.min <- predict(all.LASSOs,
s = lambda.min,
type = "nonzero"
)
included.LASSO.1se <- predict(all.LASSOs,
s = lambda.1se,
type = "nonzero"
)
### Get predictions from both models on the validation fold. First,
### we need to create a predictor matrix from the validation set.
### Remember to include the intercept in model.matrix(), then delete
### it in the next step.
matrix.valid.LASSO.raw <- model.matrix(Ozone ~ ., data = data.valid)
matrix.valid.LASSO <- matrix.valid.LASSO.raw[, -1]
pred.LASSO.min <- predict(all.LASSOs,
newx = matrix.valid.LASSO,
s = lambda.min, type = "response"
)
pred.LASSO.1se <- predict(all.LASSOs,
newx = matrix.valid.LASSO,
s = lambda.1se, type = "response"
)
### Calculate MSPEs and store them
MSPE.LASSO.min <- get.MSPE(Y.valid, pred.LASSO.min)
all.MSPEs[i, "LASSO-Min"] <- MSPE.LASSO.min
MSPE.LASSO.1se <- get.MSPE(Y.valid, pred.LASSO.1se)
all.MSPEs[i, "LASSO-1se"] <- MSPE.LASSO.1se
}
