library(gbm)
set.seed(17299534)
source("Read_Wine_Data.R")
setwd("C:/Users/Terry Liu/Desktop/STAT 452/Tut")
source("Read_Wine_Data.R")
source("Sec16_Tutorial_Helper_Boosting.R")
### We will start by fitting one boosting model so we can see how it works.
### Boosting is done in R using the gbm() function in the gbm package. This function
### uses formula/data syntax. Other options include n.trees, interaction.depth,
### shrinkage and bag.fraction for, respectively, the number of trees to fit, the
### maximum tree depth, the learning rate, and the subsampling proportion. You
### also need to set distribution to "gaussian" when doing regression. If you want
### to do even more tuning, you can use n.minobsinnode to control the terminal
### node size.
### Let's try 60 trees, interaction depth of 1, shrinkage of 0.1, and bag
### fraction of 0.8
fit.gbm.1 <- gbm(alcohol ~ .,
data = data, distribution = "gaussian", n.trees = 100,
interaction.depth = 1, shrinkage = 0.1, bag.fraction = 0.8
)
### We can get various performance measures using the gbm.perf() function. Setting
### oobag.curve to T gives a plot of improvement in out-of-bag (OOB) error of each
### tree. The gbm.perf() function returns the number of trees at which (smoothed)
### OOB improvement goes from positive (good) to negative (bad). We can just get out
### the chosen number of trees without any plots by setting plot.it to F.
### Note: This process for choosing the number of trees tends to give too few. RoT's
###       approach is to double whatever gbm.perf() says is best.
gbm.perf(fit.gbm.1, oobag.curve = T)
n.trees.gbm <- gbm.perf(fit.gbm.1, plot.it = F)
n.trees.RoT <- 2 * n.trees.gbm
### By default, gbm() fits 100 trees. RoT's rule says we need 112, so we need to
### fit more. Fortunately, the gbm.more() function lets us add trees to an
### existing gbm fit. By default, gbm.more() adds 100 new trees, but you can change
### this by setting the number explicitly.
fit.gbm.2 <- gbm.more(fit.gbm.1, 20)
