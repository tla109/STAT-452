data.train <- proj[folds != i, ]
data.valid <- proj[folds == i, ]
n.train <- nrow(data.train)
### Get response vectors
Y.train <- data.train$Y
Y.valid <- data.valid$Y
### Let's do LS. ###
fit.ls <- lm(Y ~ ., data = data.train)
pred.ls <- predict(fit.ls, newdata = data.valid)
MSPE.ls <- get.MSPE(Y.valid, pred.ls)
all.MSPEs[i, "LS"] <- MSPE.ls
### Let's do ridge regression.  ###
lambda.vals <- seq(from = 0, to = 100, by = 0.05)
fit.ridge <- lm.ridge(Y ~ .,
lambda = lambda.vals,
data = data.train
)
ind.min.GCV <- which.min(fit.ridge$GCV)
lambda.min <- lambda.vals[ind.min.GCV]
all.coefs.ridge <- coef(fit.ridge)
coef.min <- all.coefs.ridge[ind.min.GCV, ]
matrix.valid.ridge <- model.matrix(Y ~ ., data = data.valid)
pred.ridge <- matrix.valid.ridge %*% coef.min
MSPE.ridge <- get.MSPE(Y.valid, pred.ridge)
all.MSPEs[i, "Ridge"] <- MSPE.ridge
### Now we can do the LASSO. ###
matrix.train.raw <- model.matrix(Y ~ ., data = data.train)
matrix.train <- matrix.train.raw[, -1]
### 'Best' can refer to
### either the value of lambda which gives the smallest CV-MSPE
### (called the min rule), or the value of lambda which gives the
### simplest model that gives CV-MSPE close to the minimum (called
### the 1se rule).
all.LASSOs <- cv.glmnet(x = matrix.train, y = Y.train)
lambda.min <- all.LASSOs$lambda.min
lambda.1se <- all.LASSOs$lambda.1se
coef.LASSO.min <- predict(all.LASSOs, s = lambda.min, type = "coef")
coef.LASSO.1se <- predict(all.LASSOs, s = lambda.1se, type = "coef")
included.LASSO.min <- predict(all.LASSOs,
s = lambda.min,
type = "nonzero"
)
included.LASSO.1se <- predict(all.LASSOs,
s = lambda.1se,
type = "nonzero"
)
matrix.valid.LASSO.raw <- model.matrix(Y ~ ., data = data.valid)
matrix.valid.LASSO <- matrix.valid.LASSO.raw[, -1]
pred.LASSO.min <- predict(all.LASSOs,
newx = matrix.valid.LASSO,
s = lambda.min, type = "response"
)
pred.LASSO.1se <- predict(all.LASSOs,
newx = matrix.valid.LASSO,
s = lambda.1se, type = "response"
)
MSPE.LASSO.min <- get.MSPE(Y.valid, pred.LASSO.min)
all.MSPEs[i, "LASSO-Min"] <- MSPE.LASSO.min
MSPE.LASSO.1se <- get.MSPE(Y.valid, pred.LASSO.1se)
all.MSPEs[i, "LASSO-1se"] <- MSPE.LASSO.1se
### the hybrid stepwise
step <- step(
object = lm(Y ~ 1, data = data.train), scope = list(upper = fit.ls), direction = "both",
k = log(nrow(data.train)), trace = 0)
pred.sw <- predict(step, as.data.frame(data.valid))
MSPE.sw <- get.MSPE(Y.valid, pred.sw)
all.MSPEs[i, "hybrid stepwise"] <- MSPE.sw
### Partial Least Squares (PLS)
fit_pls <- plsr(Y ~ .,
data = data.train, validation = "CV")
CV_pls <- fit_pls$validation
pls_comps <- CV_pls$PRESS
n_comps <- which.min(pls_comps)
pred.pls <- predict(fit_pls, data.valid, ncomp = n_comps)
MSPE.pls <- get.MSPE(Y.valid, pred.pls)
all.MSPEs.pls[i, "PLS"] <- MSPE.pls
}
library(stringr)
library(MASS)
library(glmnet)
library(pls)
K <- 100
### Construct folds
n <- nrow(proj) # Sample size
folds <- get.folds(n, K)
### Create a container for MSPEs. Let's include ordinary least-squares
### regression for reference
all.models <- c("Ridge", "LASSO-Min", "LASSO-1se", "LS", "hybrid stepwise", "PLS")
all.MSPEs <- array(0, dim = c(K, length(all.models)))
colnames(all.MSPEs) <- all.models
### Begin cross-validation
for (i in 1:K) {
### Split data
data.train <- proj[folds != i, ]
data.valid <- proj[folds == i, ]
n.train <- nrow(data.train)
### Get response vectors
Y.train <- data.train$Y
Y.valid <- data.valid$Y
### Let's do LS. ###
fit.ls <- lm(Y ~ ., data = data.train)
pred.ls <- predict(fit.ls, newdata = data.valid)
MSPE.ls <- get.MSPE(Y.valid, pred.ls)
all.MSPEs[i, "LS"] <- MSPE.ls
### Let's do ridge regression.  ###
lambda.vals <- seq(from = 0, to = 100, by = 0.05)
fit.ridge <- lm.ridge(Y ~ .,
lambda = lambda.vals,
data = data.train
)
ind.min.GCV <- which.min(fit.ridge$GCV)
lambda.min <- lambda.vals[ind.min.GCV]
all.coefs.ridge <- coef(fit.ridge)
coef.min <- all.coefs.ridge[ind.min.GCV, ]
matrix.valid.ridge <- model.matrix(Y ~ ., data = data.valid)
pred.ridge <- matrix.valid.ridge %*% coef.min
MSPE.ridge <- get.MSPE(Y.valid, pred.ridge)
all.MSPEs[i, "Ridge"] <- MSPE.ridge
### Now we can do the LASSO. ###
matrix.train.raw <- model.matrix(Y ~ ., data = data.train)
matrix.train <- matrix.train.raw[, -1]
### 'Best' can refer to
### either the value of lambda which gives the smallest CV-MSPE
### (called the min rule), or the value of lambda which gives the
### simplest model that gives CV-MSPE close to the minimum (called
### the 1se rule).
all.LASSOs <- cv.glmnet(x = matrix.train, y = Y.train)
lambda.min <- all.LASSOs$lambda.min
lambda.1se <- all.LASSOs$lambda.1se
coef.LASSO.min <- predict(all.LASSOs, s = lambda.min, type = "coef")
coef.LASSO.1se <- predict(all.LASSOs, s = lambda.1se, type = "coef")
included.LASSO.min <- predict(all.LASSOs,
s = lambda.min,
type = "nonzero"
)
included.LASSO.1se <- predict(all.LASSOs,
s = lambda.1se,
type = "nonzero"
)
matrix.valid.LASSO.raw <- model.matrix(Y ~ ., data = data.valid)
matrix.valid.LASSO <- matrix.valid.LASSO.raw[, -1]
pred.LASSO.min <- predict(all.LASSOs,
newx = matrix.valid.LASSO,
s = lambda.min, type = "response"
)
pred.LASSO.1se <- predict(all.LASSOs,
newx = matrix.valid.LASSO,
s = lambda.1se, type = "response"
)
MSPE.LASSO.min <- get.MSPE(Y.valid, pred.LASSO.min)
all.MSPEs[i, "LASSO-Min"] <- MSPE.LASSO.min
MSPE.LASSO.1se <- get.MSPE(Y.valid, pred.LASSO.1se)
all.MSPEs[i, "LASSO-1se"] <- MSPE.LASSO.1se
### the hybrid stepwise
step <- step(
object = lm(Y ~ 1, data = data.train), scope = list(upper = fit.ls), direction = "both",
k = log(nrow(data.train)), trace = 0)
pred.sw <- predict(step, as.data.frame(data.valid))
MSPE.sw <- get.MSPE(Y.valid, pred.sw)
all.MSPEs[i, "hybrid stepwise"] <- MSPE.sw
### Partial Least Squares (PLS)
fit_pls <- plsr(Y ~ .,
data = data.train, validation = "CV")
CV_pls <- fit_pls$validation
pls_comps <- CV_pls$PRESS
n_comps <- which.min(pls_comps)
pred.pls <- predict(fit_pls, data.valid, ncomp = n_comps)
MSPE.pls <- get.MSPE(Y.valid, pred.pls)
all.MSPEs[i, "PLS"] <- MSPE.pls
}
all.MSPEs
boxplot(all.MSPEs, main = paste0("CV MSPEs over ", K, " folds"))
avg.MSPEs = colMeans(all.MSPEs)
avg.MSPEs
boxplot(all.MSPEs, main = paste0("CV MSPEs over ", K, " folds"))
all.RMSPEs <- apply(all.MSPEs.all, 1, function(W) {
best <- min(W)
return(W / best)
})
boxplot(all.MSPEs, main = paste0("CV MSPEs over ", K, " folds"))
all.RMSPEs <- apply(all.MSPEs, 1, function(W) {
best <- min(W)
return(W / best)
})
all.RMSPEs <- t(all.RMSPEs)
boxplot(all.RMSPEs, main = paste0("CV RMSPEs over ", K, " folds"))
boxplot(all.RMSPEs, main = paste0("CV RMSPEs over ", K, " folds"))
apply(all.MSPEs, 2, min)
?apply
apply(all.MSPEs, 2, var)
knitr::opts_chunk$set(echo = TRUE)
### Fit polynomial regression models
fit.poly.3 <- lm(Ozone ~ poly(Temp, degree = 3), data = AQ)
### Fit polynomial regression models
fit.poly.3 <- lm(Ozone ~ poly(Temp, degree = 3), data = AQ)
knitr::opts_chunk$set(echo = TRUE)
AQ <- na.omit(airquality[,1:4])
AQ$TWcp = AQ$Temp*AQ$Wind
AQ$TWrat = AQ$Temp/AQ$Wind
head(AQ)
library(MASS)
###############################################################
# Ridge regression using lm.ridge()
#
#  Running sequence from 0 to 100 by a small increment may be a little expensive
#  but probably covers range.
ridge <- lm.ridge(Ozone ~ ., lambda = seq(0, 100, .05), data = AQ)
select(ridge)
(coef.ri.best <- coef(ridge)[which.min(ridge$GCV), ])
coef.ri.best
# Compare MSPE to MSPE from lm
mod.lm <- lm(Ozone ~ ., data = AQ)
coef.ls <- coef(mod.lm)
length(which(coef.ls[2:6] - coef.ri.best[2:6] > 0)) /length(coef.ls[2:6])
library(glmnet)
y<- AQ[,1]
x<- as.matrix(AQ[, c(2:6)])
# Fit LASSO by glmnet(y=, x=). Gaussian is default, but other families are available
#  Function produces series of fits for many values of lambda.
lasso <- glmnet(y = y, x = x, family = "gaussian")
# cv.glmnet() uses crossvalidation to estimate optimal lambda
cv.lasso <- cv.glmnet(y = y, x = x, family = "gaussian")
cv.lasso
coef(cv.lasso, s = cv.lasso$lambda.min)
coef(cv.lasso, s = cv.lasso$lambda.1se)
step <- step(
object = lm(y ~ 1, data = AQ), scope = list(upper = mod.lm), direction = "both",
k = log(nrow(AQ)), trace = 0)
coef.step <- coef(step)
coef.step
set.seed(2928893)
### Let's define a function for constructing CV folds
get.folds <- function(n, K) {
### Get the appropriate number of fold labels
n.fold <- ceiling(n / K) # Number of observations per fold (rounded up)
fold.ids.raw <- rep(1:K, times = n.fold) # Generate extra labels
fold.ids <- fold.ids.raw[1:n] # Keep only the correct number of labels
### Shuffle the fold labels
folds.rand <- fold.ids[sample.int(n)]
return(folds.rand)
}
get.MSPE <- function(Y, Y.hat) {
return(mean((Y - Y.hat)^2))
}
### Number of folds
K <- 10
### Construct folds
n <- nrow(AQ) # Sample size
folds <- get.folds(n, K)
### Create a container for MSPEs. Let's include ordinary least-squares
### regression for reference
all.models <- c("Ridge", "LASSO-Min", "LASSO-1se")
all.MSPEs <- array(0, dim = c(K, length(all.models)))
colnames(all.MSPEs) <- all.models
### Begin cross-validation
for (i in 1:K) {
### Split data
data.train <- AQ[folds != i, ]
data.valid <- AQ[folds == i, ]
n.train <- nrow(data.train)
### Get response vectors
Y.train <- data.train$Ozone
Y.valid <- data.valid$Ozone
#######################################################################
### Let's do ridge regression. This model is fit using the    ###
### lm.ridge() function in the MASS package. We will need to make a ###
### list of candidate lambda values for the function to choose      ###
### from. Prediction also has some extra steps, but we'll discuss   ###
### that when we get there.                                         ###
#######################################################################
### Make a list of lambda values. The lm.ridge() function will
### then choose the best value from this list. Use the seq()
### function to create an equally-spaced list.
lambda.vals <- seq(from = 0, to = 100, by = 0.05)
### Use the lm.ridge() function to fit a ridge regression model. The
### syntax is almost identical to the lm() function, we just need
### to set lambda equal to our list of candidate values.
fit.ridge <- lm.ridge(Ozone ~ .,
lambda = lambda.vals,
data = data.train
)
### To get predictions, we need to evaluate the fitted regression
### equation directly (sadly, no predict() function to do this for us).
### You could do this using a for loop if you prefer, but there is
### a shortcut which uses matrix-vector multiplication. The syntax
### for this multiplication method is much shorter.
### Get best lambda value and its index
### Note: Best is chosen according to smallest GCV value. We can
###       get GCV from a ridge regression object using $GCV
ind.min.GCV <- which.min(fit.ridge$GCV)
lambda.min <- lambda.vals[ind.min.GCV]
### Get coefficients corresponding to best lambda value
### We can get the coefficients for every value of lambda using
### the coef() function on a ridge regression object
all.coefs.ridge <- coef(fit.ridge)
coef.min <- all.coefs.ridge[ind.min.GCV, ]
### We will multiply the dataset by this coefficients vector, but
### we need to add a column to our dataset for the intercept and
### create indicators for our categorical predictors. A simple
### way to do this is using the model.matrix() function from last
### week.
matrix.valid.ridge <- model.matrix(Ozone ~ ., data = data.valid)
### Now we can multiply the data by our coefficient vector. The
### syntax in R for matrix-vector multiplication is %*%. Note that,
### for this type of multiplication, order matters. That is,
### A %*% B != B %*% A. Make sure you do data %*% coefficients.
### For more information, see me in a Q&A session or, better still,
### take a course on linear algebra (it's really neat stuff)
pred.ridge <- matrix.valid.ridge %*% coef.min
### Now we just need to calculate the MSPE and store it
MSPE.ridge <- get.MSPE(Y.valid, pred.ridge)
all.MSPEs[i, "Ridge"] <- MSPE.ridge
#######################################################################
### Now we can do the LASSO. This model is fit using the glmnet()   ###
### or cv.glmnet() functions in the glmnet package. LASSO also has  ###
### a tuning parameter, lambda, which we have to choose.            ###
### Fortunately, the cv.glmnet() function does CV internally, and   ###
### lets us automatically find the 'best' value of lambda.          ###
#######################################################################
### The cv.glmnet() function has different syntax from what we're
### used to. Here, we have to provide a matrix with all of our
### predictors, and a vector of our response. LASSO handles
### the intercept differently, so we want to make sure our data
### matrix does not include an intercept (then let cv.glmnet() add
### an intercept later). Unfortunately, the model.matrix() function
### gets confused if we ask it to construct indicators for our
### categorical predictors without also including an intercept.
### A simple way to fix this is to create the data matrix with an
### intercept, then delete the intercept.
matrix.train.raw <- model.matrix(Ozone ~ ., data = data.train)
matrix.train <- matrix.train.raw[, -1]
### The cv.glmnet() function creates a list of lambda values, then
### does CV internally to choose the 'best' one. 'Best' can refer to
### either the value of lambda which gives the smallest CV-MSPE
### (called the min rule), or the value of lambda which gives the
### simplest model that gives CV-MSPE close to the minimum (called
### the 1se rule). The cv.glmnet() function gets both of these
### lambda values.
all.LASSOs <- cv.glmnet(x = matrix.train, y = Y.train)
### Get both 'best' lambda values using $lambda.min and $lambda.1se
lambda.min <- all.LASSOs$lambda.min
lambda.1se <- all.LASSOs$lambda.1se
### cv.glmnet() has a predict() function (yay!). This predict function
### also does other things, like get the coefficients, or tell us
### which predictors get non-zero coefficients. We are also able
### to specify the value of lambda for which we want our output
### (remember that, with ridge, we got a matrix of coefficients and
### had to choose the row matching our lambda). Strangely, the name
### of the input where we specify our value of lambda is s.
### Get the coefficients for our two 'best' LASSO models
coef.LASSO.min <- predict(all.LASSOs, s = lambda.min, type = "coef")
coef.LASSO.1se <- predict(all.LASSOs, s = lambda.1se, type = "coef")
### Get which predictors are included in our models (i.e. which
### predictors have non-zero coefficients)
included.LASSO.min <- predict(all.LASSOs,
s = lambda.min,
type = "nonzero"
)
included.LASSO.1se <- predict(all.LASSOs,
s = lambda.1se,
type = "nonzero"
)
### Get predictions from both models on the validation fold. First,
### we need to create a predictor matrix from the validation set.
### Remember to include the intercept in model.matrix(), then delete
### it in the next step.
matrix.valid.LASSO.raw <- model.matrix(Ozone ~ ., data = data.valid)
matrix.valid.LASSO <- matrix.valid.LASSO.raw[, -1]
pred.LASSO.min <- predict(all.LASSOs,
newx = matrix.valid.LASSO,
s = lambda.min, type = "response"
)
pred.LASSO.1se <- predict(all.LASSOs,
newx = matrix.valid.LASSO,
s = lambda.1se, type = "response"
)
### Calculate MSPEs and store them
MSPE.LASSO.min <- get.MSPE(Y.valid, pred.LASSO.min)
all.MSPEs[i, "LASSO-Min"] <- MSPE.LASSO.min
MSPE.LASSO.1se <- get.MSPE(Y.valid, pred.LASSO.1se)
all.MSPEs[i, "LASSO-1se"] <- MSPE.LASSO.1se
}
all.MSPEs
avg.MSPEs = colMeans(all.MSPEs)
avg.MSPEs
library(stringr)
all.MSPEs.LS.step <- array(0, dim = c(K, 2))
colnames(all.MSPEs.LS.step) <- c("LS", "hybrid stepwise")
coefs.sw <- matrix(NA, nrow = K, ncol = 11)
for (i in 1:K) {
### Split data
data.train <- AQ[folds != i, ]
data.valid <- AQ[folds == i, ]
n.train <- nrow(data.train)
### Get response vectors
Y.train <- data.train$Ozone
Y.valid <- data.valid$Ozone
###################################################################
### First, let's quickly do LS so we have a reference point for ###
### how well the other models do.                               ###
###################################################################
fit.ls <- lm(Ozone ~ ., data = data.train)
pred.ls <- predict(fit.ls, newdata = data.valid)
MSPE.ls <- get.MSPE(Y.valid, pred.ls)
all.MSPEs.LS.step[i, "LS"] <- MSPE.ls
step <- step(
object = lm(Ozone ~ 1, data = data.train), scope = list(upper = fit.ls), direction = "both",
k = log(nrow(data.train)), trace = 0)
pred.sw <- predict(step, as.data.frame(data.valid))
MSPE.sw <- get.MSPE(Y.valid, pred.sw)
all.MSPEs.LS.step[i, "hybrid stepwise"] <- MSPE.sw
}
all.MSPEs.all = cbind(all.MSPEs.LS.step, all.MSPEs)
boxplot(all.MSPEs.all, main = paste0("CV MSPEs over ", K, " folds"))
all.RMSPEs <- apply(all.MSPEs.all, 1, function(W) {
best <- min(W)
return(W / best)
})
all.RMSPEs <- t(all.RMSPEs)
boxplot(all.RMSPEs, main = paste0("CV RMSPEs over ", K, " folds"))
library(pls)
all.MSPEs.pls <- array(0, dim = c(K, 1))
colnames(all.MSPEs.pls) <- c("PLS")
n_comps = array(0, dim = c(K,1))
for (i in 1:K) {
### Split data
data.train <- AQ[folds != i, ]
data.valid <- AQ[folds == i, ]
n.train <- nrow(data.train)
### Get response vectors
Y.train <- data.train$Ozone
Y.valid <- data.valid$Ozone
### Now, let's do PLS using the plsr() function. The syntax is
### very similar to lm(). If we set validation = "CV", the plsr()
### function will do its own internal CV, and give MSPEs for each
### number of components. We can then use this to choose how many
### componenets to keep when doing prediction on the validation
### fold. We can use an optional input called segments to specify
### how many folds we want plsr() to use for its internal CV
### (default is 10).
fit_pls <- plsr(Ozone ~ .,
data = data.train, validation = "CV")
### Investigate the fitted PLS model. Comment out the next two
### lines when running a CV loop
### The summary function gives us lots of information about how
### errors change as we increase the number of components
# summary(fit.pls)
### The validationplot() function shows how MSPE from the internal
### CV of plsr() changes with the number of included components.
# validationplot(fit.pls)
### Get the best model from PLS. To do this, we need to find the model
### that minimizes MSPE for the plsr() function's internal CV. It
### takes a few steps, but all the information we need is contained
### in the output of plsr().
CV_pls <- fit_pls$validation
pls_comps <- CV_pls$PRESS
n_comps[i] <- which.min(pls_comps)
### Get predictions and calculate MSPE on the validation fold
### Set ncomps equal to the optimal number of components
pred.pls <- predict(fit_pls, data.valid, ncomp = n_comps[i])
MSPE.pls <- get.MSPE(Y.valid, pred.pls)
all.MSPEs.pls[i, "PLS"] <- MSPE.pls
}
n_comps
all.MSPEs.pls
mean(all.MSPEs.pls)
boxplot(cbind(all.MSPEs.all, all.MSPEs.pls), main = paste0("CV MSPEs over ", K, " folds"))
all.RMSPEs.all <- apply(cbind(all.MSPEs.all, all.MSPEs.pls), 1, function(W) {
best <- min(W)
return(W / best)
})
all.RMSPEs.all <- t(all.RMSPEs.all)
boxplot(all.RMSPEs.all, main = paste0("CV RMSPEs over ", K, " folds"))
### Fit polynomial regression models
fit.poly.3 <- lm(Ozone ~ poly(Temp, degree = 3), data = AQ)
### Fit basis splines
library(splines)
fit.basis.5 <- lm(Ozone ~ bs(Temp, degree = 5), data = AQ)
fit.basis.7 <- lm(Ozone ~ bs(Temp, degree = 7), data = AQ)
fit.basis.9 <- lm(Ozone ~ bs(Temp, degree = 9), data = AQ)
fit.basis.20 <- lm(Ozone ~ bs(Temp, degree = 20), data = AQ)
### Predicting
Temp.sort <- data.frame(Temp = sort(AQ$Temp))
pred.poly.3 <- predict(fit.poly.3, Temp.sort)
pred.basis.5 <- predict(fit.basis.5, Temp.sort)
pred.basis.7 <- predict(fit.basis.7, Temp.sort)
pred.basis.9 <- predict(fit.basis.9, Temp.sort)
pred.basis.20 <- predict(fit.basis.20, Temp.sort)
### Plots
with(AQ, plot(Temp, Ozone))
lines(Temp.sort$Temp, pred.poly.3)
lines(Temp.sort$Temp, pred.basis.5, col = 'red')
lines(Temp.sort$Temp, pred.basis.7, col = 'blue')
lines(Temp.sort$Temp, pred.basis.9, col = 'green')
lines(Temp.sort$Temp, pred.basis.20, col = 'gold')
legend(x = 55, y = 180, legend = c(
"Cubic Poly", "Cubic Spline 5 df",
"Cubic Spline 7 df", "Cubic Spline 9 df", "Cubic Spline 20 df"
),
lty = "solid", col = c('black', 'red', 'blue', 'green', 'gold'), lwd = 2
)
### Plots
with(AQ, plot(Temp, Ozone))
lines(Temp.sort$Temp, pred.poly.3)
with(AQ, plot(Temp, Ozone))
lines(Temp.sort$Temp, pred.poly.3)
### Plots
with(AQ, plot(Temp, Ozone))
lines(Temp.sort$Temp, pred.poly.3)
with(AQ, plot(Temp, Ozone))
lines(Temp.sort$Temp, pred.poly.3)
lines(Temp.sort$Temp, pred.basis.5, col = 'red')
