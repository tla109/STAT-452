###########################################################################
### In this tutorial, we will use boosting on the wine quality data to  ###
### predict alcohol using a subset of the other predictors.             ###
###########################################################################

library(gbm)

set.seed(17299534)

source("Read_Wine_Data.R")
source("Sec16_Tutorial_Helper_Boosting.R")


### We will start by fitting one boosting model so we can see how it works.
### Boosting is done in R using the gbm() function in the gbm package. This function
### uses formula/data syntax. Other options include n.trees, interaction.depth,
### shrinkage and bag.fraction for, respectively, the number of trees to fit, the
### maximum tree depth, the learning rate, and the subsampling proportion. You
### also need to set distribution to "gaussian" when doing regression. If you want
### to do even more tuning, you can use n.minobsinnode to control the terminal
### node size.
### Let's try 60 trees, interaction depth of 1, shrinkage of 0.1, and bag
### fraction of 0.8
fit.gbm.1 <- gbm(alcohol ~ .,
  data = data, distribution = "gaussian", n.trees = 100,
  interaction.depth = 1, shrinkage = 0.1, bag.fraction = 0.8
)

### We can get various performance measures using the gbm.perf() function. Setting
### oobag.curve to T gives a plot of improvement in out-of-bag (OOB) error of each
### tree. The gbm.perf() function returns the number of trees at which (smoothed)
### OOB improvement goes from positive (good) to negative (bad). We can just get out
### the chosen number of trees without any plots by setting plot.it to F.
### Note: This process for choosing the number of trees tends to give too few. RoT's
###       approach is to double whatever gbm.perf() says is best.
gbm.perf(fit.gbm.1, oobag.curve = T)
n.trees.gbm <- gbm.perf(fit.gbm.1, plot.it = F)
n.trees.RoT <- 2 * n.trees.gbm

### By default, gbm() fits 100 trees. RoT's rule says we need 112, so we need to
### fit more. Fortunately, the gbm.more() function lets us add trees to an
### existing gbm fit. By default, gbm.more() adds 100 new trees, but you can change
### this by setting the number explicitly.
fit.gbm.2 <- gbm.more(fit.gbm.1, 20)

### For completeness, let's get the SMSE. The predict function requires that we
### specify the number of trees to use.
pred.gbm <- predict(fit.gbm.2, data, n.trees = n.trees.gbm)
pred.RoT <- predict(fit.gbm.2, data, n.trees = n.trees.RoT)
(SMSE.gbm <- get.MSPE(data$alcohol, pred.gbm))
(SMSE.RoT <- get.MSPE(data$alcohol, pred.RoT))


###############################################################################
### Neither of these models do very well, even on SMSE. Let's see if we can ###
### improve by tuning.                                                      ###
###############################################################################

set.seed(10326789)

### Set parameter values
### We will stick to resampling rate of 0.8, maximum of 10000 trees, and RoT's rule
### for choosing how many trees to keep.
max.trees <- 10000
all.shrink <- c(0.001, 0.01, 0.1)
all.depth <- c(1, 2, 3)
all.pars <- expand.grid(shrink = all.shrink, depth = all.depth)
n.pars <- nrow(all.pars)

### Number of folds
K <- 5

### Get folds
n <- nrow(data)
folds <- get.folds(n, K)

### Create container for CV MSPEs
CV.MSPEs <- array(0, dim = c(K, n.pars))


for (i in 1:K) {
  ### Print progress update
  print(paste0(i, " of ", K))

  ### Split data
  data.train <- data[folds != i, ]
  data.valid <- data[folds == i, ]
  Y.valid <- data.valid$alcohol



  ### Fit boosting models for each parameter combination
  for (j in 1:n.pars) {
    ### Get current parameter values
    this.shrink <- all.pars[j, "shrink"]
    this.depth <- all.pars[j, "depth"]

    ### Fit model using current parameter values.
    fit.gbm <- gbm(alcohol ~ .,
      data = data.train, distribution = "gaussian",
      n.trees = max.trees, interaction.depth = this.depth, shrinkage = this.shrink,
      bag.fraction = 0.8
    )

    ### Choose how many trees to keep using RoT's rule. This will print many
    ### warnings about not just using the number of trees recommended by
    ### gbm.perf(). We have already addressed this problem though, so we can
    ### just ignore the warnings.
    n.trees <- gbm.perf(fit.gbm, plot.it = F) * 2

    ### Check to make sure that RoT's rule doesn't tell us to use more than 1000
    ### trees. If it does, add extra trees as necessary
    if (n.trees > max.trees) {
      extra.trees <- n.trees - max.trees
      fit.gbm <- gbm.more(fit.gbm, extra.trees)
    }

    ### Get predictions and MSPE, then store MSPE
    pred.gbm <- predict(fit.gbm, data.valid, n.trees)
    MSPE.gbm <- get.MSPE(Y.valid, pred.gbm)

    CV.MSPEs[i, j] <- MSPE.gbm # Be careful with indices for CV.MSPEs
  }
}


### We can now make an MSPE boxplot. First, add column names to indicate
### which parameter combination was used. Format is shrinkage-depth
names.pars <- paste0(
  all.pars$shrink, "-",
  all.pars$depth
)
colnames(CV.MSPEs) <- names.pars

### Make boxplot
boxplot(CV.MSPEs, las = 2, main = "MSPE Boxplot")


### Get relative MSPEs and make boxplot
CV.RMSPEs <- apply(CV.MSPEs, 1, function(W) W / min(W))
CV.RMSPEs <- t(CV.RMSPEs)
boxplot(CV.RMSPEs, las = 2, main = "RMSPE Boxplot")

###############################################################################
### This dataset appears to prefer higher interaction depth. It's not clear ###
### about shrinkage though. Let's run another round of CV with larger       ###
### interaction depths but the same shrinkage.                              ###
###############################################################################

set.seed(68211069)

### Set parameter values
### We will stick to resampling rate of 0.8, maximum of 10000 trees, and RoT's rule
### for choosing how many trees to keep.
max.trees <- 10000
all.shrink <- c(0.001, 0.01, 0.1)
all.depth <- c(3, 4, 5)
all.pars <- expand.grid(shrink = all.shrink, depth = all.depth)
n.pars <- nrow(all.pars)

### Number of folds
K <- 5

### Create folds
folds <- get.folds(n, K)

### Create container for CV MSPEs
CV.MSPEs2 <- array(0, dim = c(K, n.pars))

for (i in 1:K) {
  ### Print progress update
  print(paste0(i, " of ", K))

  ### Split data
  data.train <- data[folds != i, ]
  data.valid <- data[folds == i, ]
  Y.valid <- data.valid$alcohol



  ### Fit boosting models for each parameter combination
  for (j in 1:n.pars) {
    ### Get current parameter values
    this.shrink <- all.pars[j, "shrink"]
    this.depth <- all.pars[j, "depth"]

    ### Fit model using current parameter values.
    fit.gbm <- gbm(alcohol ~ .,
      data = data.train, distribution = "gaussian",
      n.trees = max.trees, interaction.depth = this.depth, shrinkage = this.shrink,
      bag.fraction = 0.8
    )

    ### Choose how many trees to keep using RoT's rule. This will print many
    ### warnings about not just using the number of trees recommended by
    ### gbm.perf(). We have already addressed this problem though, so we can
    ### just ignore the warnings.
    n.trees <- gbm.perf(fit.gbm, plot.it = F) * 2

    ### Check to make sure that RoT's rule doesn't tell us to use more than 1000
    ### trees. If it does, add extra trees as necessary
    if (n.trees > max.trees) {
      extra.trees <- n.trees - max.trees
      fit.gbm <- gbm.more(fit.gbm, extra.trees)
    }

    ### Get predictions and MSPE, then store MSPE
    pred.gbm <- predict(fit.gbm, data.valid, n.trees)
    MSPE.gbm <- get.MSPE(Y.valid, pred.gbm)

    CV.MSPEs2[i, j] <- MSPE.gbm # Be careful with indices for CV.MSPEs
  }
}


### We can now make an MSPE boxplot. First, add column names to indicate
### which parameter combination was used. Format is shrinkage-depth
names.pars <- paste0(
  all.pars$shrink, "-",
  all.pars$depth
)
colnames(CV.MSPEs2) <- names.pars

### Make boxplot
boxplot(CV.MSPEs2, las = 2, main = "MSPE Boxplot")


### Get relative MSPEs and make boxplot
CV.RMSPEs2 <- apply(CV.MSPEs2, 1, function(W) W / min(W))
CV.RMSPEs2 <- t(CV.RMSPEs2)
boxplot(CV.RMSPEs2, las = 2, main = "RMSPE Boxplot")



### Based on the RMSPE boxplot, the model with shrinkage = 0.1 and depth = 3 looks
### best. Let's fit this model and compare it to the default one we fit above.

fit.gbm.best <- gbm(alcohol ~ .,
  data = data, distribution = "gaussian",
  n.trees = 10000, interaction.depth = 3, shrinkage = 0.1, bag.fraction = 0.8
)

n.trees.best <- gbm.perf(fit.gbm.best, plot.it = F) * 2 # Number of trees


### Lets compare the default boosting model to our tuned version. To do this,
### we will need a test set. Fortunately, I have uploaded a 1000 observation
### subsample of the wine data to Canvas for just this purpose. Let's read-in
### the test set and get MSPEs
data.test.raw <- read.csv("Datasets/Wine_Quality_Test.csv")
data.test <- data.test.raw[, c(4, 8, 9, 10, 11)]
colnames(data.test)[1] <- "sugar"

pred.gbm <- predict(fit.gbm, data.test, n.trees.gbm)
(MSPE.gbm <- get.MSPE(data.test$alcohol, pred.gbm))

pred.RoT <- predict(fit.gbm, data.test, n.trees.RoT)
(MSPE.RoT <- get.MSPE(data.test$alcohol, pred.RoT))

pred.best <- predict(fit.gbm.best, data.test, n.trees.best)
(MSPE.best <- get.MSPE(data.test$alcohol, pred.best))
